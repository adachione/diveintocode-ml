{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sprint 深層学習スクラッチ ディープニューラルネットワーク"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "import copy\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GetMiniBatch:\n",
    "    \"\"\"\n",
    "    ミニバッチを取得するイテレータ\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : 次の形のndarray, shape (n_samples, n_features)\n",
    "      訓練データ\n",
    "    y : 次の形のndarray, shape (n_samples, 1)\n",
    "      正解値\n",
    "    batch_size : int\n",
    "      バッチサイズ\n",
    "    seed : int\n",
    "      NumPyの乱数のシード\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, X, y, batch_size=20, seed=0):\n",
    "        self.batch_size = batch_size\n",
    "        np.random.seed(seed)\n",
    "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
    "        self._X = X[shuffle_index]\n",
    "        self._y = y[shuffle_index]\n",
    "        self._stop = np.ceil(X.shape[0] / self.batch_size).astype(np.int)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._stop\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        p0 = item * self.batch_size\n",
    "        p1 = item * self.batch_size + self.batch_size\n",
    "        return self._X[p0:p1], self._y[p0:p1]\n",
    "\n",
    "    def __iter__(self):\n",
    "        self._counter = 0\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        if self._counter >= self._stop:\n",
    "            raise StopIteration()\n",
    "        p0 = self._counter * self.batch_size\n",
    "        p1 = self._counter * self.batch_size + self.batch_size\n",
    "        self._counter += 1\n",
    "        return self._X[p0:p1], self._y[p0:p1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題1】全結合層のクラス化\n",
    "全結合層のクラス化を行なってください。\n",
    "\n",
    "\n",
    "以下に雛形を載せました。コンストラクタで重みやバイアスの初期化をして、あとはフォワードとバックワードのメソッドを用意します。重みW、バイアスB、およびフォワード時の入力Xをインスタンス変数として保持しておくことで、煩雑な入出力は不要になります。\n",
    "\n",
    "\n",
    "なお、インスタンスも引数として渡すことができます。そのため、初期化方法のインスタンスinitializerをコンストラクタで受け取れば、それにより初期化が行われます。渡すインスタンスを変えれば、初期化方法が変えられます。\n",
    "\n",
    "\n",
    "また、引数として自身のインスタンスselfを渡すこともできます。これを利用してself.optimizer.update(self)という風に層の重みの更新が可能です。更新に必要な値は複数ありますが、全て全結合層が持つインスタンス変数にすることができます。\n",
    "\n",
    "\n",
    "初期化方法と最適化手法のクラスについては後述します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FC:\n",
    "    \"\"\"\n",
    "    ノード数n_nodes1からn_nodes2への全結合層\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_nodes1 : int\n",
    "      前の層のノード数\n",
    "    n_nodes2 : int\n",
    "      後の層のノード数\n",
    "    initializer : 初期化方法のインスタンス\n",
    "    optimizer : 最適化手法のインスタンス\n",
    "    \"\"\"\n",
    "    def __init__(self, n_nodes1, n_nodes2, initializer, optimizer):\n",
    "        self.n_nodes1 = n_nodes1\n",
    "        self.n_nodes2 = n_nodes2\n",
    "        self.initializer = initializer\n",
    "        self.optimizer = optimizer\n",
    "        # 初期化\n",
    "        # initializerのメソッドを使い、self.Wとself.Bを初期化する\n",
    "        self.W = self.initializer.W(self.n_nodes1, self.n_nodes2)\n",
    "        self.B = self.initializer.B(self.n_nodes2)\n",
    "        self.dW = 0\n",
    "        self.dB = 0\n",
    "        \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        フォワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            入力\n",
    "        Returns\n",
    "        ----------\n",
    "        A : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            出力\n",
    "        \"\"\"\n",
    "        self.X = X\n",
    "        A = np.dot(X, self.W) + self.B   # (batch_size, n_nodes2)\n",
    "\n",
    "        return A\n",
    "    \n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "        バックワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        dA : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            後ろから流れてきた勾配\n",
    "        Returns\n",
    "        ----------\n",
    "        dZ : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            前に流す勾配\n",
    "        \"\"\"\n",
    "        n_batch = dA.shape[0]\n",
    "        \n",
    "        # TODO　dw とdbをバッチサイズで割る　FCで実行\n",
    "        self.dB = np.sum(dA, axis=0)/n_batch\n",
    "        self.dW = np.dot(self.X.T, dA)/n_batch\n",
    "        dZ = np.dot(dA, self.W.T)    # (batch_size, n_nodes1)\n",
    "        \n",
    "        # 更新\n",
    "        self = self.optimizer.update(self)\n",
    "        return dZ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題2】初期化方法のクラス化\n",
    "初期化を行うコードをクラス化してください。\n",
    "\n",
    "\n",
    "前述のように、全結合層のコンストラクタに初期化方法のインスタンスを渡せるようにします。以下の雛形に必要なコードを書き加えていってください。標準偏差の値（sigma）はコンストラクタで受け取るようにすることで、全結合層のクラス内にこの値（sigma）を渡さなくてすむようになります。\n",
    "\n",
    "\n",
    "これまで扱ってきた初期化方法はSimpleInitializerクラスと名付けることにします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleInitializer:\n",
    "    \"\"\"\n",
    "    ガウス分布によるシンプルな初期化\n",
    "    Parameters\n",
    "    ----------\n",
    "    sigma : float\n",
    "      ガウス分布の標準偏差\n",
    "    \"\"\"\n",
    "    def __init__(self, sigma):\n",
    "        self.sigma = sigma\n",
    "    def W(self, n_nodes1, n_nodes2):\n",
    "        \"\"\"\n",
    "        重みの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes1 : int\n",
    "          前の層のノード数\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        W :\n",
    "        \"\"\"\n",
    "        W = self.sigma * np.random.randn(n_nodes1, n_nodes2)\n",
    "        \n",
    "        return W\n",
    "    \n",
    "    def B(self, n_nodes2):\n",
    "        \"\"\"\n",
    "        バイアスの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        B :\n",
    "        \"\"\"\n",
    "        B = np.random.randn(n_nodes2)\n",
    "        \n",
    "        return B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題3】最適化手法のクラス化\n",
    "最適化手法のクラス化を行なってください。\n",
    "\n",
    "\n",
    "最適化手法に関しても初期化方法同様に全結合層にインスタンスとして渡します。バックワードのときにself.optimizer.update(self)のように更新できるようにします。以下の雛形に必要なコードを書き加えていってください。\n",
    "\n",
    "\n",
    "これまで扱ってきた最適化手法はSGDクラス（Stochastic Gradient Descent、確率的勾配降下法）として作成します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    \"\"\"\n",
    "    確率的勾配降下法\n",
    "    Parameters\n",
    "    ----------\n",
    "    lr : 学習率\n",
    "    \"\"\"\n",
    "    def __init__(self, lr=0.01):\n",
    "        self.lr = lr\n",
    "    def update(self, layer):\n",
    "        \"\"\"\n",
    "        ある層の重みやバイアスの更新\n",
    "        Parameters\n",
    "        ----------\n",
    "        layer : 更新前の層のインスタンス\n",
    "        \"\"\"\n",
    "        # TODO Eの計算\n",
    "        \n",
    "        layer.W -= self.lr * layer.dW\n",
    "        layer.B -= self.lr * layer.dB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題4】活性化関数のクラス化\n",
    "活性化関数のクラス化を行なってください。\n",
    "\n",
    "\n",
    "ソフトマックス関数のバックプロパゲーションには交差エントロピー誤差の計算も含む実装を行うことで計算が簡略化されます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tanh():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    def forward(self, A):\n",
    "        self.A = A\n",
    "        return np.tanh(A)\n",
    "    \n",
    "    def backward(self, dZ):\n",
    "        \"\"\"\n",
    "        dZ : n層目のバックプロバゲーション\n",
    "        A :　n層目のフォワードプロバゲーションのA\n",
    "        \n",
    "        \"\"\"\n",
    "        y = dZ*((1-self.forward(self.A)**2))\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    def forward(self, A):\n",
    "        self.A = A\n",
    "        return 1 / (1 + np.exp(-A))\n",
    "    \n",
    "    def backward(self, dZ):\n",
    "        \"\"\"\n",
    "        dZ : n層目のバックプロバゲーション\n",
    "        A :　n層目のフォワードプロバゲーションのA\n",
    "         \n",
    "        \"\"\"\n",
    "        y = dZ*(1-self.forward(self.A))*(self.forward(self.A))\n",
    "        return y    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax():\n",
    "    \"\"\"\n",
    "    ソフトマックス関数クラス\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    def forward(self, A):\n",
    "        # ソフトマックス計算 Zが出力値\n",
    "        self.A = A\n",
    "        c = np.max(A)\n",
    "        self.Z = np.exp(A-c) / np.sum(np.exp(A-c), axis=1, keepdims=True)\n",
    "        \n",
    "        return self.Z\n",
    "    \n",
    "    def backward(self, Z, y):\n",
    "        \"\"\"\n",
    "        dZ　: 最終層のZ\n",
    "        Y　:　（バッチ）サンプルラベル\n",
    "        \"\"\"\n",
    "        dZ = self.Z - y\n",
    "        \n",
    "        # クロスエントロピー誤差計算\n",
    "        batch_size = y.shape[0]\n",
    "        error = -np.sum(y*np.log(self.Z + 1e-7)) / batch_size\n",
    "        \n",
    "        return dZ , error\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題5】ReLUクラスの作成\n",
    "現在一般的に使われている活性化関数であるReLU（Rectified Linear Unit）をReLUクラスとして実装してください。\n",
    "\n",
    "\n",
    "ReLUは以下の数式です。\n",
    "\n",
    "$$\n",
    "f(x)=ReLU(x)=\n",
    "\\begin{cases}\n",
    "x \\quad x \\geqq 0 \\\\\n",
    "0 \\quad x < 0 \\\\\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "$x$ : ある特徴量。スカラー\n",
    "\n",
    "\n",
    "実装上はnp.maximumを使い配列に対してまとめて計算が可能です。\n",
    "\n",
    "\n",
    "numpy.maximum — NumPy v1.15 Manual\n",
    "\n",
    "\n",
    "一方、バックプロパゲーションのための $x$ に関する $f(x)$ の微分は以下のようになります。\n",
    "\n",
    "$$\n",
    "\\frac{∂f(x)}{∂x}=\n",
    "        \\begin{cases}\n",
    "        1 \\quad x \\geqq 0 \\\\\n",
    "        0 \\quad x ≦ 0 \\\\\n",
    "        \\end{cases}\n",
    "$$\n",
    "数学的には微分可能ではないですが、 $x=0$ のとき $0$ とすることで対応しています。\n",
    "\n",
    "\n",
    "フォワード時の $x$ の正負により、勾配を逆伝播するかどうかが決まるということになります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ゼロつくに乗っていたコード\n",
    "class Relu():\n",
    "    \"\"\"\n",
    "    Relu関数クラス\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.mask = None\n",
    "        \n",
    "    def forward(self, A):\n",
    "        self.mask = (A <= 0)\n",
    "        out = A.copy()\n",
    "        out[self.mask] = 0\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dz):\n",
    "        \"\"\"\n",
    "        Z:\n",
    "        y:\n",
    "        \n",
    "        \"\"\"\n",
    "        dz[self.mask] = 0\n",
    "        y = dz\n",
    "        \n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [1 2]\n",
      " [3 4]]\n",
      "[[0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [1 2]\n",
      " [3 4]]\n"
     ]
    }
   ],
   "source": [
    "# Relu関数テスト\n",
    "x = np.arange(-5, 5).reshape(5, 2)\n",
    "\n",
    "relu = Relu()\n",
    "\n",
    "print(relu.forward(x))\n",
    "print(relu.backward(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題6】重みの初期値\n",
    "ここまでは重みやバイアスの初期値は単純にガウス分布で、標準偏差をハイパーパラメータとして扱ってきました。しかし、どのような値にすると良いかが知られています。シグモイド関数やハイパボリックタンジェント関数のときは Xavierの初期値 （またはGlorotの初期値）、ReLUのときは Heの初期値 が使われます。\n",
    "\n",
    "\n",
    "XavierInitializerクラスと、HeInitializerクラスを作成してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XavierInitializer:\n",
    "    \"\"\"\n",
    "    Xavierの初期値作成クラス\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    def W(self, n_nodes1, n_nodes2):\n",
    "        \"\"\"\n",
    "        重みの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes1 : int\n",
    "          前の層のノード数\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        W :\n",
    "        \"\"\"\n",
    "        sigma = 1.0 / np.sqrt(n_nodes1)\n",
    "        W = sigma * np.random.randn(n_nodes1, n_nodes2)\n",
    "        \n",
    "        return W\n",
    "    \n",
    "    def B(self, n_nodes2):\n",
    "        \"\"\"\n",
    "        バイアスの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        B :\n",
    "        \"\"\"\n",
    "        B = np.zeros(n_nodes2)\n",
    "        \n",
    "        return B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 4.01223412e-02  1.31101722e-02 -5.83438206e-02  4.02056070e-03\n",
      "   6.45825912e-03  5.19062124e-02  3.12348545e-02  2.38177707e-02\n",
      "   1.26582698e-02  5.32423029e-02 -3.50719219e-02  2.77768627e-03\n",
      "   1.76726426e-02 -7.87127367e-02 -8.40602665e-02 -2.16449978e-02\n",
      "   1.91094162e-02 -2.34486089e-02 -6.84344726e-03 -9.22883069e-02\n",
      "  -3.85954370e-02 -1.84014565e-02  1.82137491e-02 -9.07312367e-04\n",
      "   1.76490597e-02  3.45256331e-02 -2.94719425e-02 -5.38285443e-02\n",
      "   1.10903297e-01  3.02015931e-02  4.36197305e-02  1.35285248e-02\n",
      "   1.54953592e-02  2.59149886e-02 -4.96562291e-02 -1.36633963e-03\n",
      "  -2.67570842e-02 -4.13701156e-02  2.49087207e-02 -1.05330449e-02\n",
      "  -4.78262347e-02  6.20123245e-02 -4.18351953e-02  1.04094807e-02\n",
      "   6.42046377e-02  2.56742975e-02 -4.35654879e-03  1.49132088e-02\n",
      "  -8.46293818e-02  5.11441046e-02  2.86798936e-02 -4.89827457e-02\n",
      "   8.94009002e-02  5.18512153e-03  4.52402157e-02  2.08890934e-02\n",
      "  -1.82258951e-02 -9.12744276e-02 -4.87677154e-02 -7.09222326e-02\n",
      "  -8.15071250e-03 -7.29015588e-02  8.14713927e-02 -2.82459717e-03\n",
      "   3.17593026e-05 -5.43028677e-02 -8.10139420e-02 -6.58493403e-02\n",
      "   4.89192186e-02 -8.43290712e-03  1.22869147e-02  4.58611385e-02\n",
      "  -6.31997035e-02  1.89232791e-02 -5.57232548e-02  1.02454523e-01\n",
      "  -5.87483888e-02  9.12090480e-03  2.47289029e-02  1.70131761e-02\n",
      "   7.30853554e-03 -2.69306984e-02  4.93064803e-02 -9.48385915e-02\n",
      "   6.19284901e-02 -5.12862431e-02 -7.30973898e-02  8.88267100e-02\n",
      "   3.71802830e-02 -2.40968826e-02 -3.75385460e-02  3.99372622e-02\n",
      "   6.15470854e-02 -4.06166290e-02  1.47455588e-02  6.86505486e-03\n",
      "   4.26233464e-03  9.01817060e-03  3.68026963e-02  3.77008550e-02\n",
      "   7.97327960e-03  2.60861873e-02  7.50327257e-03  1.92395563e-02\n",
      "  -1.93032783e-02  1.44157224e-02 -2.97959247e-03 -6.12943818e-02\n",
      "  -3.15207257e-03 -1.94787958e-02  7.14663618e-02 -3.26347734e-02\n",
      "  -4.45991003e-02  2.64701476e-02  3.09927639e-02 -3.38178928e-02\n",
      "  -7.22657438e-02 -3.57712747e-03  2.25674420e-02  3.28409697e-02\n",
      "  -1.52330476e-04  7.48801712e-02  2.48279752e-02 -1.88781482e-03\n",
      "  -8.86694726e-03  6.93055082e-03  2.57300774e-02 -2.38504123e-02\n",
      "  -1.49263962e-02  3.82515351e-02  1.04574271e-02  1.06424817e-01\n",
      "   3.97519922e-03 -1.61614429e-02  4.62185502e-02 -6.43432285e-02\n",
      "  -3.82598657e-02  3.83299482e-02 -2.27864483e-02  1.35272533e-02\n",
      "   3.54068890e-02  4.40258965e-02  6.05078490e-02  8.43709114e-02\n",
      "  -2.68436879e-02 -1.52221812e-02 -2.48627575e-02  3.99313550e-02\n",
      "  -2.73161511e-02  4.83950092e-02 -1.43823507e-02  8.42830726e-02\n",
      "  -7.67714026e-02  3.37213248e-02 -2.79995192e-02 -1.29472595e-01\n",
      "  -4.35024191e-02 -2.12722536e-02  8.98873751e-03  2.40961991e-02\n",
      "   6.89859110e-04  7.20060342e-02  4.75203409e-02 -2.62221600e-02\n",
      "   3.33405951e-03  9.75194543e-02  1.21400130e-02  2.32105959e-02\n",
      "  -1.14802299e-02  4.30316270e-02  7.42480922e-02  2.17551590e-02\n",
      "  -5.63672513e-02  2.63174060e-02 -9.51532993e-03 -1.03706859e-01\n",
      "  -7.51589458e-02 -3.56020674e-02  3.25055312e-02  1.47773644e-02\n",
      "   2.14867580e-02 -5.88662196e-02  1.40989590e-02  3.32283237e-02\n",
      "  -7.63206253e-02 -2.89473912e-02 -5.05595471e-03  6.81967098e-02\n",
      "   1.27648483e-02  1.94750236e-02 -9.98997487e-02  2.10991290e-02\n",
      "  -1.00011209e-02 -8.04670875e-02 -5.32725479e-02 -6.43767659e-02\n",
      "   1.15487538e-02 -1.97913793e-02  4.05038610e-02  4.68740031e-02]\n",
      " [-7.08145687e-02  5.53580081e-02  1.12849080e-01  3.21582617e-02\n",
      "  -6.86523338e-03 -2.18305034e-02 -5.93264508e-02 -1.73453499e-02\n",
      "   4.04833675e-02 -2.11571163e-02  9.62147567e-03  1.95261154e-02\n",
      "   1.40212358e-03 -3.90222278e-02 -2.42143025e-02  1.00684702e-02\n",
      "  -7.70626623e-03  5.22703703e-03 -2.25699372e-02 -5.46487860e-02\n",
      "   1.39883210e-02  2.13517358e-04  1.02442483e-01 -5.47299710e-02\n",
      "   1.12956366e-01  3.35301746e-02 -4.10725714e-03  4.16103141e-02\n",
      "   1.60487670e-02  9.94762847e-03  5.23112059e-02  1.18805714e-02\n",
      "  -6.81366065e-02 -2.24293003e-02  2.12082738e-02 -8.24930626e-03\n",
      "  -3.28237812e-02  4.26934381e-02  9.19805287e-02  1.05744879e-02\n",
      "   9.22744161e-02 -5.28746866e-02  3.20994562e-02  1.51021875e-02\n",
      "   1.04140232e-01 -7.55228134e-02  4.18773766e-02 -8.97816787e-03\n",
      "   1.03028115e-01 -2.94448857e-02  8.51552551e-02 -5.65238237e-02\n",
      "   7.68676329e-02 -8.38371821e-03  4.61774985e-02  1.13943505e-02\n",
      "  -1.85031501e-02  6.61867406e-02 -3.31954882e-02 -1.22069797e-02\n",
      "   4.62581781e-02 -8.29178879e-02 -4.82745418e-02  2.54219129e-02\n",
      "  -5.36148788e-02  2.32356184e-02  1.19927590e-02  4.15902351e-02\n",
      "   3.30493839e-03  3.95314718e-02 -1.12759395e-02  6.96370156e-03\n",
      "  -3.92980513e-02 -9.29075986e-03  3.47712430e-02 -4.61918998e-02\n",
      "  -2.54665903e-02  1.80636009e-02  1.01265207e-01 -8.13639273e-02\n",
      "   6.39291908e-02  1.79646739e-02 -6.16902775e-03  3.48450658e-03\n",
      "  -6.62074393e-02 -2.14129713e-02 -7.27436564e-02  1.06514801e-01\n",
      "   8.91061230e-02  8.36137376e-02  4.73961146e-02  2.21654001e-02\n",
      "   1.22854218e-02  6.07161190e-02 -1.57549531e-02 -1.64845428e-02\n",
      "  -7.62372922e-02 -4.58946376e-02  1.41936410e-04 -3.78681876e-02\n",
      "   6.82146688e-02  4.57750373e-02  2.35513122e-03 -9.87789822e-02\n",
      "   4.34262922e-02 -1.08839728e-02  3.65074608e-02  3.09579902e-02\n",
      "   2.21499604e-02  2.65917275e-02 -1.18672554e-01  1.21746476e-02\n",
      "   4.66158564e-03 -8.24001676e-04  4.42568486e-02  1.12195103e-02\n",
      "   1.08039606e-02  9.08679308e-03 -6.88205442e-02  7.67145103e-03\n",
      "   7.82305509e-02  1.11901726e-01 -6.22947986e-03  7.16009647e-03\n",
      "  -4.15660451e-02  9.69864615e-02  6.49062713e-02  7.66509643e-02\n",
      "  -5.25238949e-02  3.85765931e-02 -1.28104673e-02  8.51496624e-02\n",
      "  -3.17778275e-02  7.03148033e-03 -1.59757263e-02 -1.01283207e-02\n",
      "   3.01759565e-03  1.56307364e-02 -9.97354408e-03 -5.00387574e-03\n",
      "  -4.17805182e-03  7.74073789e-02 -2.08659190e-02 -5.06420973e-02\n",
      "   5.48512581e-02 -1.48653538e-02  2.41877147e-02  1.72086400e-02\n",
      "  -1.29826940e-02 -8.08391053e-02  1.68866341e-02  6.93941054e-03\n",
      "  -3.83986559e-03  3.17352684e-03  1.22804821e-01  2.72235847e-02\n",
      "   3.65532303e-02 -5.47570057e-02  3.78928310e-02  6.32953789e-03\n",
      "   1.86333720e-02 -1.79560273e-01 -3.09981856e-02  8.25420931e-03\n",
      "  -4.02256216e-02 -6.31827053e-02 -2.42375291e-02 -7.38981215e-02\n",
      "  -8.44998868e-02  1.41778380e-02  6.13776229e-02 -2.26887869e-02\n",
      "   2.56506646e-02  1.24596700e-02 -5.93463709e-02 -2.37252499e-02\n",
      "  -5.09924239e-02  6.08007244e-02  4.27993090e-03  2.53787347e-02\n",
      "  -1.95207275e-02 -4.57906566e-03 -1.16778734e-02  4.32990982e-02\n",
      "  -5.83078435e-02 -6.21960694e-02 -2.07416585e-02  4.78532264e-02\n",
      "  -5.04275409e-02  2.00433205e-02 -6.26620620e-02 -2.88816810e-02\n",
      "   7.50899099e-02 -2.96268817e-02  4.24520944e-02 -3.46283612e-02\n",
      "   2.41932785e-02 -8.28870934e-02 -1.11688894e-03 -4.91630305e-02]\n",
      " [-2.72437078e-02 -1.00659731e-02 -1.92461093e-02  7.03217934e-02\n",
      "   4.89739237e-03  2.41522624e-02 -8.74247078e-02  5.72623395e-02\n",
      "   8.84983658e-03  2.96416707e-02  3.14580855e-02 -3.84151192e-02\n",
      "  -1.25527079e-01 -7.99169866e-03 -3.69673616e-02 -5.22924213e-02\n",
      "   1.32326919e-02  3.08536390e-02 -2.25330525e-02  6.25628654e-02\n",
      "   7.76234981e-02 -1.13599936e-01  1.79832381e-01  3.73832575e-02\n",
      "  -7.00634756e-02  4.75726735e-02 -6.23833640e-02  1.73460473e-02\n",
      "   9.77173793e-02 -7.87331900e-03 -5.06455910e-03  3.75326144e-02\n",
      "   1.03720940e-03  4.33639416e-02 -3.89471535e-02 -7.80384757e-02\n",
      "  -2.89551937e-02 -1.32724718e-02  2.83983948e-02 -8.44757164e-02\n",
      "   6.05093640e-02 -1.11827492e-02  4.53313992e-03  2.93410356e-02\n",
      "   7.21854624e-02 -2.15002667e-02  3.02924995e-02 -3.03150393e-02\n",
      "  -4.03915588e-02 -1.18304244e-02  5.58711782e-02 -4.36691513e-02\n",
      "   1.10878955e-02 -7.29473576e-02  4.84430693e-02  1.05038424e-02\n",
      "   3.35288532e-02 -1.84944061e-02 -7.22940972e-02  8.08242268e-02\n",
      "  -3.15270281e-03  4.77444801e-02 -3.40805389e-02  2.80364952e-02\n",
      "   2.81938899e-02  1.08133796e-02  3.10217573e-02  7.81020176e-03\n",
      "   1.63220779e-03  1.30000516e-01 -2.02908936e-02 -6.29621111e-03\n",
      "  -1.26935613e-02  3.99393165e-02 -1.37373211e-02 -4.41693029e-02\n",
      "   5.35901194e-02 -6.13255018e-03  5.06710137e-04 -2.85735100e-02\n",
      "  -5.73439005e-02  1.41461601e-03  3.96632601e-02  4.45107574e-02\n",
      "  -1.17447698e-02 -4.79069701e-03 -1.03649763e-02  2.42054487e-02\n",
      "   7.74327301e-02  8.63588306e-03  3.00328865e-02  7.35173406e-02\n",
      "  -1.30530086e-01 -1.56491027e-02  8.89585853e-02 -1.42745624e-02\n",
      "  -6.57723915e-02  1.53512863e-02 -1.85613016e-02  9.13914729e-02\n",
      "   3.21576805e-02  3.10334436e-02  4.11577283e-02  5.21727409e-02\n",
      "  -2.19554412e-02 -6.65737712e-03  3.00042109e-02  3.25586664e-02\n",
      "  -1.39918350e-02 -6.44139799e-02  4.89166827e-02 -1.21932409e-02\n",
      "   2.58985253e-02  3.96011711e-04  5.99839434e-02  8.93773704e-02\n",
      "  -6.38601958e-02  1.93561437e-02  3.58093905e-02  9.44306485e-02\n",
      "   4.27660637e-02  5.68707278e-03 -8.70047486e-02  2.27773610e-02\n",
      "   6.69738046e-02  4.14537046e-02 -1.35792225e-02  6.46253146e-02\n",
      "  -5.68625653e-02  1.34992426e-02  1.47304771e-02  1.62931546e-01\n",
      "  -3.67138603e-02 -5.47550791e-02  5.26050417e-02  1.43096995e-02\n",
      "   3.47973691e-02  1.60698827e-02  7.09032106e-02 -9.32080838e-02\n",
      "  -6.87572298e-02  3.83672435e-02  1.80599065e-02  8.74966438e-02\n",
      "  -1.36967576e-01  1.20042621e-02 -1.89702511e-02  1.10615395e-01\n",
      "   5.38470031e-02  4.22823799e-02  1.17636513e-02  1.35846487e-02\n",
      "  -4.20075697e-02  5.41205803e-02  3.83917401e-02  5.04380138e-04\n",
      "   3.63367814e-03  5.81933882e-03 -1.51739458e-03  9.28215042e-03\n",
      "  -2.55772272e-02 -3.96576876e-02 -8.85312761e-02 -4.01393880e-02\n",
      "  -1.14519211e-02 -3.98382960e-02 -1.30385470e-02  4.19582488e-03\n",
      "   4.41205206e-02 -1.77474630e-02 -7.17901607e-02  1.03336182e-02\n",
      "  -2.85564722e-02 -2.92935894e-03  4.21420164e-02  2.41330607e-02\n",
      "  -1.19739392e-02 -2.41807994e-02 -1.59976637e-02  2.27982217e-02\n",
      "   4.99276137e-02 -7.69404824e-02 -8.79492518e-02  4.05150486e-02\n",
      "   8.67086607e-02 -3.25799624e-02  5.78356377e-03  7.00256744e-02\n",
      "   4.81470340e-02 -6.69605604e-02  2.74793481e-02 -2.53275219e-02\n",
      "  -4.44828066e-02  6.48140155e-02 -3.78975019e-02  1.00038343e-02\n",
      "  -7.48635595e-02 -2.57902831e-02  2.50251729e-02 -4.28454279e-02]\n",
      " [-1.21759017e-02  9.86403615e-03  3.67158686e-02 -8.97812632e-02\n",
      "  -1.73434151e-02  5.02127034e-02  1.17619267e-01  4.54018239e-02\n",
      "   3.45731586e-02  5.21169778e-02 -5.20286498e-02  5.17508904e-02\n",
      "  -2.12169609e-02  2.95042864e-02 -8.07194672e-02  1.61435675e-03\n",
      "   5.67774776e-02  1.81019046e-02  3.72897267e-02  1.68157886e-03\n",
      "  -1.01970348e-03  5.20681749e-02  1.37555711e-02  5.46316856e-02\n",
      "  -6.16462653e-02 -4.14482475e-02 -9.06925367e-02 -1.19865119e-03\n",
      "   1.78858268e-02  1.00604740e-01 -7.97189610e-02  2.32699974e-02\n",
      "  -9.69425708e-02 -4.58786776e-02 -3.83344021e-02  7.19116597e-03\n",
      "   4.26842574e-02 -1.30785984e-02  9.46549786e-02 -7.58107790e-02\n",
      "   6.98458562e-03  4.54889820e-03  4.48243110e-02  5.24428539e-02\n",
      "  -7.45182436e-03  7.59352572e-02 -8.18531414e-02  1.32326401e-02\n",
      "  -4.68959901e-02 -4.59266842e-02  5.52786235e-02  1.30558830e-01\n",
      "   4.70804974e-02 -9.38230285e-03 -6.00223611e-03  1.75658838e-02\n",
      "   5.48188711e-02  2.96282289e-02  2.00607999e-02  2.76390736e-02\n",
      "  -1.95678444e-02 -8.10441107e-03  5.96266853e-02 -5.72464434e-02\n",
      "   2.19354382e-03 -5.55590705e-02 -1.00828531e-03 -3.72326292e-02\n",
      "  -4.84821142e-02  3.32858002e-03  6.65669602e-02  4.03360175e-02\n",
      "  -7.14152615e-02  7.81573977e-03 -2.73167179e-02  9.02729033e-03\n",
      "   3.22383843e-02  4.93726285e-03  2.67801315e-02  8.77213863e-02\n",
      "  -6.94099151e-02 -2.46745569e-02 -7.24061382e-02 -3.94031651e-02\n",
      "   3.70095289e-02  3.10433059e-02  2.31398745e-02  9.70334304e-02\n",
      "   5.29900814e-03  5.15076563e-02  3.33233899e-02  1.15724045e-01\n",
      "   5.04412220e-03  4.12093406e-02  1.77675667e-02  1.36485598e-01\n",
      "  -7.55864146e-03 -8.84788552e-02 -9.58354169e-03  1.77725131e-03\n",
      "  -6.04502172e-02 -4.35568589e-02  9.76184631e-02 -3.79505669e-02\n",
      "  -8.33640834e-02  3.19458209e-02 -8.37722333e-02 -7.22572940e-03\n",
      "  -5.56556130e-03  1.10128939e-01  5.51812680e-02  7.83697245e-02\n",
      "   1.41602926e-01 -1.98698181e-02  4.21621434e-04  1.17331210e-01\n",
      "  -2.69282866e-03 -1.46291980e-02 -5.81400339e-02  3.19116119e-02\n",
      "   3.69935940e-02  8.75761970e-03  1.97877007e-02 -2.26686783e-02\n",
      "   4.65188694e-02 -1.24650484e-02  3.77315672e-02  1.05764364e-02\n",
      "   8.60226055e-03  1.50980948e-03 -2.81617156e-02  2.27385687e-02\n",
      "   2.49828997e-03  2.19942768e-02 -5.05673971e-02 -5.04005631e-02\n",
      "  -6.23169298e-02  5.29239679e-02 -7.79348526e-02 -3.27200756e-02\n",
      "  -3.57714432e-03  5.96901984e-03  1.19803568e-01 -7.83986217e-02\n",
      "  -2.86756969e-02 -2.34726343e-02  3.43622768e-02  6.78628950e-02\n",
      "  -6.04654194e-02  2.10318505e-02 -4.23821287e-02  1.05444980e-01\n",
      "   8.43383760e-03  8.38062585e-03 -8.91408938e-03 -4.24975091e-02\n",
      "  -1.21917910e-02  2.67458676e-02  6.70631921e-03 -1.05312148e-01\n",
      "   5.72594350e-02 -1.27040964e-02 -2.10349258e-02 -5.89281017e-02\n",
      "  -4.49557921e-02 -6.07266653e-02  1.05491116e-01  2.21979709e-02\n",
      "   4.14316482e-02 -1.31508937e-01 -3.88669390e-03  2.72018469e-02\n",
      "   1.08235193e-01 -6.17401499e-02  8.45575710e-03 -2.14143909e-02\n",
      "  -4.61280593e-02  2.38790136e-02 -3.93544165e-02  8.13106828e-02\n",
      "  -1.11613292e-01  3.29201867e-02 -3.73545861e-02  3.98300768e-03\n",
      "  -2.80275722e-02  1.16608642e-01  1.77487207e-02 -1.76115880e-02\n",
      "   2.93928344e-02 -2.93477803e-02  7.15328897e-02 -1.71042889e-02\n",
      "   3.34064640e-02  4.56745079e-02  8.51767306e-02 -8.33978827e-02\n",
      "   8.69621472e-04 -4.53915850e-02  1.94816240e-02 -1.12268412e-02]\n",
      " [-8.82474235e-03 -4.61952696e-02  7.94859260e-02  5.56044309e-02\n",
      "   7.31389392e-02  5.84115697e-02 -9.28642774e-02 -3.08556809e-02\n",
      "   6.81548802e-02  6.99064323e-03 -9.30875769e-02  5.56287115e-02\n",
      "  -5.92885526e-02  2.40470745e-02  2.37656639e-02  1.65538612e-02\n",
      "   2.87328682e-02 -7.29098255e-04 -7.25062986e-02  1.04281961e-01\n",
      "   1.55849166e-02  1.93739884e-02 -1.20069144e-02  1.90095041e-02\n",
      "  -5.55082416e-03 -3.43103878e-02  2.86973418e-02 -5.85759965e-02\n",
      "  -1.62824076e-02 -4.70646478e-02 -2.23944390e-02  3.67174346e-02\n",
      "  -2.18115303e-03 -4.31953741e-03  7.51434706e-03  8.94015008e-02\n",
      "  -8.02736365e-02  2.17157466e-03 -1.13027275e-01 -2.97506745e-02\n",
      "  -4.84348876e-02 -6.62883768e-02 -3.18177688e-02  1.30536116e-02\n",
      "  -6.85753196e-04 -1.86651548e-02 -3.70541827e-02  4.72505004e-02\n",
      "   1.24279481e-02 -1.62312271e-02  3.53099389e-02  5.63263911e-02\n",
      "   1.60121830e-03 -2.81177746e-02  2.40028806e-02 -1.53577309e-02\n",
      "  -6.80511584e-03 -2.15845904e-02 -1.42635802e-04 -1.69084585e-02\n",
      "  -1.11761351e-01 -3.27480203e-02 -4.69260511e-02  3.63631114e-02\n",
      "  -8.83411184e-02 -4.94318561e-02  4.83240701e-03  4.52274446e-02\n",
      "  -4.05632543e-02  5.98998189e-02 -3.77613324e-02 -1.19373848e-02\n",
      "   2.44885654e-03 -6.60210374e-02 -1.56065384e-02 -2.27225920e-02\n",
      "  -6.69260197e-02 -5.24308058e-02  5.09259700e-02 -6.13227685e-03\n",
      "   4.23312631e-02  2.09144106e-02  9.15199332e-02 -9.99076675e-02\n",
      "  -3.60568095e-02  2.75296132e-02  4.44444426e-02  1.02242513e-02\n",
      "  -3.10460116e-02  2.51013916e-02  4.24992547e-03  9.91140670e-03\n",
      "  -9.10966421e-02 -5.73257827e-02  1.08192197e-02 -5.38920176e-02\n",
      "  -3.31942586e-03  2.87809653e-02  8.16554727e-02 -8.26949669e-02\n",
      "  -3.37661796e-02 -2.94296756e-03  2.38253499e-04  4.84570584e-02\n",
      "  -5.09342466e-02 -1.04970102e-01  5.20357962e-03 -3.39781246e-02\n",
      "  -8.66840637e-02  2.09532805e-02  2.38720621e-02  3.71435751e-02\n",
      "  -1.17651888e-02  1.79571383e-02  6.25823609e-02  7.18427626e-02\n",
      "   7.72143185e-02  8.19112747e-02 -3.46492349e-02 -1.10696757e-02\n",
      "  -5.27124901e-02  3.63806883e-02  5.33252857e-02 -2.74739036e-02\n",
      "   3.58096163e-03 -3.84241220e-02  3.97488613e-02  3.75418104e-02\n",
      "  -4.72992582e-03 -7.60358440e-03  2.68333430e-02 -2.89533511e-03\n",
      "  -1.50273848e-02  1.01108016e-02 -1.47913865e-02  5.17180316e-02\n",
      "   1.72003023e-02 -1.34553414e-01  2.46318456e-02  7.34763920e-02\n",
      "   2.64783805e-02 -3.26230023e-02 -3.11157137e-03  5.82191636e-02\n",
      "   1.49749239e-02  3.74756532e-02 -7.03702672e-02  4.17626314e-02\n",
      "   6.05136576e-02  6.15994913e-02 -8.23041070e-02  9.47389135e-03\n",
      "   1.06927841e-02 -2.86283191e-02 -5.24128484e-02 -4.04259294e-02\n",
      "   8.10004066e-02 -4.52394881e-02  6.08214688e-03 -1.32348268e-02\n",
      "  -5.82132197e-02 -1.43393166e-02 -1.07687422e-02  2.34871226e-02\n",
      "   2.45560798e-02 -5.74501474e-02 -3.17678671e-02  3.44524100e-02\n",
      "  -2.70959527e-02  3.45756029e-02 -3.39781207e-02 -9.58974907e-03\n",
      "   7.11101697e-02  6.33555172e-02 -1.36257595e-02  2.88806628e-03\n",
      "  -4.47148440e-02 -1.67693651e-02 -4.89658763e-02  6.84630933e-03\n",
      "   1.80940627e-02 -1.63497242e-02  1.80189072e-02  4.45140924e-02\n",
      "  -2.98711097e-02 -5.18428424e-02 -8.23487300e-02 -1.11140205e-01\n",
      "  -3.84002758e-03 -3.86064224e-02  5.68980797e-02 -2.89280296e-02\n",
      "  -2.29636738e-02 -4.73563141e-02 -2.44374821e-02 -2.31936681e-02\n",
      "  -8.10581211e-02 -3.22854892e-02 -6.03114699e-02  4.11001648e-03]]\n",
      "[0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# XavierInitializerテスト\n",
    "xavier = XavierInitializer()\n",
    "\n",
    "print(xavier.W(400, 200)[:5])\n",
    "print(xavier.B(200)[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeInitializer:\n",
    "    \"\"\"\n",
    "    Heの初期値作成クラス\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    def W(self, n_nodes1, n_nodes2):\n",
    "        \"\"\"\n",
    "        重みの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes1 : int\n",
    "          前の層のノード数\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        W :\n",
    "        \"\"\"\n",
    "        sigma = np.sqrt(2.0 / n_nodes1)\n",
    "        W = sigma * np.random.randn(n_nodes1, n_nodes2)\n",
    "        \n",
    "        return W\n",
    "    \n",
    "    def B(self, n_nodes2):\n",
    "        \"\"\"\n",
    "        バイアスの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        B :\n",
    "        \"\"\"\n",
    "        B = np.random.randn(n_nodes2)\n",
    "        \n",
    "        return B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-6.65999505e-02 -7.67932960e-02 -3.02925660e-02  6.04689464e-02\n",
      "   7.81633108e-02 -9.68501634e-02 -5.27337797e-03  1.74269566e-02\n",
      "  -6.73547338e-02 -1.03511135e-02 -2.29523372e-02 -2.35166959e-02\n",
      "  -6.15275312e-02 -8.63689134e-02 -5.21532335e-02  1.20696950e-02\n",
      "   2.06782655e-02  6.92339905e-02  1.39884415e-02  9.18877745e-02\n",
      "  -9.38455443e-02  5.79466490e-02  1.57891774e-02 -4.09491256e-02\n",
      "   1.57222041e-01  6.64037393e-02 -2.09892769e-02 -5.48430725e-02\n",
      "  -7.82672015e-02  7.49596348e-02 -4.34586164e-02  1.00010312e-01\n",
      "  -1.06263495e-01  2.73673595e-02 -1.15711077e-01 -1.03629822e-01\n",
      "   3.70451748e-02  3.57608900e-03 -3.67316875e-02  2.91219464e-02\n",
      "   5.60953417e-02 -3.40847416e-02 -6.25597370e-02  3.57895290e-02\n",
      "  -1.47306220e-01  6.53370926e-02  7.45863689e-02 -4.03717573e-02\n",
      "  -2.18725998e-02 -1.10601513e-01  7.86030944e-02  5.17293203e-02\n",
      "  -4.67512926e-02 -5.74636697e-02 -1.25157352e-02 -4.27167040e-02\n",
      "   1.52242900e-02  9.82782261e-03  3.73487913e-02  1.96808276e-02\n",
      "  -3.12745401e-02  6.44680920e-03 -7.13986451e-02  5.58796028e-02\n",
      "   6.89570227e-03  2.10384565e-02 -1.01356005e-03  8.27703134e-02\n",
      "  -1.11919350e-01 -9.80118855e-02  1.87968653e-02 -6.16060661e-02\n",
      "  -2.08422925e-02  3.46936597e-02 -1.23489490e-01  1.28812753e-03\n",
      "  -1.84584468e-02  2.96367262e-02 -7.26269776e-03  5.51922603e-02\n",
      "   8.98526229e-02  3.64959291e-02 -6.24875014e-02 -7.32424903e-02\n",
      "   4.26821264e-02  6.29204717e-02  2.47167029e-02 -1.76870219e-02\n",
      "   1.08330079e-01  1.40459975e-02  1.08978069e-01 -5.00957872e-02\n",
      "  -1.22793482e-01  1.00850836e-01  2.39107825e-02 -1.37143876e-02\n",
      "   8.72294395e-02 -1.68780032e-01 -1.89748590e-01 -8.12024413e-02\n",
      "  -6.20560957e-02 -6.83255809e-02  7.84569228e-03 -2.01793163e-02\n",
      "   1.55267523e-01  1.25628992e-01  1.60415886e-01  4.93113044e-04\n",
      "   4.57077996e-02 -2.93852753e-02 -6.06845757e-03  3.92181279e-02\n",
      "   6.66709313e-03 -8.46061734e-02 -8.66257886e-03 -1.75246032e-02\n",
      "  -6.71044750e-02 -9.98657413e-02 -6.01335316e-02 -1.37880939e-01\n",
      "   1.41106257e-01 -1.12303707e-01  3.15345266e-02 -1.25973863e-01\n",
      "  -1.66969483e-01  1.64999738e-02  1.12353939e-01 -1.69118913e-01\n",
      "  -6.34256240e-02 -1.28153931e-01  3.65268187e-02  1.21285092e-02\n",
      "   2.57321433e-02  1.44382331e-01  4.26858881e-02 -4.85981540e-02\n",
      "  -1.06280114e-01 -1.58561735e-01  1.65566238e-02  9.38175515e-02\n",
      "   1.02276174e-01  5.60889582e-02 -3.72086393e-02 -4.58735775e-02\n",
      "   1.24475683e-01  3.00840345e-03  5.32092962e-02  4.21054884e-02\n",
      "  -6.59507720e-02 -9.02266558e-02 -5.48127997e-02 -9.85065042e-02\n",
      "   2.46774024e-03 -2.78164809e-02  1.01249411e-01  6.31495361e-02\n",
      "  -4.00308401e-02 -4.93182754e-02  5.87546748e-02 -4.55247381e-02\n",
      "   1.25059350e-01  2.83264312e-02  2.71284142e-02 -7.40336342e-02\n",
      "  -2.51686522e-02  2.72239616e-02  8.90583746e-02  5.59091483e-04\n",
      "   1.36099907e-03  2.68122868e-02  3.83383663e-02  9.71137459e-02\n",
      "  -2.58712176e-02 -1.03486014e-01  8.79560469e-03 -1.13818946e-01\n",
      "   6.30193625e-02 -7.36651747e-02 -8.60818961e-02  7.95737717e-02\n",
      "  -4.46322227e-02 -5.82205588e-03  4.41340550e-02 -5.52148226e-02\n",
      "   1.59191660e-01  1.68398031e-01 -7.78132169e-02 -1.41857047e-01\n",
      "   3.27315378e-02  3.58382651e-02  2.22533029e-03 -7.03174464e-02\n",
      "  -3.91624558e-02 -9.35891953e-02  3.19035081e-02 -8.70817397e-02\n",
      "   5.40130276e-02 -1.48346820e-02  8.16151242e-02  9.85913174e-02]\n",
      " [ 4.28348954e-02  8.23458016e-02 -1.31315800e-02  6.50127603e-02\n",
      "  -5.41976175e-02  3.57250520e-02  1.15566470e-01 -4.27752531e-02\n",
      "   6.03673157e-02  9.26324604e-02 -3.00462694e-02  3.87509750e-02\n",
      "   3.94262965e-02 -5.35602426e-02 -9.21204715e-02 -5.81705714e-02\n",
      "   5.37250580e-02 -8.94135410e-02  3.01967351e-02  3.79087770e-02\n",
      "  -1.22944634e-02  9.57674136e-03 -8.26850053e-02 -1.47227424e-01\n",
      "   4.67905068e-02  1.19751507e-02 -9.79795348e-03 -7.38661061e-03\n",
      "   6.94739052e-03 -6.87455454e-02 -3.84531114e-03 -1.41497055e-02\n",
      "   2.94346994e-02  4.88762983e-02 -1.72555820e-01 -4.80978662e-02\n",
      "  -2.57264697e-02  2.45226546e-02 -5.57430819e-02  4.65510391e-02\n",
      "  -7.61399785e-02  1.10735143e-02 -1.94240147e-02 -2.64675983e-02\n",
      "   4.26838320e-02  9.22025834e-02 -2.99590913e-02 -8.94354572e-02\n",
      "   4.85881579e-03  9.72450836e-03 -1.42832501e-02 -6.38886089e-02\n",
      "  -1.75243582e-01  5.23295979e-02  1.52884428e-01 -5.06886558e-02\n",
      "  -1.34234340e-02 -4.68701670e-03  8.14227708e-03 -4.81950904e-02\n",
      "   1.81439444e-01 -3.68792774e-02 -2.56790998e-02  2.37137588e-02\n",
      "   1.06001028e-01  4.56666121e-02 -4.64664733e-02  7.25034716e-03\n",
      "  -1.82893556e-03  1.16762860e-02 -7.58806308e-03  7.30809869e-02\n",
      "   8.28977356e-02 -1.24548453e-01 -4.95906121e-03 -5.25921631e-02\n",
      "  -2.62125823e-02  9.91798518e-02 -6.68465250e-02 -4.46739473e-03\n",
      "   8.72693574e-02 -4.85735724e-02  2.70495045e-02 -7.66371621e-02\n",
      "   5.74897707e-02 -9.73475311e-02 -1.94076569e-03 -1.00694027e-01\n",
      "   6.33871327e-02 -1.44548540e-02 -5.06913052e-02 -1.47994361e-01\n",
      "  -2.03807848e-01 -9.33669485e-02 -5.54983818e-02  1.45348618e-01\n",
      "   2.75434095e-02  6.54014831e-02  6.63001274e-02  4.70248612e-02\n",
      "   6.66168838e-02 -3.35452097e-02 -1.28091926e-01  9.20877485e-02\n",
      "   8.09510168e-02 -9.42527943e-03 -1.11071181e-02  5.86067641e-02\n",
      "   6.94612687e-02 -1.06672747e-01 -2.13683185e-02 -2.39148181e-02\n",
      "  -5.01552312e-02 -7.53039126e-02 -3.67109997e-02 -2.86793809e-02\n",
      "   5.86189444e-02  5.09796699e-02  2.37118768e-02  6.24374473e-02\n",
      "  -3.69705020e-02 -1.72803920e-02 -7.85493723e-03 -1.11417317e-02\n",
      "  -2.37959735e-03 -3.96344557e-02 -1.04128704e-01 -6.78434497e-03\n",
      "   5.61297788e-03  1.51836535e-01  2.69232898e-02  5.61416055e-03\n",
      "  -2.09633865e-01  9.24320281e-02  1.16141004e-02 -1.05026365e-01\n",
      "  -5.33585196e-02 -4.47584981e-02 -6.02832939e-02  6.47351180e-02\n",
      "  -9.73193514e-03  5.46791292e-02  6.23030747e-02  1.07835711e-02\n",
      "   2.96457221e-02 -2.47708747e-02 -8.55974970e-02 -7.01680788e-03\n",
      "  -1.85020331e-02 -1.04549513e-01 -1.96902243e-02 -3.02587874e-02\n",
      "  -6.82553666e-02 -1.31045288e-01  1.41870201e-01 -1.42242561e-01\n",
      "  -6.30116855e-02  1.64208335e-01 -5.86316850e-02 -7.55818739e-02\n",
      "   1.55383268e-01  5.15449995e-02 -8.78459758e-02  2.59142348e-02\n",
      "  -1.47259562e-01 -8.40318245e-03 -4.97328501e-02 -1.66340005e-01\n",
      "  -2.61067862e-02  5.88394390e-02  1.43564638e-01 -4.00511749e-02\n",
      "  -9.62148152e-03  1.24684508e-01 -4.70902253e-03  7.39153050e-02\n",
      "   1.48438019e-01  2.85552254e-02 -6.97191247e-02  1.25369854e-01\n",
      "  -4.67212647e-02  1.08345011e-01 -4.30120699e-02  4.73599357e-02\n",
      "   2.38492683e-02 -4.55252108e-02 -5.81057120e-02 -4.55302664e-03\n",
      "  -6.70275038e-02 -7.04688614e-02 -3.66671018e-02 -6.20605940e-03\n",
      "  -3.80782317e-02 -8.17687606e-02  8.48813953e-02 -3.24882832e-02\n",
      "  -3.02052908e-01  3.67657441e-02 -5.76789838e-02  1.15043149e-01]\n",
      " [ 7.13466334e-03 -2.21117994e-02 -1.39667633e-02  7.88363927e-02\n",
      "   1.11376406e-01  6.47635932e-02 -2.47004566e-02 -6.44826496e-02\n",
      "  -1.07764512e-02  2.50086313e-02 -3.84202679e-02 -2.99884790e-02\n",
      "   3.18518247e-02 -3.09959599e-02 -5.84809181e-02  2.58748358e-02\n",
      "   2.64337029e-02 -6.79660870e-02 -1.25122224e-02  9.64591043e-02\n",
      "   9.83469259e-02 -5.83048722e-03  6.70640400e-03  6.19012671e-02\n",
      "   3.42796466e-02  2.82189376e-02 -7.35601225e-02 -5.31162838e-02\n",
      "   8.19735430e-03 -6.80129853e-02 -2.09726800e-02  1.55414031e-03\n",
      "   2.15120897e-02  4.28778668e-02  1.55477357e-04  1.85284780e-02\n",
      "   5.42987746e-02 -4.20956036e-02  3.69623702e-02 -6.90983951e-02\n",
      "   3.21507558e-02  9.68800317e-02  9.97572236e-02  4.20934639e-02\n",
      "   6.63331138e-02  2.23874525e-05  1.34586294e-03  4.13436404e-02\n",
      "  -2.50688118e-02  1.57669061e-01 -1.83021728e-03  3.79806159e-02\n",
      "   2.70746891e-02  4.76621305e-02 -8.51249067e-02 -1.49721215e-01\n",
      "  -1.00006162e-01 -5.47876775e-02  8.44970899e-02 -3.89361193e-02\n",
      "   2.71375342e-02  7.88669296e-03  3.74784787e-02 -1.14136282e-01\n",
      "  -5.02190709e-02 -6.20098046e-02  5.00354399e-02 -3.08399509e-02\n",
      "   7.53288151e-02  8.07835563e-02  7.09363695e-02  5.05156993e-02\n",
      "  -1.18508393e-04 -5.93910701e-02 -1.27116437e-01  7.09647269e-03\n",
      "   1.29754071e-01 -1.26683457e-02 -1.92727267e-02 -9.61136625e-02\n",
      "  -2.01045072e-02  7.42754428e-02  3.88533688e-02  1.21701727e-02\n",
      "   2.22331572e-02 -4.15535002e-02 -1.00965572e-01  3.27109440e-02\n",
      "   1.78585114e-01 -1.17319590e-01  8.51978159e-03  2.25470134e-02\n",
      "  -8.05129763e-03 -6.06890493e-02  4.69307691e-02  4.42084445e-02\n",
      "  -5.76475617e-02 -5.21672591e-02 -1.48402912e-02  6.04800451e-02\n",
      "   7.65881204e-03  2.35369916e-02 -1.02868293e-01 -6.86258102e-02\n",
      "   1.19088680e-01 -6.35114498e-02  1.07643556e-01 -8.06978518e-03\n",
      "  -4.65934220e-02 -2.52371141e-02  2.00827646e-02 -8.71703739e-02\n",
      "   1.76984148e-02 -7.45808736e-02 -9.23735673e-02  6.84696598e-02\n",
      "   1.23321927e-02  6.34678202e-02 -7.28896598e-02  5.04909628e-02\n",
      "  -1.27364214e-02 -7.56338879e-02  6.10312816e-02  8.69207062e-02\n",
      "   4.85133646e-02  3.95107241e-02 -5.80840429e-02 -4.66699211e-02\n",
      "   4.53759264e-02 -1.33196786e-01 -2.57382627e-02  1.60807733e-02\n",
      "  -1.43310612e-01 -1.01091427e-02  2.21539044e-03  5.24197466e-03\n",
      "  -4.68291777e-02 -2.07840836e-02 -1.84860743e-03 -1.02810489e-01\n",
      "  -1.57181783e-02  2.02206027e-02 -1.42651468e-02 -3.06282932e-02\n",
      "   4.19010638e-02 -3.29673083e-02 -5.89201797e-02 -1.22036702e-01\n",
      "  -1.93127349e-02 -3.48723403e-02 -7.02547773e-02  7.66293561e-02\n",
      "   4.60556625e-02 -1.77613987e-02 -1.66930350e-01  1.49845947e-01\n",
      "   1.01937844e-02 -2.09689778e-02 -8.20394398e-02  1.16238754e-02\n",
      "  -3.41574820e-02  7.47824286e-02  2.40015925e-03 -8.36557781e-02\n",
      "  -4.14557023e-02  1.06960740e-02 -1.58776230e-02  6.97287355e-02\n",
      "  -1.61501028e-01  8.27857797e-02 -8.39239866e-02 -9.54216985e-02\n",
      "   3.40536008e-02 -4.77331267e-02 -6.81840680e-02  1.63648290e-01\n",
      "  -1.56097965e-02  1.27054489e-01  1.06027259e-01 -1.14679861e-01\n",
      "   8.63010620e-02 -1.94930674e-01 -1.26126433e-01 -1.69626562e-02\n",
      "  -1.24792956e-01 -3.78464134e-02  4.10224456e-02 -6.63688643e-04\n",
      "  -1.00590894e-01  4.64957684e-02 -2.62232843e-05 -4.26699106e-02\n",
      "  -2.21980218e-02  2.83716716e-03 -2.57647761e-02  5.19167734e-02\n",
      "   1.23220077e-02  1.01147984e-03 -6.42280331e-02 -3.45373751e-02]\n",
      " [ 4.98067499e-02  2.87416358e-02 -1.66674617e-02 -6.29750935e-02\n",
      "  -4.25272963e-02 -5.49348140e-02 -1.24078598e-01 -1.18856285e-01\n",
      "  -6.57327123e-02  6.39896469e-02 -2.83291903e-02 -6.91368516e-02\n",
      "  -2.20929489e-02  8.88706558e-02 -3.71786224e-02  9.37809602e-02\n",
      "   3.10204477e-02 -2.05324568e-02 -2.40579127e-02 -4.95513427e-03\n",
      "   2.24572696e-02 -7.13330992e-02 -7.71965031e-02  7.02341272e-02\n",
      "   6.32578887e-02  5.38046922e-02 -6.78555851e-02 -3.91779629e-02\n",
      "   1.65113680e-01  2.16267400e-02  1.46304089e-02 -7.34704270e-02\n",
      "  -8.37404322e-03 -1.56903612e-02 -2.09137979e-02 -1.05648508e-01\n",
      "  -4.78877246e-03  6.23640319e-02 -7.27236117e-02  7.78906426e-02\n",
      "   3.79251487e-02 -8.16667936e-02 -7.08161530e-03  7.00430154e-02\n",
      "   8.59273811e-02  1.41607084e-01  4.23165099e-02 -6.40047702e-02\n",
      "  -1.68569926e-01  2.69175974e-02  8.24085991e-03 -5.07675910e-02\n",
      "  -2.70460320e-02 -3.50560091e-02 -2.17083655e-02  4.99554146e-02\n",
      "   8.79416451e-02 -4.62739371e-02 -1.25988206e-01 -9.40866731e-03\n",
      "   5.45419274e-02  1.15722835e-01 -1.35330700e-02  1.16568216e-01\n",
      "   1.05082382e-02  3.16939279e-02  1.61162252e-01 -2.50132377e-03\n",
      "   1.52121242e-02 -3.52841897e-02  1.46981582e-02  1.36393106e-01\n",
      "   1.18328466e-03 -6.20956162e-02  1.30873538e-01  3.25763595e-03\n",
      "   2.61063207e-02  1.37911827e-01 -1.41242515e-02  1.84189601e-03\n",
      "  -2.82382442e-02 -5.50128222e-03 -4.81506528e-03  2.21352440e-02\n",
      "  -1.54704105e-02  3.27315346e-02  5.70685101e-02 -2.04141447e-03\n",
      "   1.36707425e-01 -1.61021518e-02  2.97507120e-02  1.05905820e-01\n",
      "   4.12856631e-03  3.34596325e-02 -2.02338781e-02 -5.96858877e-02\n",
      "   6.73234483e-02  1.05049457e-01 -1.13628501e-01  5.25437474e-02\n",
      "  -1.26553287e-01 -2.44488872e-02  2.10748391e-02  5.53547180e-02\n",
      "   1.07981367e-01 -1.00332181e-01  3.35250224e-02 -3.62408008e-02\n",
      "   3.90970072e-02  2.04584112e-02  2.87786433e-02  9.19954655e-02\n",
      "  -4.51943242e-02  6.58052346e-02  4.45876847e-02  3.75102937e-02\n",
      "   8.20659017e-02 -1.76252704e-02  9.43835085e-02 -1.29184967e-01\n",
      "  -4.24550383e-02  7.76489490e-02  1.76820033e-02 -1.78177549e-01\n",
      "   7.37104295e-02 -1.37995538e-01 -8.01020552e-02 -1.44311460e-01\n",
      "   6.36407144e-04  8.74230840e-02 -8.52969237e-02  7.39243354e-02\n",
      "   2.94798975e-02  1.38333889e-01 -3.29618850e-03  1.58117323e-02\n",
      "   7.10788573e-02 -1.02636128e-01  1.45075504e-01 -8.65616039e-02\n",
      "   5.50793644e-02  2.36428296e-02 -1.10905920e-01 -4.24012098e-02\n",
      "  -5.76889718e-02  3.47895289e-02 -3.91108144e-02  1.09135599e-02\n",
      "   3.66011374e-03  4.12143417e-02  1.03797827e-01 -1.08774245e-02\n",
      "   3.49875358e-02  2.25152930e-03 -5.50095684e-03  4.11552880e-02\n",
      "  -4.02817443e-03  4.51359153e-03 -4.34148290e-02 -4.38675552e-02\n",
      "  -3.30826356e-02  1.02013398e-01 -1.22711323e-01 -1.19870858e-01\n",
      "   7.56517895e-02  8.94269346e-02 -1.11632348e-01 -2.27399280e-02\n",
      "  -1.39311288e-02 -8.45356231e-02 -1.35212412e-02 -1.03791926e-02\n",
      "   6.48143689e-02 -4.61123447e-02  5.78902082e-02  9.81616977e-02\n",
      "   8.34835639e-02 -8.06131425e-02 -5.23144979e-02  3.69259392e-02\n",
      "   6.83532748e-02 -3.21225399e-02  1.24711700e-02  8.11134764e-02\n",
      "   4.60904717e-02 -5.83796089e-02 -4.82200234e-02  1.17760242e-01\n",
      "  -1.21127519e-01 -4.16539156e-02  1.12492199e-02  1.47264126e-02\n",
      "  -5.79333289e-02 -2.36123855e-02  1.17967316e-01 -1.00969247e-01\n",
      "   2.66350504e-02  5.31658026e-02 -1.46169136e-01 -4.09575811e-03]\n",
      " [-5.65145560e-02 -6.57270937e-03  3.15096400e-02  2.25772229e-02\n",
      "   4.45567526e-02 -1.43812638e-02  3.71865940e-02 -8.76694472e-02\n",
      "  -3.72254142e-02 -6.20938514e-03  3.81861814e-02  5.99346631e-02\n",
      "  -8.04553258e-02  1.94388224e-02 -3.29135479e-02  1.08524579e-01\n",
      "   4.68288304e-04 -6.19788568e-02 -9.08973739e-03 -1.21648888e-01\n",
      "   1.09926608e-01 -6.43335989e-03 -7.13021826e-02 -7.79649244e-02\n",
      "  -1.51380427e-02 -2.08905251e-02  1.02351774e-02  6.80640427e-02\n",
      "  -6.75817427e-02 -3.42706759e-02 -7.75440074e-02  2.05831290e-02\n",
      "  -4.16559396e-02 -4.56755109e-03  8.07566385e-02 -1.17750017e-03\n",
      "   6.50311308e-02 -9.74857082e-02 -4.78010338e-02 -2.73412557e-02\n",
      "   4.04178759e-02  1.91029086e-02  1.45415148e-01 -6.56718025e-02\n",
      "   5.37425428e-02  1.15858015e-01 -1.02602093e-01 -1.04076138e-01\n",
      "   8.30726292e-02 -1.78767967e-01  1.85123652e-01  9.30459572e-02\n",
      "  -2.86984895e-02  1.20272328e-01 -2.05497736e-02 -5.56346601e-02\n",
      "   2.27308154e-02  3.70334943e-02 -6.29874364e-02  1.16171848e-02\n",
      "  -7.89044147e-02 -5.71212321e-02 -9.41993861e-02 -1.07783431e-01\n",
      "   5.29660334e-02 -7.33123279e-02  3.53768217e-02 -7.76341446e-02\n",
      "   6.33854052e-02  1.18215910e-01 -1.11206692e-01  4.45377572e-02\n",
      "  -3.85275520e-03  1.10215077e-01  4.36175442e-02 -3.74036157e-02\n",
      "   1.49609069e-02 -1.02501293e-01 -1.57394429e-02  1.02814936e-02\n",
      "  -1.72331479e-02  2.15532841e-02  3.08392644e-02  3.02800246e-02\n",
      "  -3.00329327e-02  1.83498294e-02 -2.22358510e-02 -8.94166920e-02\n",
      "   3.65785630e-02 -4.51118711e-02 -6.85059123e-03 -5.09348508e-02\n",
      "   6.11137824e-02  8.54728788e-03  7.92877592e-02 -1.12703319e-01\n",
      "  -1.87303201e-02  1.80865054e-01  3.40684069e-02 -7.99887031e-02\n",
      "   1.00631724e-02 -3.02754340e-04 -2.26363517e-02  6.02393803e-03\n",
      "   9.33581682e-02 -7.07174175e-02  1.50766740e-02 -4.34847250e-02\n",
      "  -1.68429004e-02 -5.63846956e-03  1.24282068e-02 -2.20449377e-02\n",
      "  -2.70664657e-03 -5.98933837e-02  6.32531711e-02  5.08227057e-02\n",
      "   5.97467912e-02 -1.07796550e-01  5.28190972e-02 -2.70827000e-02\n",
      "   4.06209655e-02 -7.30849586e-02  1.04849998e-01 -1.19688553e-01\n",
      "   3.94339043e-02 -2.24567541e-02  1.59237094e-02  4.86930556e-02\n",
      "  -1.19186537e-01  1.90697272e-02  3.13839006e-03  8.70805133e-03\n",
      "   3.16749570e-02 -1.88856818e-02  5.12888340e-03  6.11759375e-02\n",
      "  -3.44266215e-02  4.05070601e-02 -3.07964757e-02  6.32023809e-02\n",
      "   2.49592156e-02 -2.28917881e-02 -9.33140538e-02  3.35533417e-02\n",
      "  -7.59080601e-03 -6.13067663e-03  1.71882510e-02 -1.03216491e-01\n",
      "  -6.00383765e-04 -2.88535404e-02 -1.04324846e-01 -2.13851164e-01\n",
      "  -3.11427900e-02 -5.40880960e-02  3.84549836e-02 -8.10642709e-02\n",
      "   2.41868101e-02 -1.23471780e-01  3.10815060e-02  3.51868897e-02\n",
      "  -1.38945655e-02  1.10445064e-02 -3.50036151e-02 -9.57010927e-02\n",
      "   1.02487074e-01  2.18950933e-02  8.27691243e-02 -8.18827989e-03\n",
      "  -2.94057838e-02  1.78580784e-01  3.81307904e-02  8.19501986e-02\n",
      "   4.03962650e-02 -9.28611454e-02  6.44435978e-02 -3.85242745e-02\n",
      "  -4.89134216e-02 -9.91839912e-02 -4.69856998e-02  4.73651201e-02\n",
      "   5.15482376e-02 -7.53780414e-02 -1.21473083e-02 -5.24041186e-05\n",
      "  -3.57698010e-02 -9.49070590e-02  7.57303180e-02 -1.70862877e-02\n",
      "  -1.77285799e-01  6.60997603e-02 -1.35385132e-02 -4.59311904e-04\n",
      "  -1.46023358e-01 -1.37694008e-01  8.83028472e-02  6.31393083e-02\n",
      "   1.24945228e-01  1.12791631e-01 -1.46599415e-01  2.76971933e-02]]\n",
      "[-1.61301642 -0.76634395  1.61370713  1.55973125 -1.03699021]\n"
     ]
    }
   ],
   "source": [
    "# HeInitializerテスト\n",
    "he = HeInitializer()\n",
    "\n",
    "print(he.W(400, 200)[:5])\n",
    "print(he.B(200)[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題7】最適化手法\n",
    "学習率は学習過程で変化させていく方法が一般的です。基本的な手法である AdaGrad のクラスを作成してください。\n",
    "\n",
    "\n",
    "まず、これまで使ってきたSGDを確認します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adagrad:\n",
    "    \"\"\"\n",
    "    確率的勾配降下法\n",
    "    Parameters\n",
    "    ----------\n",
    "    lr : 学習率\n",
    "    \"\"\"\n",
    "    def __init__(self, lr=0.01):\n",
    "        self.lr = lr\n",
    "        self.HW = 0\n",
    "        self.HB = 0\n",
    "    def update(self, layer):\n",
    "        \"\"\"\n",
    "        ある層の重みやバイアスの更新\n",
    "        Parameters\n",
    "        ----------\n",
    "        layer : 更新前の層のインスタンス\n",
    "        \"\"\"\n",
    "        self.HW += np.square(layer.dW)\n",
    "        layer.W -= self.lr * layer.dW / (np.sqrt(self.HW) + 1e-7)\n",
    "        \n",
    "        self.HB += np.square(layer.dB)\n",
    "        layer.B -= self.lr * layer.dB / (np.sqrt(self.HB) + 1e-7)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題8】クラスの完成\n",
    "任意の構成で学習と推定が行えるScratchDeepNeuralNetrowkClassifierクラスを完成させてください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "(10000, 28, 28)\n",
      "uint8\n",
      "[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   3  18  18  18 126 136\n",
      "  175  26 166 255 247 127   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0  30  36  94 154 170 253 253 253 253 253\n",
      "  225 172 253 242 195  64   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0  49 238 253 253 253 253 253 253 253 253 251\n",
      "   93  82  82  56  39   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0  18 219 253 253 253 253 253 198 182 247 241\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0  80 156 107 253 253 205  11   0  43 154\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0  14   1 154 253  90   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0 139 253 190   2   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0  11 190 253  70   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0  35 241 225 160 108   1\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0  81 240 253 253 119\n",
      "   25   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0  45 186 253 253\n",
      "  150  27   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  16  93 252\n",
      "  253 187   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 249\n",
      "  253 249  64   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0  46 130 183 253\n",
      "  253 207   2   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0  39 148 229 253 253 253\n",
      "  250 182   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0  24 114 221 253 253 253 253 201\n",
      "   78   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0  23  66 213 253 253 253 253 198  81   2\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0  18 171 219 253 253 253 253 195  80   9   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0  55 172 226 253 253 253 253 244 133  11   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0 136 253 253 253 212 135 132  16   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]]\n",
      "1.0\n",
      "0.0\n",
      "(60000,)\n",
      "(60000, 10)\n",
      "float64\n",
      "(48000, 784)\n",
      "(12000, 784)\n"
     ]
    }
   ],
   "source": [
    "# データセットの読み込みから前処理まで\n",
    "from keras.datasets import mnist\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "print(X_train.shape) # (60000, 28, 28)\n",
    "print(X_test.shape) # (10000, 28, 28)\n",
    "print(X_train[0].dtype) # uint8\n",
    "print(X_train[0])\n",
    "\n",
    "X_train = X_train.reshape(-1, 784)\n",
    "X_test = X_test.reshape(-1, 784)\n",
    "\n",
    "X_train = X_train.astype(np.float)\n",
    "X_test = X_test.astype(np.float)\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "print(X_train.max()) # 1.0\n",
    "print(X_train.min()) # 0.0\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "y_train_one_hot = enc.fit_transform(y_train[:, np.newaxis])\n",
    "y_test_one_hot = enc.transform(y_test[:, np.newaxis])\n",
    "print(y_train.shape) # (60000,)\n",
    "print(y_train_one_hot.shape) # (60000, 10)\n",
    "print(y_train_one_hot.dtype) # float64\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2)\n",
    "print(X_train.shape) # (48000, 784)\n",
    "print(X_val.shape) # (12000, 784)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SGD, SimpleInitializer, Tanh \n",
    "class ScratchDeepNeuralNetrowkClassifier:\n",
    "    def __init__(self, lr, n_nodes1, n_nodes2, n_output, sigma=0.01):\n",
    "        self.lr = lr\n",
    "        self.n_nodes1 = n_nodes1\n",
    "        self.n_nodes2 = n_nodes2\n",
    "        self.n_output = n_output\n",
    "        self.sigma = sigma\n",
    "    \n",
    "    def fit(self, X, y, X_val=None, y_val=None, epoch=20, batch_size=20):\n",
    "        \"\"\"\n",
    "        ニューラルネットワーク分類器を学習する。\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (n_samples, self.n_features)\n",
    "            訓練データの特徴量\n",
    "        y : 次の形のndarray, shape (n_samples, )\n",
    "            訓練データの正解値\n",
    "        X_val : 次の形のndarray, shape (n_samples, self.n_features)\n",
    "            検証データの特徴量\n",
    "        y_val : 次の形のndarray, shape (n_samples, )\n",
    "            検証データの正解値\n",
    "        \"\"\"\n",
    "        self.n_samples = X.shape[0]\n",
    "        self.n_features = X.shape[1]\n",
    "        self.loss = np.zeros(epoch)\n",
    "        self.val_loss = np.zeros(epoch)\n",
    "        \n",
    "        # 各インスタンス初期化\n",
    "        optimizer = SGD(self.lr)\n",
    "        self.FC1 = FC(self.n_features, self.n_nodes1, SimpleInitializer(self.sigma), optimizer)\n",
    "        self.activation1 = Tanh()\n",
    "        self.FC2 = FC(self.n_nodes1, self.n_nodes2, SimpleInitializer(self.sigma), optimizer)\n",
    "        self.activation2 = Tanh()\n",
    "        self.FC3 = FC(self.n_nodes2, self.n_output, SimpleInitializer(self.sigma), optimizer)\n",
    "        self.activation3 = Softmax()\n",
    "              \n",
    "        # ラベル化\n",
    "        enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "        y_train_one_hot = enc.fit_transform(y[:, np.newaxis]) \n",
    "        if X_val is not None and y_val is not None:\n",
    "            y_val_one_hot = enc.fit_transform(y_val[:, np.newaxis])\n",
    "            \n",
    "        for i in range(epoch):\n",
    "            get_mini_batch = GetMiniBatch(X, y_train_one_hot, batch_size=batch_size)\n",
    "            loss_list = []\n",
    "            \n",
    "            for mini_X_train, mini_y_train in get_mini_batch:\n",
    "                # フォワードプロバゲーション\n",
    "                A1 = self.FC1.forward(mini_X_train)     #shape(n_batch, n_nodes1)\n",
    "                Z1 = self.activation1.forward(A1)     #shape(n_batch, n_nodes1)\n",
    "                A2 = self.FC2.forward(Z1)        #shape(n_batch, n_nodes2\n",
    "                Z2 = self.activation2.forward(A2)       #shape(n_batch, n_nodes2)\n",
    "                A3 = self.FC3.forward(Z2)       #shape(n_batch, n_output)\n",
    "                Z3 = self.activation3.forward(A3)     #shape(n_batch , n_output)\n",
    "                \n",
    "                # バックプロバゲーション\n",
    "                dA3, error = self.activation3.backward(Z3, mini_y_train) # 交差エントロピー誤差とソフトマックスを合わせている\n",
    "                loss_list.append(error)\n",
    "                dZ2 = self.FC3.backward(dA3)     \n",
    "                dA2 = self.activation2.backward(dZ2)\n",
    "                dZ1 = self.FC2.backward(dA2)\n",
    "                dA1 = self.activation1.backward(dZ1)\n",
    "                dZ0 = self.FC1.backward(dA1) # dZ0は使用しない        \n",
    "            \n",
    "            self.loss[i] =np.sum(loss_list)/len(loss_list)\n",
    "            \n",
    "            # val_lossの計算\n",
    "            if X_val is not None and y_val is not None:\n",
    "                # フォワードプロバゲーション**************************\n",
    "                A1 = self.FC1.forward(X_val)     \n",
    "                Z1 = self.activation1.forward(A1)\n",
    "                A2 = self.FC2.forward(Z1)        \n",
    "                Z2 = self.activation2.forward(A2)\n",
    "                A3 = self.FC3.forward(Z2)       \n",
    "                Z3 = self.activation3.forward(A3)\n",
    "\n",
    "                # クロスエントロピー誤差**************************            \n",
    "                _, val_error = self.activation3.backward(Z3, y_val_one_hot) \n",
    "                self.val_loss[i]= val_error\n",
    "\n",
    "    def predict(self,X):\n",
    "        A1 = self.FC1.forward(X)\n",
    "        Z1 = self.activation1.forward(A1)\n",
    "        A2 = self.FC2.forward(Z1)\n",
    "        Z2 = self.activation2.forward(A2)\n",
    "        A3 = self.FC3.forward(Z2)\n",
    "        y = self.activation3.forward(A3)\n",
    "        \n",
    "        pred = np.argmax(y, axis=1)\n",
    "        \n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:0.9666\n"
     ]
    }
   ],
   "source": [
    "# SGD、Tanhで実行\n",
    "scr_DNN = ScratchDeepNeuralNetrowkClassifier(lr=0.01, n_nodes1=400, n_nodes2=200, n_output=10)\n",
    "\n",
    "scr_DNN.fit(X_train, y_train, X_val, y_val, epoch=20)\n",
    "pred = scr_DNN.predict(X_test)\n",
    "print(\"Accuracy:{}\".format(accuracy_score(y_test, pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAD8CAYAAABw1c+bAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvIxREBQAAIABJREFUeJzt3Xl8VPW9+P/Xe5ZkBrInEEiAsMimIksQYq0K2gLurVuxKOot5Wddatur3+rtrVpqv4/2tre97f26VC21VRS32tIWi6hE3FgVlH0TSNghIWSbTGbm8/vjTMIkzCRDZrIw834+HvM4Zz7nc2beM4T3mfM5n8/niDEGpZRSycPW3QEopZTqWpr4lVIqyWjiV0qpJKOJXymlkowmfqWUSjKa+JVSKslo4ldKqSSjiV8ppZKMJn6llEoyju4OIJy8vDwzePDgDu1bW1tL79694xtQHGl8sdH4YqPxxaYnx7d27dqjxpg+UVU2xvS4R3FxsemoZcuWdXjfrqDxxUbji43GF5ueHB+wxkSZY7WpRymlkowmfqWUSjKa+JVSKsn0yIu7SqnE09jYSHl5OR6PJ2KdzMxMNm/e3IVRnZ6eEJ/L5WLAgAE4nc4Ov4YmfqVUlygvLyc9PZ3BgwcjImHrVFdXk56e3sWRRa+74zPGcOzYMcrLyxkyZEiHX0ebepRSXcLj8ZCbmxsx6av2iQi5ubltnjVFI6ES/9o9lfxjp5e1eyq7OxSlVBia9GMXj+8wYRL/mt0VfOP3H/P69kZmPbtCk79SSkWQMIl/5RcV+AIGAzT6AqzYday7Q1JKqR4pYRJ/ydBcmk6AnA4bJUNzuzUepVTPcvz4cZ544onT3u+KK67g+PHjp73f7bffzmuvvXba+3WFhEn8xUXZFBdlk5kiLJhTQnFRdneHpJSK0do9lTy+bEdcmm4jJX6/39/mfosXLyYrKyvm9+9JEqo759kFGWzaV6lJX6ke7id/38im/SdOKff7/djtdgCqPY1sOVhNwIBNYFS/dNJdkfuun12QwSNXnxNx+4MPPsjOnTsZN24cTqeTtLQ0+vfvz7p169i0aRNf+9rXKCsrw+PxcN999zF37lwABg8ezJo1a6ipqWH69OlcfPHFfPTRRxQWFvK3v/0Nt9vd7ud95513uP/++/H5fJx//vk8+eSTpKam8uCDD7Jo0SIcDgfTpk3jV7/6Fa+++io/+clPsNvtZGZmsnz58nZf/3QlVOIvyHJT57P+YNr6A1FK9XwnPD4CxloPGOt5LP+vf/7zn7NhwwbWrVtHaWkpV155JRs2bGjuDz9//nxycnKor6/n/PPP5/rrryc3t2WT8c6dO3n55Zd55plnuOmmm3j99de55ZZb2nxfj8fD7bffzjvvvMOIESOYPXs2Tz75JLNnz+aNN95gy5YtiEhzc9K8efNYsmQJhYWFHWpiikZCJf7CLOvIe6DKo4lfqR4s0i/z0AFSa/dUMuvZFTT6AjgdNn47c3xcz+YnTZrUYhDU7373O9544w0AysrK2L59+ymJv6ioiHHjxgFQXFzM7t27232frVu3MmTIEEaMGAHAbbfdxuOPP84999yDy+Vizpw5XHnllVx11VUAXHjhhdx+++3cdNNNXHfddfH4qKdImDZ+sH7xA+yrrO/mSJRSsSouymbBnBJ+MG1kp1y3C51Xv7S0lLfffpuPP/6Y9evXM378+LCDpFJTU5vX7XY7Pp+v3fexZkw+lcPhYNWqVVx//fX89a9/ZcaMGQA89dRTPPbYY5SVlTFu3DiOHYt/D8V2E7+IDBSRZSKyWUQ2ish9YeqIiPxORHaIyGciMiFk220isj34uC3eHyBU0y/+fcc18SuVCIqLsrl76llxSfrp6elUV1eH3VZVVUV2dja9evViy5YtrFixIub3azJq1Ch2797Njh07AHj++ee55JJLqKmpoaqqiiuuuIL/+Z//Yd26dYDVnDR58mTmzZtHXl4eZWVlcYulSTRNPT7g340xn4hIOrBWRJYaYzaF1LkcGB58TAaeBCaLSA7wCDARMMF9FxljOmV0Vd/0VOyiiV8pdarc3FwuvPBCzj33XNxuN/n5+c3bZsyYwVNPPcV5553HyJEjKSkpidv7ulwu/vjHP3LjjTc2X9y98847qaio4Nprr8Xj8WCM4Te/+Q0ADzzwANu3b8cYw2WXXcbYsWPjFkuTdhO/MeYAcCC4Xi0im4FCIDTxXwv8OXgXmBUikiUi/YEpwFJjTAWAiCwFZgAvxfVTBNlsQo5L2K+JXykVxosvvhi2PDU1lTfffDPstqZ2/Ly8PFauXNlcfv/997f5Xs8991zz+mWXXcann37aYnv//v1ZtWrVKfv95S9/afN14+G0Lu6KyGBgPLCy1aZCIPR8pDxYFqk83GvPBeYC5OfnU1paejqhNctKCbB5z8EO79/ZampqemxsoPHFSuOLLDMzM2JTSxO/399une7UU+LzeDwx/TtGnfhFJA14HfieMaZ1B9xwswaZNspPLTTmaeBpgIkTJ5opU6ZEG1oLz3y2hC9qHXR0/85WWlraY2MDjS9WGl9kmzdvbndK4+6e9rg94eK7++67+fDDD1uU3Xfffdxxxx2dFofL5WL8+PEd3j+qxC8iTqykv8AYE+48pBwYGPJ8ALA/WD6lVXlpRwKNVo5b+PiAB58/gMOeUJ2WlFI90OOPP97dIZy2aHr1CPAHYLMx5tcRqi0CZgd795QAVcFrA0uAaSKSLSLZwLRgWafJcwkBA4eqGzrzbZRS6owVzS/+C4Fbgc9FZF2w7D+AQQDGmKeAxcAVwA6gDrgjuK1CRH4KrA7uN6/pQm9nyXVbrUv7Kuubu3cqpZQ6KZpePR8Qvq0+tI4B7o6wbT4wv0PRdUCOyzqJ0Z49SikVXsI1gjf/4tfEr5RSYSVc4k+1Czm9UzTxK6VikpaWFnHb7t27Offcc7swmvhKuMQPUJDl0qYepRJB2Sp4/7+tpYqbhJqds0lhlpsvjtZ2dxhKqUjefBAOfn5KsdvvA3swLTWcgEMbwARAbJB/LqRmRH7NfmPg8p9H3PzDH/6QoqIi7rrrLgAeffRRRITly5dTWVlJY2Mjjz32GNdee+1pfRSPx8N3vvMd1qxZg8Ph4Ne//jVTp05l48aN3HHHHXi9XgKBAK+//joFBQXcdNNNlJeX4/f7+fGPf8w3vvGN03q/eEjIxF+Q5eaD7UcxxsTljvRKqW7gqbKSPlhLT1Xbib8dM2fO5Hvf+15z4n/llVf417/+xfe//30yMjI4evQoJSUlXHPNNaeVN5r68X/++eds2bKFadOmsW3bNp566inuu+8+Zs2ahdfrxe/3s3jxYgoKCvjnP/8JWJPDdYeETPyFWW5qvX5O1PvI7KXz8ivV40T4ZV4fOjK2bBX86Rrwe8GeAtc/CwMndfgtx48fz+HDh9m/fz9HjhwhOzub/v378/3vf5/ly5djs9nYt28fhw4dol+/flG/7gcffMC9994LWDNxFhUVsW3bNi644AJ+9rOfUV5eznXXXcfw4cMZM2YM999/Pz/84Q+56qqruOiiizr8eWKRkG38Oj2zUglg4CS4bRFc+iNrGUPSb3LDDTfw2muv8fLLLzNz5kwWLFjAkSNHWLt2LevWrSM/Pz/sPPxtiTTf/je/+U0WLVqE2+1m+vTpvPvuu4wYMYK1a9cyZswYHnroIebNmxfzZ+qIhPzFXxCS+M8u6PipoVKqmw2cFJeE32TmzJl8+9vf5ujRo7z33nu88sor9O3bF6fTybJly9izZ89pv+bFF1/MggULuPTSS9m2bRt79+5l5MiR7Nq1i6FDh/Ld736XXbt28dlnnzFq1ChycnK45ZZbSEtLazGDZ1dK6MSvPXuUUqHOOeccqqurKSwspH///syaNYurr76aiRMnMm7cOEaNGnXar3nXXXdx5513MmbMGBwOB8899xypqam8/PLLvPDCCzidTvr168fDDz/M6tWreeCBB7DZbDidTp588slO+JTtS8jEn5eWQorDpolfKXWKzz8/2ZsoLy+Pjz/+OGy9mpqaiK8xePBgNmzYAFgzZYb75f7QQw/x0EMPtSibPn0606dP70DU8ZWQbfwiQmGWm3JN/EopdYqE/MUPOohLKRW7zz//nFtvvbX5eSAQwO12t7gT15koYRN/YZab97Yd6e4wlFIhzrSxNWPGjGm+CTr0jBvFROpFdDoSsqkHrAu8h6sb8PoC3R2KUgqrLfzYsWNxSVzJyhjDsWPHcLlcMb1Owv7iL8hyYwwcrPIwKLdXd4ejVNIbMGAA5eXlHDkS+Uzc4/HEnNQ6U0+Iz+VyMWDAgJheo93ELyLzgauAw8aYU6ajE5EHgFkhrzca6BO8CctuoBrwAz5jzMSYoj0NA4JdOsuP12niV6oHcDqdDBkypM06paWlMd1LtrP19PiiFU1Tz3PAjEgbjTG/NMaMM8aMAx4C3mt1l62pwe1dlvQhtC//6Y3CU0qpRNdu4jfGLAeivV3izcBLMUUUJ/0yrdMx7dmjlFItxe3iroj0wjozeD2k2ABvichaEZkbr/eKhstpp096KvsqNfErpVQoieYKu4gMBv4Rro0/pM43gFuMMVeHlBUYY/aLSF9gKXBv8Awi3P5zgbkA+fn5xQsXLjydz9Gspqam+c458z6ux+2AB87vOTddD42vJ9L4YqPxxUbj67ipU6eujbpJ3RjT7gMYDGxop84bwDfb2P4ocH8071dcXGw6atmyZc3rd72w1kz91bKIdbtDaHw9kcYXG40vNhpfxwFrTBT51RgTn6YeEckELgH+FlLWW0TSm9aBacCGeLxftJpG7xrtN6yUUs2i6c75EjAFyBORcuARwAlgjHkqWO3rwFvGmND7HeYDbwRH6TmAF40x/4pf6O0ryHLjaQxQUeslNy21K99aKaV6rHYTvzHm5ijqPIfV7TO0bBcwtqOBxUNhSJdOTfxKKWVJ2CkbIPSGLHXdHIlSSvUcCZ34T96CUQdxKaVUk4RO/Fm9nPRKsesgLqWUCpHQiV9EKMhy6yAupZQKkdCJH6x2/v1VmviVUqpJwif+wiy3NvUopVSIJEj8Lo7WePE0+rs7FKWU6hESPvGfnJ5Zf/UrpRQkQeIv1Hn5lVKqhYRP/DqISymlWkr4xN8v04VNdBCXUko1SfjE77TbyM9waRu/UkoFJXziB3QQl1JKhUiaxK+DuJRSypIUib8wy82B4x4CAb0hi1JKJUnid+H1Bzha09DdoSilVLdrN/GLyHwROSwiYW+bKCJTRKRKRNYFHw+HbJshIltFZIeIPBjPwE/HyS6d2tyjlFLR/OJ/DpjRTp33jTHjgo95ACJiBx4HLgfOBm4WkbNjCbajCrM18SulVJN2E78xZjlQ0YHXngTsMMbsMsZ4gYXAtR14nZjptA1KKXVSvNr4LxCR9SLypoicEywrBMpC6pQHy7pchstJeqpDp21QSilAjGm/p4uIDAb+YYw5N8y2DCBgjKkRkSuA3xpjhovIjcB0Y8ycYL1bgUnGmHsjvMdcYC5Afn5+8cKFCzv0gWpqakhLSzul/D8/qKNPLxv3TXB16HXjJVJ8PYXGFxuNLzYaX8dNnTp1rTFmYlSVjTHtPoDBwIYo6+4G8oALgCUh5Q8BD0XzGsXFxaajli1bFrb8jj+uMpf/z/IOv268RIqvp9D4YqPxxUbj6zhgjYkivxpjYm/qEZF+IiLB9UlYzUfHgNXAcBEZIiIpwExgUazv11GFOohLKaUAcLRXQUReAqYAeSJSDjwCOAGMMU8BNwDfEREfUA/MDB59fCJyD7AEsAPzjTEbO+VTRKEgy83xukZqG3z0Tm33YyulVMJqNwMaY25uZ/v/A/5fhG2LgcUdCy2+CrKstv39x+sZnp/ezdEopVT3SYqRuwADtC+/UkoBSZT4dfSuUkpZkibx90134bCJDuJSSiW9pEn8dpvQL9Olg7iUUkkvaRI/6A1ZlFIKkizxF2a5tY1fKZX0ki7xHzzhwa83ZFFKJbGkSvwFWW78AcOhE9rOr5RKXkmW+E8O4lJKqWSVVIlfB3EppVSSJf7+mZr4lVIqqRJ/71QHWb2c2tSjlEpqSZX4IdilU/vyK6WSWNIl/oIst47eVUoltaRL/IVZbm3qUUoltaRM/NUNPqrqG7s7FKWU6hbtJn4RmS8ih0VkQ4Tts0Tks+DjIxEZG7Jtt4h8LiLrRGRNPAPvqKbpmfVXv1IqWUXzi/85YEYb278ALjHGnAf8FHi61fapxphxJtq7v3cyHcSllEp20dx6cbmIDG5j+0chT1cAA2IPq/MU6iAupVSSE+u+6O1UshL/P4wx57ZT735glDFmTvD5F0AlYIDfG2Nanw2E7jsXmAuQn59fvHDhwig/Qks1NTWkpaVF3B4whrlv1TFtsJObRqZ06D1i0V583U3ji43GFxuNr+OmTp26NuqWFWNMuw9gMLChnTpTgc1AbkhZQXDZF1gPXBzN+xUXF5sO2bvS7HzubmP2rmyz2sX/9a6558VPOvYeMVq2bFm3vG+0NL7YaHyx0fg6Dlhjosivxpj49OoRkfOAZ4FrjTHHQg4q+4PLw8AbwKR4vF9Yez6G565iyBcvwJ+ugbJVEatag7jqOi0UpZTqyWJO/CIyCPgLcKsxZltIeW8RSW9aB6YBYXsGxcXuD8DfgGDA74Xd70esqoO4lFLJLJrunC8BHwMjRaRcRL4lIneKyJ3BKg8DucATrbpt5gMfiMh6YBXwT2PMvzrhM1iGXgJiwwDYU2DwRRGrFmS5OVTtodEf6LRwlFKqp4qmV8/N7WyfA8wJU74LGHvqHp1k4CQ47xuY9QuRb75iPY9gQJYbY+BglYeBOb26LESllOoJEmvk7jlfx4YBW9sfq2kQl3bpVEolo8RK/APOt5Z7V7RZrakvvw7iUkolo8RK/L1yqO01oM0ePQD9M63Ruzo9s1IqGSVW4geqMkdD2UoIRL5w63LayUtLYX+VJn6lVPJJuMR/ImMUeI7Dse1t1ivMcrNPu3QqpZJQwiX+qszR1ko77fwFOohLKZWkEi7x17sLoFduu+38TYO4TBRzFSmlVCJJuMSPCAycDGXt9OzJclPf6Od4nd6QRSmVXBIv8YM1eOvYDqg9FrGK9uVXSiWrBE38JdaybGXEKoWa+JVSSSoxE3/BOLA52078TTdk0b78Sqkkk5iJ3+mG/mPbTPzZvZy4nDYdvauUSjqJmfgBBpXAvk/A5w27WUSsnj06iEsplWQSN/EPnAT+BjiwPmIV64YsmviVUsklgRP/ZGvZzgVeHb2rlEo2USV+EZkvIodFJOwdtMTyOxHZISKficiEkG23icj24OO2eAXervR+kFXUZn/+giw3R2sa8DT6uywspZTqbtH+4n8OmNHG9suB4cHHXOBJABHJAR4BJmPdb/cREcnuaLCnbVCJNYI3wujcpi6dB6r0V79SKnlElfiNMcuBijaqXAv8OXiz9xVAloj0B6YDS40xFcaYSmApbR9A4mvgJKg5BJW7w25uGsSlPXuUUskkXm38hUBZyPPyYFmk8q7RPJAr/Lw9OohLKZWM2r3nbpQkTJlpo/zUFxCZi9VMRH5+PqWlpR0KpKam5uS+xs+X7b04tPIvbK/MP6WuL2AQ4MNPN9O3ZmeH3i+m+HogjS82Gl9sNL6uEa/EXw4MDHk+ANgfLJ/Sqrw03AsYY54GngaYOHGimTJlSrhq7SotLaXFvuUlFNaUUxjh9fqueJuUrD5MmdI194U/Jb4eRuOLjcYXG42va8SrqWcRMDvYu6cEqDLGHACWANNEJDt4UXdasKzrDJwMhzaCpyrsZh3EpZRKNlH94heRl7B+ueeJSDlWTx0ngDHmKWAxcAWwA6gD7ghuqxCRnwKrgy81zxjT1kXi+Bs4GTBQvgbOuuyUzYVZbjbsC39QUEqpRBRV4jfG3NzOdgPcHWHbfGD+6YcWJwMmgtisC7wREv9bmw4RCBhstnCXJJRSKrEk7sjdJqnpkH9OxIFcBVluvL4Ax2rDz+mjlFKJJvETP1jNPeVrIHDqCF3t0qmUSjZJkvhLwFtjXeRtRQdxKaWSTZIk/knWMsyEbU03ZNHEr5RKFsmR+LMGQXr/sIk/w+UgLdVBuU7PrJRKEsmR+EWsX/1hEr91QxaX/uJXSiWN5Ej8YLXzH98LJw6csqlQB3EppZJIEiX+yDdmKdA7cSmlkkjyJP5+Y8Dhipj4K+saqfP6uiEwpZTqWsmT+B0pUFgcNvEP0J49SqkkkjyJH6wLvAfWg7euRXFB8yAuvROXUirxJVniL4GAD/Z/2qJYB3EppZJJkiX+poFcLeftyU9PxW4TvcCrlEoKyZX4e+VA3ohTbsXosNvol6F9+ZVSySG5Ej+cHMgVCLQoLshy6URtSqmkkISJvwTqK+HYjhbFhVluTfxKqaQQVeIXkRkislVEdojIg2G2/0ZE1gUf20TkeMg2f8i2RfEMvkOaB3K1bOcvyHJzsMqDPxD2XvBKKZUw2r0Dl4jYgceBr2LdPH21iCwyxmxqqmOM+X5I/XuB8SEvUW+MGRe/kGOUNxzc2VZzz4TZzcUFWW58AcOR6gb6Zbq6MUCllOpc0fzinwTsMMbsMsZ4gYXAtW3Uvxl4KR7BdQoR61f/3pYDuZqmZ953vC7cXkoplTDEul1uGxVEbgBmGGPmBJ/fCkw2xtwTpm4RsAIYYIzxB8t8wDrAB/zcGPPXCO8zF5gLkJ+fX7xw4cIOfaCamhrS0tLarDNoz2sM/eJ5PvzS8zSmZACwrzrAjz6sZ1I/O9OKnJyVbe/Q+8cjvu6k8cVG44uNxtdxU6dOXWuMmRhN3Whuth7uDuSRjhYzgdeakn7QIGPMfhEZCrwrIp8bY3ae8oLGPA08DTBx4kQzZcqUKEI7VWlpKe3uu9sJXzzPhUVOGGnV/WDHUfhwJasP+vnsmGHBnBKKi7I7FEPM8XUjjS82Gl9sNL6uEU1TTzkwMOT5AGB/hLozadXMY4zZH1zuAkpp2f7fPQongM3RYt6e9WXW9WgDNPoCrNh1rJuCU0qpzhVN4l8NDBeRISKSgpXcT+mdIyIjgWzg45CybBFJDa7nARcCm1rv2+Wcbug/tkU7f8nQXBw26+TGbrNRMjS3u6JTSqlO1W7iN8b4gHuAJcBm4BVjzEYRmSci14RUvRlYaFpeNBgNrBGR9cAyrDb+7k/8YPXn3/8J+LwAFBdl8/y3JpGe6qAotxcTBmV1c4BKKdU5ourHb4xZbIwZYYwZZoz5WbDsYWPMopA6jxpjHmy130fGmDHGmLHB5R/iG34MBk4CnwcOftZcdMGwPH505Wi2H67h7c2HuzE4pZTqPMk3crdJhDty3VA8gCF5vfnVkq06mEsplZCSN/Fn9IesQbC35Qheh93GD746gq2Hqvn7+kjXsJVS6syVvIkfrHb+spXQaizDlWP6c3b/DH69dBteXyDCzkopdWZK8sQ/CWoOwfG9LYptNuGB6SPZW1HHK2vKuik4pZTqHEme+MO38wNMGdmHiUXZ/O6d7dR7/adsV0qpM1VyJ/78cyAlLWziFxH+z4xRHK5u4M8f7+7y0JRSqrMkd+K32WHAxFMmbGsyaUgOl4zow5Pv7eSEp7GLg1NKqc6R3IkfrAu8hzeC50TYzQ9MH8nxukaeXb6riwNTSqnOoYl/4CQwAdi3JuzmcwszuXJMf5794AuO1jR0cXBKKRV/mvgHnA/IKTdgD/WDaSPwNPp5Ytkpk4oqpdQZRxO/K8O6yNtqIFeoYX3SuKF4AC+s2KP35VVKnfE08YPVrbN8DQQid9u87ysjAPjd29u7KiqllOoUmvjBSvzeajgceeLQwiw3t5QU8eraMnYeqenC4JRSKr408QMMijyQK9RdU4fhctr59dJtXRCUUkp1Dk38AFlFkJYfsT9/k7y0VOZ8eQj//OwAG/ZVdVFwSikVX1ElfhGZISJbRWSHiDwYZvvtInJERNYFH3NCtt0mItuDj9viGXzciFjNPe384geYc/FQsno5+eWSrV0QmFJKxV+7iV9E7MDjwOXA2cDNInJ2mKovG2PGBR/PBvfNAR4BJgOTgEdEJP53MI+HgZPh+B6oPthmtQyXk+9cMoz3th1hpd6XVyl1BormF/8kYIcxZpcxxgssBK6N8vWnA0uNMRXGmEpgKTCjY6F2skEl1vKt/2yzTz/A7AsG0zc9lV8u2YoxerMWpdSZJZrEXwiEzk1cHixr7XoR+UxEXhORgae5b/fzBUflfv4q/OmqNpO/O8XOdy8bzpo9lZRuPdJFASqlVHxIe79YReRGYLoxZk7w+a3AJGPMvSF1coEaY0yDiNwJ3GSMuVREHgBSjTGPBev9GKgzxvx3mPeZC8wFyM/PL164cGGHPlBNTQ1paWmnvd+gPa8x5IsXEAwGOJo7iY3n/ofV/h+GL2B46P16XA7hJ19yYYtQL17xdRWNLzYaX2w0vo6bOnXqWmPMxKgqG2PafAAXAEtCnj8EPNRGfTtQFVy/Gfh9yLbfAze3957FxcWmo5YtW9axHfeuNOan+cY8mm3Mo1nGPJJhzAs3GlN9KOIub3xSbop++A+zaN2+zo+vi2h8sdH4YqPxdRywxrSTW5se0TT1rAaGi8gQEUkBZgKLQiuISP+Qp9cAm4PrS4BpIpIdvKg7LVjW8wycBLctgkt/BLe/CTN+AbtK4YkLYOubYXe5emwBI/PT+fXSbTT69RaNSqkzQ7uJ3xjjA+7BStibgVeMMRtFZJ6IXBOs9l0R2Sgi64HvArcH960Afop18FgNzAuW9UwDJ8FF/w5FJVByJ/x/70F6f3hpJvz9e+CtbVHdbhPunz6SL47W8vra8m4KWimlTo8jmkrGmMXA4lZlD4esP4TVBBRu3/nA/Bhi7D59R8O334F3H4OP/he+WA7XPwOFxc1VvjK6L+MHZfHbd7bztfGFuJz2bgxYKaXapyN32+NIhWk/hdv+bvX8efar8N4vwe8DrFs0PjB9JAeqPLywYk83B6uUUu3TxB+tIRfBdz6Ec74Oyx6DP14OFdZdub40LI8vn5XHb9/Zzm+WbmPtnspuDlYppSLTxH863Flwwx9EOOxrAAAXHklEQVTgumfhyFZ46iL49AUwhqvH9qfa4+O372zn5qdXsHZ3z72UoZRKbpr4O+K8G+E7H0D/cfC3u+GVW6muPExTT36vP8BdL37CG5+W4/Vpbx+lVM8S1cVdFUbWIKv750f/C+8+xuzUlRx2TsdpvKzibPbbxvD9l9fzfxdvYXZJEd+cPIjctNTujloppTTxx8Rmhy9/D4ZdSsrLt/Af9c9jAGNzwJXP8H7qRcz/4Av+e+k2/nfZDr4+rpBzU/UMQCnVvbSpJx76nwfjZgGCALaAD9vrd3DJm1/lT/mv8NF1jXxzXC5/W7+PH39Yz6xnV/DO5kMEAjrBm1Kq6+kv/ngZNhU++A34vWB3wPnfhqPbYd0CChqf4VF7Kj8adgH/Oj6IBYcm8q0/HWVwbm/uuHAINxQPoHeq/lMopbqGZpt4aZryYff7MPgi6zlAowf2fgQ73sG5fSlXHy/lav5MXU4B7/vH8to/RvPEW2O55vwRXNp7N+z5gOyzL2XU+V/p1o+jlEpcmvjjaeCkkwm/idMFwy61HtN/xsf/eoUL8mroteMdpu8qZXrKm/hwsGnlAEZLGTYCeHc9w98PPc3Ur15Nmp4JKKXiTLNKF2tw9YWJN8HEfwOfF8pW4tixlKGrnsfZ6AfAZbx8adW9vLvyaY6mjyKlcCz9R01i3Mhh2jNIKRUzTfzdyZFijQgechHlWRcz+B8zceIjgA1vzllMqdtFRt1HsB3YDvsX5fCR8yzqc84mrWgCReeW0G/QiJP3DChbdWpTk1JKtaKJv4cYdf5X2MJCKje927KNv64C7751HNq6ivq96yiq3ES/Q6uxH34OVsMJenOo13CcGf0YdOgdMH6wp2C7/e+a/JVSYWni70FGnf8VaH1Rt1cOKcMvZeDwS5uL/A217Ny0mkPbVuPbt56cE5sZXPsWNrHGCBifh5o/XkdVXjEp/UeTOWgMKf1GQ94ISO2Zdw9SSnUdTfxnIHtqb4aNn8Kw8VMA6y5qf1r4EjO3fLe5qWi9bxC5B3cy9ND7pKz3N+9bndqPxpwRuAvPwV1wNvQZZR0Q3FlQtopBe16Dsl56tqBUAtPEnwBEhDFfupzbN/2YYrORtXIO93/rVlLTUnm7/Bj7d2/Gs28Tjopt9KvbzfD6PQzb/zFIY/NrNKRkk+KtYggBAn98CdtXfwqjr4KMAWDTcX5KJZKoEr+IzAB+i3U/3WeNMT9vtf0HwBzABxwB/s0Ysye4zQ98Hqy61xhzDSruiouyeWDObFbsOsYDQ3MpLsoGYEhebxg3CJgOQEWtl80HTvDCvkoO7t2G9+Bmelft4IrAx5wrlYiABHyw5CFY8hB+Wyr+7KE4+45A8oZD7lnQtHRndeMnVkp1VLuJX0TswOPAV4FyYLWILDLGbAqp9ikw0RhTJyLfAf4L+EZwW70xZlyc41ZhFBdlNyf8SHJ6p3DhWXlceFYeMBy4Ek+jn+dffZWztt6L0/jwYedXzKbWB0PlAEMOH2DY0ZUMkr9j5+RcQ353Lra84dYBIW845RW1HD1YhnvM1YwsuaJzP6xSqsOi+cU/CdhhjNkFICILgWuB5sRvjFkWUn8FcEs8g1Sdy+W0M+HC6dyx+T8pDmxkre0cHpgzmxH5aew4XMP2wzWsOlzDzoMV1B3cRe+aLxgq+xniO8hZtQc4q/zvZJvjDAAGAKb8RbzLckjpdzbkDLEe2SFLPVNQqltFk/gLgbKQ5+XA5Dbqfwt4M+S5S0TWYDUD/dwY89fTjlJ1uqamopfeXs0DXzm/+cxh/KBsxg8KPYu4kNoGX/MB4e3D1Tx5qIZJu5/k2+Yv2MVggG31GQT2HmFQ2UayAi3vSGbcOUjOEMgZ2vKA4KmCQxutsQ16cVmpTiPGtD1DpIjcCEw3xswJPr8VmGSMuTdM3VuAe4BLjDENwbICY8x+ERkKvAtcZozZGWbfucBcgPz8/OKFCxd26APV1NSQltZzuywmanwnyjfx1e2P4MRHIw5+mfVjtthHcrA2QFVtHX38hymSgxTJIQbbDjPcfohBcog+5ig2Tv4NWmtCdfpZ1KQNwePqQ0NqXzyuvnhcfTjWmEpaekbcPm+8Jeq/b1fR+Dpu6tSpa40xE6OpG80v/nJgYMjzAcD+1pVE5CvAjwhJ+gDGmP3B5S4RKQXGA6ckfmPM08DTABMnTjRTpkyJJv5TlJaW0tF9u0LixjeFLatHNA9AezhkPIIxhopaL18crWXXkVp2Ha3lvaM1fHG0ln1HT9A3cIi77H/jOvty7AIBY2isrSCz/igFvpZnCwGxY8sshMxBkDUQMgeeXNZXWjOiDpvabWcMifvv2zU0vq4RTeJfDQwXkSHAPmAm8M3QCiIyHvg9MMMYczikPBuoM8Y0iEgecCHWhV+VgMIOQMPqbpqblkpuWioTB+e02OYPGPYfr+evf3fj3bUCp7HOGO6XH/BxwzBorKdQjjJAjlIoRxnmOMpwTxUDGo7R58A20rxHEFqetZrS/4tkD4HcYZDeD9ILrGVGAaT3tx69+5zaTVWnvFBJot3Eb4zxicg9wBKs7pzzjTEbRWQesMYYswj4JZAGvCrWvDFN3TZHA78XkQDWTV9+3qo3kEpydpswMKcXX5pyBXfsONo8DuGB2bOYPyiLozVe9lbUURZ8lG7axVspGZRX1rO/uh678dFPKrjH/ldutL9nXWMwsL/aj927j4yy9bi9xxDT6s5nNgek9QseEPoDNtj6Twj4we6Eq34DQ6dC7zxw6MR4KrFE1Y/fGLMYWNyq7OGQ9bCTxxtjPgLGxBKgSg6RxiH0SU+lT3pq8/NS+z6mTLkAAK8vwL7j9ZRV1PFhqQ3vvo+azxh+2DiXj2qHEjBgx08eVfSTCgY5TzCiVzVFKScotFXSp7aCzKpNpNWVYQ/4rGD8Xvjb3SeDc2VaZwi9+1oHgrS+wed5wbI+VtnxPQza87qOfFY9no7cVT1GNOMQQqU4bAzJ682QvN70Tr2GO549fvKMYc4tPDcgk8PVDRyoqmf/cU/zckNVPUurPOw/7uFojXU5aoJsY0HKz3Dix4+NZxw340zLpsBRQ77tBDlSRWbtcXof30iK930cnsqwMQ0BzB9eQLKKIKMQeuWAOzu4zImwzLbOMrSpSXURTfwqIUQ6YyjIclOQ5aa4KPx+DT4/B6s8PFE6kFlroMS2mRWB0dRmF5PpcnK42sPh6gbqvP4W+znwkW+vZXjvega765jmWUKJ531sAgEDFV47zgYfqTXbcTYcRzyViN8b+QM4e0FjPWBAbDDoS1Y3V1cmuLKCy0xrDETTetPD2cuamlsPHCpKmvhVwjjdMwaAVIedotze3DRxILPWjWK9bwROh40FXx/T4rVqGnwcqW7g8AnrQGA9PBw50cCO6gb21AjjWdnc1PTtytl8UjGieX+bGArcAQb3bmBAaj2FKR7yU+rIs9WQIzUMqlxBdsWnCGBMAHNkK7aKndbYhsa6tj+EzWEl/4Zqmg8cA0sgaxCkplszsqamQ2oG+QfLYUstpJwss5bpcPBz2POBHjiSgCZ+pbAOGgvmlLBi1zFKQs4YmqSlOkhLdVhzH4Wxds8I7njWT3FgI2vkHG658Xr+PcPF0ZoGKmq9VNR6OVrjpaK2gZ21Xlad8HKs1ktVvTVR3gTJZ0HKxuZxELMq72arczRZvVLIS4P+Li/9Ur30dXro4/CQY68ny1ZHBrWkUUv2wY9JPXzywCEVO+HEPutg0HACgtcvRgNsae/bEMgcYF3DSEmzDiopvYOPtMjrx/fAkS0w8AIYNBlSeln72lNO3iyoLTo7bJfRxK9UUEfOGEL3bRr5/H9CRj63p9EfoLLWyxOlg7llBUwWq6mp17Av8Y38dI7Xezle18jhOi/bjjZyvN7N8TovgVbjLidI3xYHju/UfZcD6eeRkekg0+UgJ9XQJ8VL/eFdjCnqQ5bdQ4Z4SJd60qgnu2wprr3vIVgjryU13bpo7a2FmoPWsvlRA617SYVa9XTL52IPHjx6gdNtrTuD6ym9raW3Dna8zRDjhz++BBNmW9OFO1zBui5wuFsunb2C293W8sBnsPfDjp+xJFFTmSZ+peKkuCib6mEpp3XwcNpt9M1wcfXYAhauHsW6pqamr46I+DqBgKG6wcfxOuugUFnn5bW1/Zn1+clrFN7+YynKcnPC08j+qga2eBo5Ud9ItScPs8cAqcFHJgATZAoLUj46eeA4MotyzxjSUh2kuxykpzmC607SUuxkpfjJdnjJsDeSbvdStHMBfba+GDxwCGbkFdiGXGQdKBrrreaqxjorwTetN9bDif3WsuYQxvgRsM5O1syP7R/DnW01XzlcVndchzu4dIUsXcEDiQvqjsHnr1rdeW0OuOAuyBsZrJsK9lRwpJBRtRn2ZYSUNW1PsZrKylbCkIt7/IFHE79SPUB7TU2hbDYh0+0k0+2kKNcqS3c5mbX50MlrFNeeG/Y13l22jOKSL3OivpETnkZO1Ps44Wnkr5/2Y9bGkweOxoKxjMjpRbXHR7XHx4EqDzUeH9WeRmpbXegGmCDDWZDiPNlU9dn5bNo0mN4pDnqnWo+0VLu1nuKgt9tO7yzrYNI71YHr4Bq+seme5v0/nPwkg0afT29bI25pxG3zkhrwYvN7rAOFrx4aPSeX29/C7Hy3+cAj2UOgz0jweYL1POBrsBK8r+Hk86ZlY+3Js5hAI3z427Df/QSw5iJujz3VOhOxpwQPDE6rzO4MX+atgS/et2JwuOC2RZ2a/DXxK9VDxNrUFM2BwyYnDxqh8tJSmbX18MkDxzXhDxxgjbau9VoHhKaDwUurC5n1yckDh3vol5hdmEFNg4/aBh+1DX5qG3xU1FoD8uqCz2u8PqzpwtJYJP/RvP8ny92wfMMp790rxR58ZNArJTu47qCozsl/mvebDxzPpHwLe1YJbqcdd4o94rJX0/qhtThf+Jo1hsPuxHbTn6DvaPB5wd/QvFy/dhVjzxl1ssznsda3vYXZ/tbJA0/hBOh3nvV6TQ9fA/gbrfr+Ruvsx3/c2lZz0LpfNljPd7+viV8p1b6uOHCANdo6w+Ukw3Xy4CEizPospFfUtMhNVaGMMdQ3+vl45zHuWmDjU98InHYbP758JAWZbuq8fuq8Puq8fmq9fuq9vuDSOnA0bX/rxCA2e08eOD7dko3ZsvW0voMJ8qC1f8No9r7qINO9G5fTOjC4nHZczhROVI5ikL9/sMyG22kn1WnHZUvjm2ZZ84FnSd5c+oy4mFSHLbivjVSHndTg0uW0kWK3IU0XvctWEXjuauuAYHNiG3zRacV+ujTxK6WArjtwhBIReqU4uGx0Pi9+u4SX3l7NzadxcbzJ2j2VzHrW13zgee1bkzl3QCb1Xj/1jdaBos7rx9NoPW9eD5Yv23qYD7bDJ/4RCHB2hovBub1D6vs4Vuul8kSAsvqjeBr9eBoD1Dc2NXul88/QM5aPUuGjle18dpoPDAIMqbcOPJ/4z+GBwHCKT+sbOD2a+JVScRHLgaNp/9O9OB66b7gDT6rDTjS3/Rk7MIvVuyto9AVwOmzMi3CNpPXsnMYYGnwBVu46xtznbazzW2csv7j2HAbn9qbBF7AOEr4ADSHLpvKm5ad7j/NJ3Qg+8Y/AHoAVu47F9F22RxO/UiohdNcZi8tp55KRfXnx26e/fxPrjGVF84GnZGhuRz5G1DTxK6UU8Tlj6eoDT0dp4ldKqR4g1gPP6bC1X0UppVQi0cSvlFJJJqrELyIzRGSriOwQkQfDbE8VkZeD21eKyOCQbQ8Fy7eKyPT4ha6UUqoj2k38ImIHHgcuB84GbhaRs1tV+xZQaYw5C/gN8Ivgvmdj3aP3HGAG8ETw9ZRSSnWTaH7xTwJ2GGN2GWO8wELg2lZ1rgX+FFx/DbhMrCFp1wILjTENxpgvgB3B11NKKdVNounVUwiUhTwvByZHqhO8OXsVkBssX9Fq38JwbyIic4G5APn5+ZSWlkYR2qlqamo6vG9X0Phio/HFRuOLTU+PL1rRJP5wd1AwUdaJZl+r0JingacBROTI1KlT90QRWzh5wNEO7tsVNL7YaHyx0fhi05Pji3CD0VNFk/jLgYEhzwcA+yPUKRcRB9Yk3xVR7nsKY0yfKOIKS0TWGGMmdnT/zqbxxUbji43GF5ueHl+0omnjXw0MF5EhIpKCdbF2Uas6i4Dbgus3AO8aY0ywfGaw188QYDiwKj6hK6WU6oh2f/EH2+zvAZYAdmC+MWajiMwD1hhjFgF/AJ4XkR1Yv/RnBvfdKCKvAJsAH3C3MebUuzgopZTqMlFN2WCMWQwsblX2cMi6B7gxwr4/A34WQ4yn6+n2q3QrjS82Gl9sNL7Y9PT4oiJWi4xSSqlkoVM2KKVUkjljE38s00h0QWwDRWSZiGwWkY0icl+YOlNEpEpE1gUfD4d7rU6McbeIfB587zVhtouI/C74/X0mIhO6MLaRId/LOhE5ISLfa1WnS78/EZkvIodFZENIWY6ILBWR7cFl2KkVReS2YJ3tInJbuDqdFN8vRWRL8N/vDREJe0+S9v4WOjG+R0VkX8i/4RUR9m3z/3onxvdySGy7RWRdhH07/fuLO2PMGffAusi8ExgKpADrgbNb1bkLeCq4PhN4uQvj6w9MCK6nA9vCxDcF+Ec3foe7gbw2tl8BvIk1FqMEWNmN/9YHgaLu/P6Ai4EJwIaQsv8CHgyuPwj8Isx+OcCu4DI7uJ7dRfFNAxzB9V+Eiy+av4VOjO9R4P4o/v3b/L/eWfG12v7fwMPd9f3F+3Gm/uKPZRqJTmeMOWCM+SS4Xg1sJsKI5R7sWuDPxrICyBKR/t0Qx2XATmNMRwf0xYUxZjlWj7VQoX9jfwK+FmbX6cBSY0yFMaYSWIo1b1Wnx2eMecsY4ws+XYE1jqZbRPj+ohHN//WYtRVfMG/cBLwU7/ftLmdq4g83jUTrxNpiGgmgaRqJLhVsYhoPhLvz8gUisl5E3hSRc7o0MGsE9VsisjY4XUZr0XzHXWEmkf/Ddef3B5BvjDkA1sEe6BumTk/5Hv8N6wwunPb+FjrTPcGmqPkRmsp6wvd3EXDIGLM9wvbu/P465ExN/LFMI9FlRCQNeB34njHmRKvNn2A1X4wF/hf4a1fGBlxojJmANevq3SJycavtPeH7SwGuAV4Ns7m7v79o9YTv8UdY42gWRKjS3t9CZ3kSGAaMAw5gNae01u3fH3Azbf/a767vr8PO1MR/OtNIIC2nkegSIuLESvoLjDF/ab3dGHPCGFMTXF8MOEUkr6viM8bsDy4PA29w6qypHZpuI84uBz4xxhxqvaG7v7+gQ03NX8Hl4TB1uvV7DF5MvgqYZYIN0q1F8bfQKYwxh4wxfmNMAHgmwvt29/fnAK4DXo5Up7u+v1icqYk/lmkkOl2wTfAPwGZjzK8j1OnXdM1BRCZh/Vsc66L4eotIetM61kXADa2qLQJmB3v3lABVTc0aXSjiL63u/P5ChP6N3Qb8LUydJcA0EckONmVMC5Z1OhGZAfwQuMYYUxehTjR/C50VX+g1o69HeN9o/q93pq8AW4wx5eE2duf3F5Puvrrc0QdWr5NtWFf8fxQsm4f1Rw7gwmoi2IE1P9DQLozty1ino58B64KPK4A7gTuDde4BNmL1UlgBfKkL4xsafN/1wRiavr/Q+ATrBjw7gc+BiV3879sLK5FnhpR12/eHdQA6ADRi/Qr9FtY1o3eA7cFlTrDuRODZkH3/Lfh3uAO4owvj24HVPt70N9jUy60AWNzW30IXxfd88G/rM6xk3r91fMHnp/xf74r4guXPNf3NhdTt8u8v3g8duauUUknmTG3qUUop1UGa+JVSKslo4ldKqSSjiV8ppZKMJn6llEoymviVUirJaOJXSqkko4lfKaWSzP8P0WINsOEtZBMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# loss可視化\n",
    "plt.plot(scr_DNN.loss, marker=\".\", label=\"train_loss\")\n",
    "plt.plot(scr_DNN.val_loss, marker=\".\", label=\"val_loss\")\n",
    "\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adagrad、Heinitializer, Relu\n",
    "# Node数を３００→２００→１００→５０→１０\n",
    "class ScratchDeepNeuralNetrowkClassifier2:\n",
    "    def __init__(self, lr, n_nodes1, n_nodes2, n_nodes3, n_nodes4, n_output, sigma=0.01):\n",
    "        self.lr = lr\n",
    "        self.n_nodes1 = n_nodes1\n",
    "        self.n_nodes2 = n_nodes2\n",
    "        self.n_nodes3 = n_nodes3\n",
    "        self.n_nodes4 = n_nodes4\n",
    "        self.n_output = n_output\n",
    "        self.sigma = sigma\n",
    "    \n",
    "    def fit(self, X, y, X_val=None, y_val=None, epoch=20, batch_size=20):\n",
    "        \"\"\"\n",
    "        ニューラルネットワーク分類器を学習する。\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (n_samples, self.n_features)\n",
    "            訓練データの特徴量\n",
    "        y : 次の形のndarray, shape (n_samples, )\n",
    "            訓練データの正解値\n",
    "        X_val : 次の形のndarray, shape (n_samples, self.n_features)\n",
    "            検証データの特徴量\n",
    "        y_val : 次の形のndarray, shape (n_samples, )\n",
    "            検証データの正解値\n",
    "        \"\"\"\n",
    "        self.n_samples = X.shape[0]\n",
    "        self.n_features = X.shape[1]\n",
    "        self.loss = np.zeros(epoch)\n",
    "        self.val_loss = np.zeros(epoch)\n",
    "        \n",
    "        # 各インスタンス初期化\n",
    "        optimizer = Adagrad(self.lr)\n",
    "        self.FC1 = FC(self.n_features, self.n_nodes1, HeInitializer(), copy.deepcopy(optimizer))\n",
    "        self.activation1 = Tanh()\n",
    "        self.FC2 = FC(self.n_nodes1, self.n_nodes2, HeInitializer(), copy.deepcopy(optimizer))\n",
    "        self.activation2 = Tanh()\n",
    "        self.FC3 = FC(self.n_nodes2, self.n_nodes3, HeInitializer(), copy.deepcopy(optimizer))\n",
    "        self.activation3 = Relu()\n",
    "        self.FC4 = FC(self.n_nodes3, self.n_nodes4, HeInitializer(), copy.deepcopy(optimizer))        \n",
    "        self.activation4 = Relu()\n",
    "        self.FC5 = FC(self.n_nodes4, self.n_output, HeInitializer(), copy.deepcopy(optimizer))\n",
    "        self.activation5 = Softmax()\n",
    "              \n",
    "        # ラベル化\n",
    "        enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "        y_train_one_hot = enc.fit_transform(y[:, np.newaxis])\n",
    "        \n",
    "        if X_val is not None and y_val is not None:\n",
    "            y_val_one_hot = enc.fit_transform(y_val[:, np.newaxis])\n",
    "        \n",
    "        for i in range(epoch):\n",
    "            get_mini_batch = GetMiniBatch(X, y_train_one_hot, batch_size=batch_size)\n",
    "            loss_list = []\n",
    "            \n",
    "            for mini_X_train, mini_y_train in get_mini_batch:\n",
    "                # フォワードプロバゲーション\n",
    "                A1 = self.FC1.forward(mini_X_train)     #shape(n_batch, n_nodes1)\n",
    "                Z1 = self.activation1.forward(A1)     #shape(n_batch, n_nodes1)\n",
    "                A2 = self.FC2.forward(Z1)        #shape(n_batch, n_nodes2\n",
    "                Z2 = self.activation2.forward(A2)       #shape(n_batch, n_nodes2)\n",
    "                A3 = self.FC3.forward(Z2)       #shape(n_batch, n_output)\n",
    "                Z3 = self.activation3.forward(A3)     #shape(n_batch , n_output)\n",
    "                A4 = self.FC4.forward(Z3)       #shape(n_batch, n_output)\n",
    "                Z4 = self.activation4.forward(A4)     #shape(n_batch , n_output)\n",
    "                A5 = self.FC5.forward(Z4)       #shape(n_batch, n_output)\n",
    "                Z5 = self.activation5.forward(A5)     #shape(n_batch , n_output)\n",
    "                \n",
    "                # バックプロバゲーション\n",
    "                dA5, error = self.activation5.backward(Z5, mini_y_train) # 交差エントロピー誤差とソフトマックスを合わせている\n",
    "                loss_list.append(error)\n",
    "                dZ4 = self.FC5.backward(dA5)     \n",
    "                dA4 = self.activation4.backward(dZ4)\n",
    "                dZ3 = self.FC4.backward(dA4)     \n",
    "                dA3 = self.activation3.backward(dZ3)\n",
    "                dZ2 = self.FC3.backward(dA3)     \n",
    "                dA2 = self.activation2.backward(dZ2)\n",
    "                dZ1 = self.FC2.backward(dA2)\n",
    "                dA1 = self.activation1.backward(dZ1)\n",
    "                dZ0 = self.FC1.backward(dA1) # dZ0は使用しない        \n",
    "            \n",
    "            self.loss[i] =np.sum(loss_list)/len(loss_list)\n",
    "            \n",
    "             # val_lossの計算\n",
    "            if X_val is not None and y_val is not None:\n",
    "                # フォワードプロバゲーション**************************\n",
    "                A1 = self.FC1.forward(X_val)     #shape(n_batch, n_nodes1)\n",
    "                Z1 = self.activation1.forward(A1)     #shape(n_batch, n_nodes1)\n",
    "                A2 = self.FC2.forward(Z1)        #shape(n_batch, n_nodes2\n",
    "                Z2 = self.activation2.forward(A2)       #shape(n_batch, n_nodes2)\n",
    "                A3 = self.FC3.forward(Z2)       #shape(n_batch, n_output)\n",
    "                Z3 = self.activation3.forward(A3)     #shape(n_batch , n_output)\n",
    "                A4 = self.FC4.forward(Z3)       #shape(n_batch, n_output)\n",
    "                Z4 = self.activation4.forward(A4)     #shape(n_batch , n_output)\n",
    "                A5 = self.FC5.forward(Z4)       #shape(n_batch, n_output)\n",
    "                Z5 = self.activation5.forward(A5)     #shape(n_batch , n_output)\n",
    "                # クロスエントロピー誤差**************************            \n",
    "                _, val_error = self.activation5.backward(Z5, y_val_one_hot) # 交差エントロピー誤差とソフトマックスを合わせている\n",
    "                self.val_loss[i]= val_error\n",
    "            \n",
    "    def predict(self,X):\n",
    "        A1 = self.FC1.forward(X)\n",
    "        Z1 = self.activation1.forward(A1)\n",
    "        A2 = self.FC2.forward(Z1)\n",
    "        Z2 = self.activation2.forward(A2)\n",
    "        A3 = self.FC3.forward(Z2)\n",
    "        Z3 = self.activation3.forward(A3)\n",
    "        A4 = self.FC4.forward(Z3)       #shape(n_batch, n_output)\n",
    "        Z4 = self.activation4.forward(A4)     #shape(n_batch , n_output)\n",
    "        A5 = self.FC5.forward(Z4)       #shape(n_batch, n_output)\n",
    "        y = self.activation5.forward(A5)     #shape(n_batch , n_output)\n",
    "        \n",
    "        pred = np.argmax(y, axis=1)\n",
    "        \n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:0.9779\n"
     ]
    }
   ],
   "source": [
    "scr_DNN2 = ScratchDeepNeuralNetrowkClassifier2(lr=0.01, n_nodes1=300, n_nodes2=200, n_nodes3=100, n_nodes4=50, n_output=10)\n",
    "\n",
    "scr_DNN2.fit(X_train, y_train, X_val, y_val, epoch=20)\n",
    "pred2 = scr_DNN2.predict(X_test)\n",
    "print(\"Accuracy:{}\".format(accuracy_score(y_test, pred2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# loss可視化\n",
    "plt.plot(scr_DNN2.loss, marker=\".\", label=\"train_loss\")\n",
    "plt.plot(scr_DNN2.val_loss, marker=\".\", label=\"val_loss\")\n",
    "\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 以下は各クラスの実装に問題がないか過程で調べるのに作成"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adagrad、SimpleInitializer, Tanhで実行\n",
    "\n",
    "class ScratchDeepNeuralNetrowkClassifier2:\n",
    "    def __init__(self, lr, n_nodes1, n_nodes2, n_output, sigma=0.01):\n",
    "        self.lr = lr\n",
    "        self.n_nodes1 = n_nodes1\n",
    "        self.n_nodes2 = n_nodes2\n",
    "        self.n_output = n_output\n",
    "        self.sigma = sigma\n",
    "    \n",
    "    \n",
    "    def fit(self, X, y, X_val=None, y_val=None, epoch=20):\n",
    "        \"\"\"\n",
    "        ニューラルネットワーク分類器を学習する。\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (n_samples, self.n_features)\n",
    "            訓練データの特徴量\n",
    "        y : 次の形のndarray, shape (n_samples, )\n",
    "            訓練データの正解値\n",
    "        X_val : 次の形のndarray, shape (n_samples, self.n_features)\n",
    "            検証データの特徴量\n",
    "        y_val : 次の形のndarray, shape (n_samples, )\n",
    "            検証データの正解値\n",
    "        \"\"\"\n",
    "        self.n_features = X.shape[1]\n",
    "        self.loss = np.zeros(epoch)\n",
    "        \n",
    "        # 各インスタンス初期化\n",
    "        optimizer = Adagrad(self.lr)\n",
    "        self.FC1 = FC(self.n_features, self.n_nodes1, SimpleInitializer(sigma=0.01), copy.deepcopy(optimizer))\n",
    "        self.activation1 = Tanh()\n",
    "        self.FC2 = FC(self.n_nodes1, self.n_nodes2, SimpleInitializer(sigma=0.01), copy.deepcopy(optimizer))\n",
    "        self.activation2 = Tanh()\n",
    "        self.FC3 = FC(self.n_nodes2, self.n_output, SimpleInitializer(sigma=0.01), copy.deepcopy(optimizer))\n",
    "        self.activation3 = Softmax()\n",
    "              \n",
    "        # ラベル化\n",
    "        enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "        y_train_one_hot = enc.fit_transform(y[:, np.newaxis])\n",
    "        \n",
    "        for i in range(epoch):\n",
    "            get_mini_batch = GetMiniBatch(X, y_train_one_hot, batch_size=20)\n",
    "            \n",
    "            for mini_X_train, mini_y_train in get_mini_batch:\n",
    "                # フォワードプロバゲーション\n",
    "                A1 = self.FC1.forward(mini_X_train)     #shape(n_batch, n_nodes1)\n",
    "                Z1 = self.activation1.forward(A1)     #shape(n_batch, n_nodes1)\n",
    "                A2 = self.FC2.forward(Z1)        #shape(n_batch, n_nodes2\n",
    "                Z2 = self.activation2.forward(A2)       #shape(n_batch, n_nodes2)\n",
    "                A3 = self.FC3.forward(Z2)       #shape(n_batch, n_output)\n",
    "                Z3 = self.activation3.forward(A3)     #shape(n_batch , n_output)\n",
    "                \n",
    "                # バックプロバゲーション\n",
    "                dA3, self.loss[i] = self.activation3.backward(Z3, mini_y_train) # 交差エントロピー誤差とソフトマックスを合わせている\n",
    "                dZ2 = self.FC3.backward(dA3)     \n",
    "                dA2 = self.activation2.backward(dZ2)\n",
    "                dZ1 = self.FC2.backward(dA2)\n",
    "                dA1 = self.activation1.backward(dZ1)\n",
    "                dZ0 = self.FC1.backward(dA1) # dZ0は使用しない        \n",
    "        \n",
    "        \n",
    "    def predict(self,X):\n",
    "        A1 = self.FC1.forward(X)\n",
    "        Z1 = self.activation1.forward(A1)\n",
    "        A2 = self.FC2.forward(Z1)\n",
    "        Z2 = self.activation2.forward(A2)\n",
    "        A3 = self.FC3.forward(Z2)\n",
    "        y = self.activation3.forward(A3)\n",
    "        \n",
    "        pred = np.argmax(y, axis=1)\n",
    "        \n",
    "        return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scr_DNN2 = ScratchDeepNeuralNetrowkClassifier2(lr=0.01, n_nodes1=400, n_nodes2=200, n_output=10)\n",
    "\n",
    "scr_DNN2.fit(X_train, y_train, epoch=20)\n",
    "\n",
    "pred2 = scr_DNN2.predict(X_test)\n",
    "print(\"Accuracy:{}\".format(accuracy_score(y_test, pred2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SGD、XavierInitializer, sigmoidで実行\n",
    "\n",
    "class ScratchDeepNeuralNetrowkClassifier3:\n",
    "    def __init__(self, lr, n_nodes1, n_nodes2, n_output, sigma=0.01):\n",
    "        self.lr = lr\n",
    "        self.n_nodes1 = n_nodes1\n",
    "        self.n_nodes2 = n_nodes2\n",
    "        self.n_output = n_output\n",
    "        self.sigma = sigma\n",
    "    \n",
    "    \n",
    "    def fit(self, X, y, X_val=None, y_val=None, epoch=20):\n",
    "        \"\"\"\n",
    "        ニューラルネットワーク分類器を学習する。\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (n_samples, self.n_features)\n",
    "            訓練データの特徴量\n",
    "        y : 次の形のndarray, shape (n_samples, )\n",
    "            訓練データの正解値\n",
    "        X_val : 次の形のndarray, shape (n_samples, self.n_features)\n",
    "            検証データの特徴量\n",
    "        y_val : 次の形のndarray, shape (n_samples, )\n",
    "            検証データの正解値\n",
    "        \"\"\"\n",
    "        self.n_features = X.shape[1]\n",
    "        self.loss = np.zeros(epoch)\n",
    "        \n",
    "        # 各インスタンス初期化\n",
    "        optimizer = SGD(self.lr)\n",
    "        self.FC1 = FC(self.n_features, self.n_nodes1, XavierInitializer(), copy.deepcopy(optimizer))\n",
    "        self.activation1 = Sigmoid()\n",
    "        self.FC2 = FC(self.n_nodes1, self.n_nodes2, XavierInitializer(), copy.deepcopy(optimizer))\n",
    "        self.activation2 = Sigmoid()\n",
    "        self.FC3 = FC(self.n_nodes2, self.n_output, XavierInitializer(), copy.deepcopy(optimizer))\n",
    "        self.activation3 = Softmax()\n",
    "              \n",
    "        # ラベル化\n",
    "        enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "        y_train_one_hot = enc.fit_transform(y[:, np.newaxis])\n",
    "        \n",
    "        for i in range(epoch):\n",
    "            get_mini_batch = GetMiniBatch(X, y_train_one_hot, batch_size=20)\n",
    "            \n",
    "            for mini_X_train, mini_y_train in get_mini_batch:\n",
    "                # フォワードプロバゲーション\n",
    "                A1 = self.FC1.forward(mini_X_train)     #shape(n_batch, n_nodes1)\n",
    "                Z1 = self.activation1.forward(A1)     #shape(n_batch, n_nodes1)\n",
    "                A2 = self.FC2.forward(Z1)        #shape(n_batch, n_nodes2\n",
    "                Z2 = self.activation2.forward(A2)       #shape(n_batch, n_nodes2)\n",
    "                A3 = self.FC3.forward(Z2)       #shape(n_batch, n_output)\n",
    "                Z3 = self.activation3.forward(A3)     #shape(n_batch , n_output)\n",
    "                \n",
    "                # バックプロバゲーション\n",
    "                dA3, self.loss[i] = self.activation3.backward(Z3, mini_y_train) # 交差エントロピー誤差とソフトマックスを合わせている\n",
    "                dZ2 = self.FC3.backward(dA3)     \n",
    "                dA2 = self.activation2.backward(dZ2)\n",
    "                dZ1 = self.FC2.backward(dA2)\n",
    "                dA1 = self.activation1.backward(dZ1)\n",
    "                dZ0 = self.FC1.backward(dA1) # dZ0は使用しない        \n",
    "        \n",
    "        \n",
    "    def predict(self,X):\n",
    "        A1 = self.FC1.forward(X)\n",
    "        Z1 = self.activation1.forward(A1)\n",
    "        A2 = self.FC2.forward(Z1)\n",
    "        Z2 = self.activation2.forward(A2)\n",
    "        A3 = self.FC3.forward(Z2)\n",
    "        y = self.activation3.forward(A3)\n",
    "        \n",
    "        pred = np.argmax(y, axis=1)\n",
    "        \n",
    "        return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scr_DNN3 = ScratchDeepNeuralNetrowkClassifier3(lr=0.01, n_nodes1=400, n_nodes2=200, n_output=10)\n",
    "\n",
    "scr_DNN3.fit(X_train, y_train, epoch=20)\n",
    "\n",
    "pred3 = scr_DNN3.predict(X_test)\n",
    "\n",
    "print(\"Accuracy:{}\".format(accuracy_score(y_test, pred3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SGD、Heinitializer, Reluで実行\n",
    "\n",
    "class ScratchDeepNeuralNetrowkClassifier4:\n",
    "    def __init__(self, lr, n_nodes1, n_nodes2, n_output, sigma=0.01):\n",
    "        self.lr = lr\n",
    "        self.n_nodes1 = n_nodes1\n",
    "        self.n_nodes2 = n_nodes2\n",
    "        self.n_output = n_output\n",
    "        self.sigma = sigma\n",
    "    \n",
    "    \n",
    "    def fit(self, X, y, X_val=None, y_val=None, epoch=20):\n",
    "        \"\"\"\n",
    "        ニューラルネットワーク分類器を学習する。\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (n_samples, self.n_features)\n",
    "            訓練データの特徴量\n",
    "        y : 次の形のndarray, shape (n_samples, )\n",
    "            訓練データの正解値\n",
    "        X_val : 次の形のndarray, shape (n_samples, self.n_features)\n",
    "            検証データの特徴量\n",
    "        y_val : 次の形のndarray, shape (n_samples, )\n",
    "            検証データの正解値\n",
    "        \"\"\"\n",
    "        self.n_features = X.shape[1]\n",
    "        self.loss = np.zeros(epoch)\n",
    "        \n",
    "        # 各インスタンス初期化\n",
    "        optimizer = SGD(self.lr)\n",
    "        self.FC1 = FC(self.n_features, self.n_nodes1, HeInitializer(), copy.deepcopy(optimizer))\n",
    "        self.activation1 = Relu()\n",
    "        self.FC2 = FC(self.n_nodes1, self.n_nodes2, HeInitializer(), copy.deepcopy(optimizer))\n",
    "        self.activation2 = Relu()\n",
    "        self.FC3 = FC(self.n_nodes2, self.n_output, HeInitializer(), copy.deepcopy(optimizer))\n",
    "        self.activation3 = Softmax()\n",
    "              \n",
    "        # ラベル化\n",
    "        enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "        y_train_one_hot = enc.fit_transform(y[:, np.newaxis])\n",
    "        \n",
    "        for i in range(epoch):\n",
    "            get_mini_batch = GetMiniBatch(X, y_train_one_hot, batch_size=20)\n",
    "            \n",
    "            for mini_X_train, mini_y_train in get_mini_batch:\n",
    "                # フォワードプロバゲーション\n",
    "                A1 = self.FC1.forward(mini_X_train)     #shape(n_batch, n_nodes1)\n",
    "                Z1 = self.activation1.forward(A1)     #shape(n_batch, n_nodes1)\n",
    "                A2 = self.FC2.forward(Z1)        #shape(n_batch, n_nodes2\n",
    "                Z2 = self.activation2.forward(A2)       #shape(n_batch, n_nodes2)\n",
    "                A3 = self.FC3.forward(Z2)       #shape(n_batch, n_output)\n",
    "                Z3 = self.activation3.forward(A3)     #shape(n_batch , n_output)\n",
    "                \n",
    "                # バックプロバゲーション\n",
    "                dA3, self.loss[i] = self.activation3.backward(Z3, mini_y_train) # 交差エントロピー誤差とソフトマックスを合わせている\n",
    "                dZ2 = self.FC3.backward(dA3)     \n",
    "                dA2 = self.activation2.backward(dZ2)\n",
    "                dZ1 = self.FC2.backward(dA2)\n",
    "                dA1 = self.activation1.backward(dZ1)\n",
    "                dZ0 = self.FC1.backward(dA1) # dZ0は使用しない        \n",
    "        \n",
    "        \n",
    "    def predict(self,X):\n",
    "        A1 = self.FC1.forward(X)\n",
    "        Z1 = self.activation1.forward(A1)\n",
    "        A2 = self.FC2.forward(Z1)\n",
    "        Z2 = self.activation2.forward(A2)\n",
    "        A3 = self.FC3.forward(Z2)\n",
    "        y = self.activation3.forward(A3)\n",
    "        \n",
    "        pred = np.argmax(y, axis=1)\n",
    "        \n",
    "        return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scr_DNN4 = ScratchDeepNeuralNetrowkClassifier4(lr=0.01, n_nodes1=400, n_nodes2=200, n_output=10)\n",
    "\n",
    "scr_DNN4.fit(X_train, y_train, epoch=20)\n",
    "\n",
    "pred4 = scr_DNN4.predict(X_test)\n",
    "print(\"Accuracy:{}\".format(accuracy_score(y_test, pred4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 自習\n",
    "- sklearnモデル[sklearn.neural_network.MLPClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html)を利用してみる\n",
    "- パラメータが非常に多い。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:0.9754\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "clf = MLPClassifier(random_state=1, max_iter=300, early_stopping=True)\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "pred3 = clf.predict(X_test)\n",
    "print(\"Accuracy:{}\".format(accuracy_score(y_test, pred3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### スクラッチモデルをsklearnと同じようにインスタンス時のパラメータでinitilizerやoptimizerを選べるようにしてみる"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SGD, SimpleInitializer, Tanh \n",
    "# val_loss追加\n",
    "# loss計算をエポックごとの平均値を取得し格納\n",
    "class ScratchDeepNeuralNetrowkClassifier５:\n",
    "    def __init__(self, lr, n_nodes1, n_nodes2, n_output, sigma=0.01):\n",
    "        self.lr = lr\n",
    "        self.n_nodes1 = n_nodes1\n",
    "        self.n_nodes2 = n_nodes2\n",
    "        self.n_output = n_output\n",
    "        self.sigma = sigma\n",
    "        \n",
    "    def fit(self, X, y, X_val=None, y_val=None, epoch=20, batch_size=20):\n",
    "        \"\"\"\n",
    "        ニューラルネットワーク分類器を学習する。\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (n_samples, self.n_features)\n",
    "            訓練データの特徴量\n",
    "        y : 次の形のndarray, shape (n_samples, )\n",
    "            訓練データの正解値\n",
    "        X_val : 次の形のndarray, shape (n_samples, self.n_features)\n",
    "            検証データの特徴量\n",
    "        y_val : 次の形のndarray, shape (n_samples, )\n",
    "            検証データの正解値\n",
    "        \"\"\"\n",
    "        self.n_samples = X.shape[0]\n",
    "        self.n_features = X.shape[1]\n",
    "        self.loss = np.zeros(epoch)\n",
    "        self.val_loss = np.zeros(epoch)\n",
    "    \n",
    "        # 各インスタンス初期化\n",
    "        optimizer = SGD(self.lr)\n",
    "        self.FC1 = FC(self.n_features, self.n_nodes1, SimpleInitializer(self.sigma), optimizer)\n",
    "        self.activation1 = Tanh()\n",
    "        self.FC2 = FC(self.n_nodes1, self.n_nodes2, SimpleInitializer(self.sigma), optimizer)\n",
    "        self.activation2 = Tanh()\n",
    "        self.FC3 = FC(self.n_nodes2, self.n_output, SimpleInitializer(self.sigma), optimizer)\n",
    "        self.activation3 = Softmax()\n",
    "              \n",
    "        # ラベル化\n",
    "        enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "        y_train_one_hot = enc.fit_transform(y[:, np.newaxis])\n",
    "        \n",
    "        loss_list = []\n",
    "        \n",
    "        for i in range(epoch):\n",
    "            get_mini_batch = GetMiniBatch(X, y_train_one_hot, batch_size=batch_size)\n",
    "            \n",
    "            for mini_X_train, mini_y_train in get_mini_batch:\n",
    "                # フォワードプロバゲーション\n",
    "                A1 = self.FC1.forward(mini_X_train)     #shape(n_batch, n_nodes1)\n",
    "                Z1 = self.activation1.forward(A1)     #shape(n_batch, n_nodes1)\n",
    "                A2 = self.FC2.forward(Z1)        #shape(n_batch, n_nodes2\n",
    "                Z2 = self.activation2.forward(A2)       #shape(n_batch, n_nodes2)\n",
    "                A3 = self.FC3.forward(Z2)       #shape(n_batch, n_output)\n",
    "                Z3 = self.activation3.forward(A3)     #shape(n_batch , n_output)\n",
    "                \n",
    "                # バックプロバゲーション\n",
    "                dA3, error = self.activation3.backward(Z3, mini_y_train) # 交差エントロピー誤差とソフトマックスを合わせている\n",
    "                loss_list.append(error)\n",
    "                dZ2 = self.FC3.backward(dA3)     \n",
    "                dA2 = self.activation2.backward(dZ2)\n",
    "                dZ1 = self.FC2.backward(dA2)\n",
    "                dA1 = self.activation1.backward(dZ1)\n",
    "                dZ0 = self.FC1.backward(dA1) # dZ0は使用しない        \n",
    "            \n",
    "            self.loss[i] = np.sum(loss_list) / len(loss_list)\n",
    "        \n",
    "        if X_val is not None and y_val is not None:\n",
    "            y_val_one_hot = enc.fit_transform(y_val[:, np.newaxis])\n",
    "            \n",
    "            for i in range(epoch):\n",
    "                get_mini_batch = GetMiniBatch(X_val, y_val_one_hot, batch_size=20)\n",
    "                val_loss_list = []\n",
    "\n",
    "                for mini_X_val, mini_y_val in get_mini_batch:\n",
    "                    # フォワードプロバゲーション**************************\n",
    "                    A1 = self.FC1.forward(mini_X_val)     #shape(n_batch, n_nodes1)\n",
    "                    Z1 = self.activation1.forward(A1)     #shape(n_batch, n_nodes1)\n",
    "                    A2 = self.FC2.forward(Z1)        #shape(n_batch, n_nodes2\n",
    "                    Z2 = self.activation2.forward(A2)       #shape(n_batch, n_nodes2)\n",
    "                    A3 = self.FC3.forward(Z2)       #shape(n_batch, n_output)\n",
    "                    Z3 = self.activation3.forward(A3)     #shape(n_batch , n_output)\n",
    "                    \n",
    "                    # バックプロバゲーション**************************            \n",
    "                    dA3, val_error = self.activation3.backward(Z3, mini_y_val) # 交差エントロピー誤差とソフトマックスを合わせている\n",
    "                    val_loss_list.append(error)\n",
    "                    dZ2 = self.FC3.backward(dA3)     \n",
    "                    dA2 = self.activation2.backward(dZ2)\n",
    "                    dZ1 = self.FC2.backward(dA2)\n",
    "                    dA1 = self.activation1.backward(dZ1)\n",
    "                    dZ0 = self.FC1.backward(dA1) # dZ0は使用しない        \n",
    "                \n",
    "                self.val_loss[i] = np.sum(val_loss_list) / len(val_loss_list)\n",
    "        \n",
    "    def predict(self,X):\n",
    "        A1 = self.FC1.forward(X)\n",
    "        Z1 = self.activation1.forward(A1)\n",
    "        A2 = self.FC2.forward(Z1)\n",
    "        Z2 = self.activation2.forward(A2)\n",
    "        A3 = self.FC3.forward(Z2)\n",
    "        y = self.activation3.forward(A3)\n",
    "        \n",
    "        pred = np.argmax(y, axis=1)\n",
    "        \n",
    "        return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 上記は個人的宿題"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "426.667px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "747.205px",
    "left": "1763.33px",
    "right": "20px",
    "top": "120px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
