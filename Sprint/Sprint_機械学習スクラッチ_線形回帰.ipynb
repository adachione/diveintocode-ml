{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sprint 機械学習スクラッチ 線形回帰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題1】仮定関数\n",
    "以下の数式で表される線形回帰の仮定関数を実装してください。メソッドの雛形を用意してあります。\n",
    "\n",
    "$$\n",
    "hθ(x)=θ_0x_0+θ_1x_1+...+θj_xj+...+θ_nx_n.(x_0=1)\n",
    "$$\n",
    "\n",
    "$x$ : 特徴量ベクトル\n",
    "\n",
    "\n",
    "$\\theta$ : パラメータベクトル\n",
    "\n",
    "\n",
    "$n$ : 特徴量の数\n",
    "\n",
    "\n",
    "$x_j$ : j番目の特徴量\n",
    "\n",
    "\n",
    "$\\theta_j$ : j番目のパラメータ（重み）\n",
    "\n",
    "\n",
    "特徴量の数$n$は任意の値に対応できる実装にしてください。\n",
    "\n",
    "\n",
    "なお、ベクトル形式で表すと以下のようになります。\n",
    "\n",
    "\n",
    "$$\n",
    "hθ(x)=θT⋅x\n",
    "$$\n",
    "雛形\n",
    "\n",
    "\n",
    "クラスの外から呼び出すことがないメソッドのため、Pythonの慣例としてアンダースコアを先頭にひとつつけています。\n",
    "\n",
    "## 【問題2】最急降下法\n",
    "最急降下法により学習させる実装を行なってください。以下の式で表されるパラメータの更新式のメソッド_gradient_descentを追加し、fit\n",
    "メソッドから呼び出すようにしてください。\n",
    "\n",
    "$$\n",
    "θj:=θj−\n",
    "α\n",
    "1\n",
    "m\n",
    "m\n",
    "∑\n",
    "i\n",
    "=\n",
    "1\n",
    " \n",
    "[\n",
    "(\n",
    "h\n",
    "θ\n",
    "(\n",
    "x\n",
    "(\n",
    "i\n",
    ")\n",
    ")\n",
    "−\n",
    "y\n",
    "(\n",
    "i\n",
    ")\n",
    ")\n",
    "x\n",
    "(\n",
    "i\n",
    ")\n",
    "j\n",
    "]\n",
    "$$\n",
    "\n",
    "$\\alpha$ : 学習率\n",
    "\n",
    "\n",
    "$i$ : サンプルのインデックス\n",
    "\n",
    "\n",
    "$j$ : 特徴量のインデックス\n",
    "\n",
    "\n",
    "雛形\n",
    "\n",
    "\n",
    "ScratchLinearRegressionクラスへ以下のメソッドを追加してください。コメントアウト部分の説明も記述してください。\n",
    "\n",
    "## 【問題3】推定\n",
    "推定する仕組みを実装してください。ScratchLinearRegressionクラスの雛形に含まれるpredictメソッドに書き加えてください。\n",
    "\n",
    "\n",
    "仮定関数 $h_\\theta(x)$ の出力が推定結果です。\n",
    "\n",
    "## 【問題4】平均二乗誤差\n",
    "線形回帰の指標値として用いられる平均二乗誤差（mean square error, MSE）の関数を作成してください。\n",
    "\n",
    "\n",
    "平均二乗誤差関数は回帰問題全般で使える関数のため、ScratchLinearRegressionクラスのメソッドではなく、別の関数として作成してください。雛形を用意してあります。\n",
    "\n",
    "\n",
    "平均二乗誤差は以下の数式で表されます。\n",
    "\n",
    "\n",
    "L\n",
    "(\n",
    "θ\n",
    ")\n",
    "=\n",
    "1\n",
    "m\n",
    "m\n",
    "∑\n",
    "i\n",
    "=\n",
    "1\n",
    " \n",
    "(\n",
    "h\n",
    "θ\n",
    "(\n",
    "x\n",
    "(\n",
    "i\n",
    ")\n",
    ")\n",
    "−\n",
    "y\n",
    "(\n",
    "i\n",
    ")\n",
    ")\n",
    "2\n",
    ".\n",
    "\n",
    "$m$ : 入力されるデータの数\n",
    "\n",
    "\n",
    "$h_\\theta()$ : 仮定関数\n",
    "\n",
    "\n",
    "$x^{(i)}$ : i番目のサンプルの特徴量ベクトル\n",
    "\n",
    "\n",
    "$y^{(i)}$ : i番目のサンプルの正解値\n",
    "\n",
    "\n",
    "なお、最急降下法のための目的関数（損失関数）としては、これを2で割ったものを使用します。（問題5, 9）\n",
    "\n",
    "## 【問題5】目的関数\n",
    "以下の数式で表される線形回帰の 目的関数（損失関数） を実装してください。そして、これをself.loss, self.val_lossに記録するようにしてください。\n",
    "\n",
    "\n",
    "目的関数（損失関数） \n",
    "J\n",
    "(\n",
    "θ\n",
    ")\n",
    " は次の式です。\n",
    "\n",
    "\n",
    "J\n",
    "(\n",
    "θ\n",
    ")\n",
    "=\n",
    "1\n",
    "2\n",
    "m\n",
    "m\n",
    "∑\n",
    "i\n",
    "=\n",
    "1\n",
    " \n",
    "(\n",
    "h\n",
    "θ\n",
    "(\n",
    "x\n",
    "(\n",
    "i\n",
    ")\n",
    ")\n",
    "−\n",
    "y\n",
    "(\n",
    "i\n",
    ")\n",
    ")\n",
    "2\n",
    ".\n",
    "\n",
    "m\n",
    " : 入力されるデータの数\n",
    "\n",
    "\n",
    "h\n",
    "θ\n",
    "(\n",
    ")\n",
    " : 仮定関数\n",
    "\n",
    "\n",
    "x\n",
    "(\n",
    "i\n",
    ")\n",
    " : i番目のサンプルの特徴量ベクトル\n",
    "\n",
    "\n",
    "y\n",
    "(\n",
    "i\n",
    ")\n",
    " : i番目のサンプルの正解値"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScratchLinearRegression():\n",
    "    \"\"\"\n",
    "    線形回帰のスクラッチ実装\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    num_iter : int\n",
    "      イテレーション数\n",
    "    lr : float\n",
    "      学習率\n",
    "    no_bias : bool\n",
    "      バイアス項を入れない場合はTrue\n",
    "    verbose : bool\n",
    "      学習過程を出力する場合はTrue\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    self.coef_ : 次の形のndarray, shape (n_features,)\n",
    "      パラメータ\n",
    "    self.loss : 次の形のndarray, shape (self.iter,)\n",
    "      訓練データに対する損失の記録\n",
    "    self.val_loss : 次の形のndarray, shape (self.iter,)\n",
    "      検証データに対する損失の記録\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_iter, lr, no_bias, verbose):\n",
    "        # ハイパーパラメータを属性として記録\n",
    "        self.iter = num_iter\n",
    "        self.lr = lr\n",
    "        self.no_bias = no_bias\n",
    "        self.verbose = verbose\n",
    "        # 損失を記録する配列を用意\n",
    "        self.loss = np.zeros(self.iter)\n",
    "        self.val_loss = np.zeros(self.iter)\n",
    "        #np.random.seed(0)\n",
    "\n",
    "    def fit(self, X, y, X_val=None, y_val=None):\n",
    "        \"\"\"\n",
    "        線形回帰を学習する。検証データが入力された場合はそれに対する損失と精度もイテレーションごとに計算する。\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (n_samples, n_features)\n",
    "            訓練データの特徴量\n",
    "        y : 次の形のndarray, shape (n_samples, )\n",
    "            訓練データの正解値\n",
    "        X_val : 次の形のndarray, shape (n_samples, n_features)\n",
    "            検証データの特徴量\n",
    "        y_val : 次の形のndarray, shape (n_samples, )\n",
    "            検証データの正解値\n",
    "        \"\"\"\n",
    "        \n",
    "        # 訓練データ・学習データのインスタンス変数化\n",
    "        self.X, self.y, self.X_val, self.y_val, self.theta = self._setting(X, y, X_val, y_val)\n",
    "            \n",
    "\n",
    "        for i in range(self.iter):\n",
    "            self.linear_hypo = self._linear_hypothesis(self.X)\n",
    "            #print(\"linear_hypothetis\", self.linear_hypo)\n",
    "            #print(\"linear_hypothetis.shape\", self.linear_hypo.shape)\n",
    "            \n",
    "            self.grad = self._gradient_descent(self.X, self.y)\n",
    "            #print(\"[theta]\", self.grad)\n",
    "            #print(\"[theta].shape\", self.grad.shape)\n",
    "            \n",
    "            self.pred = self.predict(self.X)\n",
    "            #print(\"[pred]\", self.pred)\n",
    "            \n",
    "            self.loss[i] = self.loss_func(self.y)\n",
    "            \n",
    "            if self.verbose:\n",
    "                # verboseをTrueにした際は学習過程を出力\n",
    "                print(\"iter{}:[loss]:{}\".format(i, self.loss[i]))\n",
    "\n",
    "            if X_val is not None and y_val is not None:\n",
    "                self.val_loss[i] = self.loss_func(self.y_val)\n",
    "                print(\"iter{}:[val_loss]:{}\".format(i, self.val_loss[i]))\n",
    "                \n",
    "\n",
    "    def _setting(self, X, y, X_val, y_val):       \n",
    "        X_copy = np.copy(X)\n",
    "        y_copy = np.copy(y)\n",
    "        \n",
    "        if X_val is not None and y_val is not None:\n",
    "            X_val_copy = np.copy(X_val)\n",
    "            y_val_copy = np.copy(y_val)\n",
    "                \n",
    "        if self.no_bias == False:\n",
    "            X_ones = np.ones(X.shape[0]).reshape(-1, 1)\n",
    "            X_val_ones = np.ones(X_val.shape[0]).reshape(-1, 1)                \n",
    "            X_copy = np.hstack((X_ones, X_copy))\n",
    "            X_val_copy = np.hstack((X_val_ones, X_val_copy))\n",
    "            np.random.seed(0)\n",
    "            theta = np.random.randn(X.shape[1] + 1)\n",
    "            return X_copy, y_copy, X_val_copy, y_val_copy, theta\n",
    "\n",
    "        else:\n",
    "            theta = np.random.randn(X.shape[1])\n",
    "            return X_copy, y_copy, X_val_copy, y_val_copy, theta\n",
    "\n",
    "    def _linear_hypothesis(self, X):\n",
    "        \"\"\"\n",
    "        線形の仮定関数を計算する\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (n_samples, n_features)\n",
    "          訓練データ\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "          次の形のndarray, shape (n_samples, 1)\n",
    "          線形の仮定関数による推定結果\n",
    "\n",
    "        \"\"\"\n",
    "        self.hypo = np.dot(self.X, self.theta.T)\n",
    "\n",
    "        return self.hypo\n",
    "\n",
    "    def _gradient_descent(self, X, y, alpha=0.01, error=0):\n",
    "        \"\"\"\n",
    "        最急降下法\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (n_samples, n_features)\n",
    "          訓練データ\n",
    "\n",
    "        alpha：学習率\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "\n",
    "        \"\"\"\n",
    "        m = len(self.y)\n",
    "\n",
    "        grad = np.dot(self.X.T,(self.linear_hypo-self.y))\n",
    "        \n",
    "        grad_average = np.average(grad, axis=1)\n",
    "        \n",
    "        self.theta = self.theta - alpha*grad_average\n",
    "                \n",
    "        return self.theta\n",
    "\n",
    "    def predict(self, X):#self.linear_hypo\n",
    "        \"\"\"\n",
    "        線形回帰を使い推定する。\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (n_samples, n_features)\n",
    "            サンプル\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            次の形のndarray, shape (n_samples, 1)\n",
    "            線形回帰による推定結果\n",
    "        \"\"\"\n",
    "        # print(self.X)\n",
    "\n",
    "        pred = np.dot(self.X, self.grad.T)\n",
    "\n",
    "        return pred\n",
    "    \n",
    "    def loss_func(self, y):\n",
    "        \"\"\"\n",
    "        損失関数の計算\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        y : 次の形のndarray, shape (n_samples,)\n",
    "        訓練データまたはテストデータ\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        \"\"\"\n",
    "        loss = np.average((self.pred - y) ** 2) / 2\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 問題1  単独で作成した関数\n",
    "\n",
    "def _linear_hypothesis(X):\n",
    "    \"\"\"\n",
    "    線形の仮定関数を計算する\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : 次の形のndarray, shape (n_samples, n_features)\n",
    "      訓練データ\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "      次の形のndarray, shape (n_samples, 1)\n",
    "      線形の仮定関数による推定結果\n",
    "\n",
    "    \"\"\"\n",
    "    line_hypo = np.dot(X, theta.T)\n",
    "    \n",
    "    return line_hypo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 問題２　単独で作成した関数\n",
    "\n",
    "def _gradient_descent_1(X, y, alpha=0.1, error=0):\n",
    "    \"\"\"\n",
    "    最急降下法\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : 次の形のndarray, shape (n_samples, n_features)\n",
    "      訓練データ\n",
    "    \n",
    "    alpha：学習率\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    \n",
    "    \"\"\"\n",
    "    # データフレーム型だった場合にnd_array型に変換\n",
    "    if type(X) is pd.core.frame.DataFrame:\n",
    "        X = X.values\n",
    "        \n",
    "    np.random.seed(0)\n",
    "    theta = np.random.random(size=X.shape[1])\n",
    "\n",
    "    y = np.ones(5)\n",
    "    \n",
    "    # gradientを求める\n",
    "    gradient = 0\n",
    "    for x in range(X.shape[1]):\n",
    "        gradient += alpha * (_linear_hypothesis(X)[x] - y[x]) * X[x, :]\n",
    "    \n",
    "    gradient = gradient / X.shape[1]\n",
    "    \n",
    "    # thetaの更新\n",
    "    error = theta - gradient\n",
    "    \n",
    "    return error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 問題3 単独で作成した関数\n",
    "def predict(X):\n",
    "    \"\"\"\n",
    "    線形回帰を使い推定する。\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : 次の形のndarray, shape (n_samples, n_features)\n",
    "        サンプル\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        次の形のndarray, shape (n_samples, 1)\n",
    "        線形回帰による推定結果\n",
    "    \"\"\"\n",
    "\n",
    "    theta = np.random.random(size=X.shape[1])\n",
    "    theta = np.dot(X, _gradient_descent(X).T)    \n",
    "    \n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 問題4\n",
    "\n",
    "def MSE(y_pred, y):\n",
    "    \"\"\"\n",
    "    平均二乗誤差の計算\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_pred : 次の形のndarray, shape (n_samples,)\n",
    "      推定した値\n",
    "    y : 次の形のndarray, shape (n_samples,)\n",
    "      正解値\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    mse : numpy.float\n",
    "      平均二乗誤差\n",
    "    \"\"\"\n",
    "    mse = np.average((y_pred - y) ** 2)\n",
    "    \n",
    "    return mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題6】学習と推定\n",
    "機械学習スクラッチ入門のSprintで用意したHouse Pricesコンペティションのデータに対してスクラッチ実装の学習と推定を行なってください。\n",
    "\n",
    "\n",
    "scikit-learnによる実装と比べ、正しく動いているかを確認してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>MSZoning</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>Street</th>\n",
       "      <th>Alley</th>\n",
       "      <th>LotShape</th>\n",
       "      <th>LandContour</th>\n",
       "      <th>Utilities</th>\n",
       "      <th>LotConfig</th>\n",
       "      <th>...</th>\n",
       "      <th>PoolArea</th>\n",
       "      <th>PoolQC</th>\n",
       "      <th>Fence</th>\n",
       "      <th>MiscFeature</th>\n",
       "      <th>MiscVal</th>\n",
       "      <th>MoSold</th>\n",
       "      <th>YrSold</th>\n",
       "      <th>SaleType</th>\n",
       "      <th>SaleCondition</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>65.0</td>\n",
       "      <td>8450</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Inside</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>208500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>80.0</td>\n",
       "      <td>9600</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>FR2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2007</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>181500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>68.0</td>\n",
       "      <td>11250</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Inside</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>223500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>70</td>\n",
       "      <td>RL</td>\n",
       "      <td>60.0</td>\n",
       "      <td>9550</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Corner</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2006</td>\n",
       "      <td>WD</td>\n",
       "      <td>Abnorml</td>\n",
       "      <td>140000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>84.0</td>\n",
       "      <td>14260</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>FR2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>250000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 80 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    MSSubClass MSZoning  LotFrontage  LotArea Street Alley LotShape  \\\n",
       "Id                                                                    \n",
       "1           60       RL         65.0     8450   Pave   NaN      Reg   \n",
       "2           20       RL         80.0     9600   Pave   NaN      Reg   \n",
       "3           60       RL         68.0    11250   Pave   NaN      IR1   \n",
       "4           70       RL         60.0     9550   Pave   NaN      IR1   \n",
       "5           60       RL         84.0    14260   Pave   NaN      IR1   \n",
       "\n",
       "   LandContour Utilities LotConfig    ...     PoolArea PoolQC Fence  \\\n",
       "Id                                    ...                             \n",
       "1          Lvl    AllPub    Inside    ...            0    NaN   NaN   \n",
       "2          Lvl    AllPub       FR2    ...            0    NaN   NaN   \n",
       "3          Lvl    AllPub    Inside    ...            0    NaN   NaN   \n",
       "4          Lvl    AllPub    Corner    ...            0    NaN   NaN   \n",
       "5          Lvl    AllPub       FR2    ...            0    NaN   NaN   \n",
       "\n",
       "   MiscFeature MiscVal MoSold  YrSold  SaleType  SaleCondition  SalePrice  \n",
       "Id                                                                         \n",
       "1          NaN       0      2    2008        WD         Normal     208500  \n",
       "2          NaN       0      5    2007        WD         Normal     181500  \n",
       "3          NaN       0      9    2008        WD         Normal     223500  \n",
       "4          NaN       0      2    2006        WD        Abnorml     140000  \n",
       "5          NaN       0     12    2008        WD         Normal     250000  \n",
       "\n",
       "[5 rows x 80 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../Week4/train.csv\")\n",
    "df = df.set_index(\"Id\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1095, 2)\n",
      "(365, 2)\n",
      "(1095, 1)\n",
      "(365, 1)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# X変数には2つ、y変数にはSalePriceを抽出\n",
    "X = df.loc[:, [\"GrLivArea\", \"YearBuilt\"]]\n",
    "y = df.loc[:, [\"SalePrice\"]]\n",
    "\n",
    "# スクラッチ関数で分割\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 標準化せずに実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  98.588892  , 1041.18622755]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([-2021422.10210011])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# sklearn\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "lr_pred = lr.predict(X_test)\n",
    "\n",
    "# 係数\n",
    "display(lr.coef_)\n",
    "# 切片\n",
    "display(lr.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題7】学習曲線のプロット\n",
    "学習曲線を表示する関数を作成し、実行してください。グラフを見て損失が適切に下がっているかどうか確認してください。\n",
    "\n",
    "\n",
    "線形回帰クラスの雛形ではself.loss, self.val_lossに損失を記録しておくようになっているため、入力にはこれを利用してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter0:[loss]:8.103016024494817e+25\n",
      "iter0:[val_loss]:8.1030160235461e+25\n",
      "iter1:[loss]:3.7256245732678916e+41\n",
      "iter1:[val_loss]:3.725624573267891e+41\n",
      "iter2:[loss]:1.7165104408148675e+57\n",
      "iter2:[val_loss]:1.7165104408148658e+57\n",
      "iter3:[loss]:7.908494361975749e+72\n",
      "iter3:[val_loss]:7.908494361975752e+72\n",
      "iter4:[loss]:3.643687890631828e+88\n",
      "iter4:[val_loss]:3.643687890631827e+88\n",
      "iter5:[loss]:1.678759677464093e+104\n",
      "iter5:[val_loss]:1.6787596774640928e+104\n",
      "iter6:[loss]:7.734564922328338e+119\n",
      "iter6:[val_loss]:7.734564922328336e+119\n",
      "iter7:[loss]:3.563553219724717e+135\n",
      "iter7:[val_loss]:3.563553219724716e+135\n",
      "iter8:[loss]:1.641839156737938e+151\n",
      "iter8:[val_loss]:1.641839156737938e+151\n",
      "iter9:[loss]:7.564460667171353e+166\n",
      "iter9:[val_loss]:7.564460667171351e+166\n",
      "iter10:[loss]:3.485180929590642e+182\n",
      "iter10:[val_loss]:3.485180929590641e+182\n",
      "iter11:[loss]:1.6057306193283877e+198\n",
      "iter11:[val_loss]:1.6057306193283875e+198\n",
      "iter12:[loss]:7.398097470226826e+213\n",
      "iter12:[val_loss]:7.398097470226825e+213\n",
      "iter13:[loss]:3.408532260652079e+229\n",
      "iter13:[val_loss]:3.408532260652079e+229\n",
      "iter14:[loss]:1.5704162074995974e+245\n",
      "iter14:[val_loss]:1.5704162074995971e+245\n",
      "iter15:[loss]:7.235393055383938e+260\n",
      "iter15:[val_loss]:7.235393055383941e+260\n",
      "iter16:[loss]:3.3335693057607186e+276\n",
      "iter16:[val_loss]:3.333569305760719e+276\n",
      "iter17:[loss]:1.5358784562562121e+292\n",
      "iter17:[val_loss]:1.5358784562562123e+292\n",
      "iter18:[loss]:inf\n",
      "iter18:[val_loss]:inf\n",
      "iter19:[loss]:inf\n",
      "iter19:[val_loss]:inf\n",
      "iter20:[loss]:inf\n",
      "iter20:[val_loss]:inf\n",
      "iter21:[loss]:inf\n",
      "iter21:[val_loss]:inf\n",
      "iter22:[loss]:inf\n",
      "iter22:[val_loss]:inf\n",
      "iter23:[loss]:inf\n",
      "iter23:[val_loss]:inf\n",
      "iter24:[loss]:inf\n",
      "iter24:[val_loss]:inf\n",
      "iter25:[loss]:inf\n",
      "iter25:[val_loss]:inf\n",
      "iter26:[loss]:inf\n",
      "iter26:[val_loss]:inf\n",
      "iter27:[loss]:inf\n",
      "iter27:[val_loss]:inf\n",
      "iter28:[loss]:inf\n",
      "iter28:[val_loss]:inf\n",
      "iter29:[loss]:inf\n",
      "iter29:[val_loss]:inf\n",
      "iter30:[loss]:inf\n",
      "iter30:[val_loss]:inf\n",
      "iter31:[loss]:inf\n",
      "iter31:[val_loss]:inf\n",
      "iter32:[loss]:inf\n",
      "iter32:[val_loss]:inf\n",
      "iter33:[loss]:inf\n",
      "iter33:[val_loss]:inf\n",
      "iter34:[loss]:inf\n",
      "iter34:[val_loss]:inf\n",
      "iter35:[loss]:inf\n",
      "iter35:[val_loss]:inf\n",
      "iter36:[loss]:inf\n",
      "iter36:[val_loss]:inf\n",
      "iter37:[loss]:inf\n",
      "iter37:[val_loss]:inf\n",
      "iter38:[loss]:inf\n",
      "iter38:[val_loss]:inf\n",
      "iter39:[loss]:nan\n",
      "iter39:[val_loss]:nan\n",
      "iter40:[loss]:nan\n",
      "iter40:[val_loss]:nan\n",
      "iter41:[loss]:nan\n",
      "iter41:[val_loss]:nan\n",
      "iter42:[loss]:nan\n",
      "iter42:[val_loss]:nan\n",
      "iter43:[loss]:nan\n",
      "iter43:[val_loss]:nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/adachi-yuya/.pyenv/versions/anaconda3-5.3.1/lib/python3.7/site-packages/ipykernel_launcher.py:178: RuntimeWarning: overflow encountered in square\n",
      "/Users/adachi-yuya/.pyenv/versions/anaconda3-5.3.1/lib/python3.7/site-packages/numpy/core/_methods.py:75: RuntimeWarning: overflow encountered in reduce\n",
      "  ret = umr_sum(arr, axis, dtype, out, keepdims)\n",
      "/Users/adachi-yuya/.pyenv/versions/anaconda3-5.3.1/lib/python3.7/site-packages/ipykernel_launcher.py:142: RuntimeWarning: invalid value encountered in subtract\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter44:[loss]:nan\n",
      "iter44:[val_loss]:nan\n",
      "iter45:[loss]:nan\n",
      "iter45:[val_loss]:nan\n",
      "iter46:[loss]:nan\n",
      "iter46:[val_loss]:nan\n",
      "iter47:[loss]:nan\n",
      "iter47:[val_loss]:nan\n",
      "iter48:[loss]:nan\n",
      "iter48:[val_loss]:nan\n",
      "iter49:[loss]:nan\n",
      "iter49:[val_loss]:nan\n",
      "iter50:[loss]:nan\n",
      "iter50:[val_loss]:nan\n",
      "iter51:[loss]:nan\n",
      "iter51:[val_loss]:nan\n",
      "iter52:[loss]:nan\n",
      "iter52:[val_loss]:nan\n",
      "iter53:[loss]:nan\n",
      "iter53:[val_loss]:nan\n",
      "iter54:[loss]:nan\n",
      "iter54:[val_loss]:nan\n",
      "iter55:[loss]:nan\n",
      "iter55:[val_loss]:nan\n",
      "iter56:[loss]:nan\n",
      "iter56:[val_loss]:nan\n",
      "iter57:[loss]:nan\n",
      "iter57:[val_loss]:nan\n",
      "iter58:[loss]:nan\n",
      "iter58:[val_loss]:nan\n",
      "iter59:[loss]:nan\n",
      "iter59:[val_loss]:nan\n",
      "iter60:[loss]:nan\n",
      "iter60:[val_loss]:nan\n",
      "iter61:[loss]:nan\n",
      "iter61:[val_loss]:nan\n",
      "iter62:[loss]:nan\n",
      "iter62:[val_loss]:nan\n",
      "iter63:[loss]:nan\n",
      "iter63:[val_loss]:nan\n",
      "iter64:[loss]:nan\n",
      "iter64:[val_loss]:nan\n",
      "iter65:[loss]:nan\n",
      "iter65:[val_loss]:nan\n",
      "iter66:[loss]:nan\n",
      "iter66:[val_loss]:nan\n",
      "iter67:[loss]:nan\n",
      "iter67:[val_loss]:nan\n",
      "iter68:[loss]:nan\n",
      "iter68:[val_loss]:nan\n",
      "iter69:[loss]:nan\n",
      "iter69:[val_loss]:nan\n",
      "iter70:[loss]:nan\n",
      "iter70:[val_loss]:nan\n",
      "iter71:[loss]:nan\n",
      "iter71:[val_loss]:nan\n",
      "iter72:[loss]:nan\n",
      "iter72:[val_loss]:nan\n",
      "iter73:[loss]:nan\n",
      "iter73:[val_loss]:nan\n",
      "iter74:[loss]:nan\n",
      "iter74:[val_loss]:nan\n",
      "iter75:[loss]:nan\n",
      "iter75:[val_loss]:nan\n",
      "iter76:[loss]:nan\n",
      "iter76:[val_loss]:nan\n",
      "iter77:[loss]:nan\n",
      "iter77:[val_loss]:nan\n",
      "iter78:[loss]:nan\n",
      "iter78:[val_loss]:nan\n",
      "iter79:[loss]:nan\n",
      "iter79:[val_loss]:nan\n",
      "iter80:[loss]:nan\n",
      "iter80:[val_loss]:nan\n",
      "iter81:[loss]:nan\n",
      "iter81:[val_loss]:nan\n",
      "iter82:[loss]:nan\n",
      "iter82:[val_loss]:nan\n",
      "iter83:[loss]:nan\n",
      "iter83:[val_loss]:nan\n",
      "iter84:[loss]:nan\n",
      "iter84:[val_loss]:nan\n",
      "iter85:[loss]:nan\n",
      "iter85:[val_loss]:nan\n",
      "iter86:[loss]:nan\n",
      "iter86:[val_loss]:nan\n",
      "iter87:[loss]:nan\n",
      "iter87:[val_loss]:nan\n",
      "iter88:[loss]:nan\n",
      "iter88:[val_loss]:nan\n",
      "iter89:[loss]:nan\n",
      "iter89:[val_loss]:nan\n",
      "iter90:[loss]:nan\n",
      "iter90:[val_loss]:nan\n",
      "iter91:[loss]:nan\n",
      "iter91:[val_loss]:nan\n",
      "iter92:[loss]:nan\n",
      "iter92:[val_loss]:nan\n",
      "iter93:[loss]:nan\n",
      "iter93:[val_loss]:nan\n",
      "iter94:[loss]:nan\n",
      "iter94:[val_loss]:nan\n",
      "iter95:[loss]:nan\n",
      "iter95:[val_loss]:nan\n",
      "iter96:[loss]:nan\n",
      "iter96:[val_loss]:nan\n",
      "iter97:[loss]:nan\n",
      "iter97:[val_loss]:nan\n",
      "iter98:[loss]:nan\n",
      "iter98:[val_loss]:nan\n",
      "iter99:[loss]:nan\n",
      "iter99:[val_loss]:nan\n"
     ]
    }
   ],
   "source": [
    "scr_lr = ScratchLinearRegression(num_iter=100, \n",
    "                                 lr=0.01, \n",
    "                                 no_bias = False,\n",
    "                                 verbose=True)\n",
    "\n",
    "scr_lr.fit(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 損失なく上昇している、かつ、学習回数を増やすとinfになり計算不可。\n",
    "## 授業内の学習共有で発表されていた、標準化するしないが関係しているかも。\n",
    "## sklearnとスクラッチ関数それぞれ標準化して試してみる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "scaler.fit(X_train)   #後でスケーリングするために使用する平均と標準を計算します。\n",
    "X_train_scaler = scaler.transform(X_train)   # 標準化　センタリングとスケーリングによって標準化を実行する\n",
    "X_test_scaler = scaler.transform(X_test)     # 標準化　センタリングとスケーリングによって標準化を実行する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[50915.49019418, 31435.11963558]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([180733.14977169])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# sklearn\n",
    "# 標準化して実行\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train_scaler, y_train)\n",
    "lr_pred = lr.predict(X_test_scaler)\n",
    "\n",
    "# 係数\n",
    "display(lr.coef_)\n",
    "# 切片\n",
    "display(lr.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter0:[loss]:1972317562952.6665\n",
      "iter0:[val_loss]:1971162032445.973\n",
      "iter1:[loss]:161489555602709.7\n",
      "iter1:[val_loss]:161503211439222.78\n",
      "iter2:[loss]:1.5851217091586936e+16\n",
      "iter2:[val_loss]:1.5851083374321614e+16\n",
      "iter3:[loss]:1.5690015347404787e+18\n",
      "iter3:[val_loss]:1.5690028673855764e+18\n",
      "iter4:[loss]:1.5533452487013322e+20\n",
      "iter4:[val_loss]:1.553345116124728e+20\n",
      "iter5:[loss]:1.53785054388398e+22\n",
      "iter5:[val_loss]:1.5378505570755675e+22\n",
      "iter6:[loss]:1.522510483470024e+24\n",
      "iter6:[val_loss]:1.5225104821574622e+24\n",
      "iter7:[loss]:1.5073234413805409e+26\n",
      "iter7:[val_loss]:1.5073234415111414e+26\n",
      "iter8:[loss]:1.4922878900525503e+28\n",
      "iter8:[val_loss]:1.4922878900395552e+28\n",
      "iter9:[loss]:1.477402318349273e+30\n",
      "iter9:[val_loss]:1.4774023183505665e+30\n",
      "iter10:[loss]:1.4626652302237384e+32\n",
      "iter10:[val_loss]:1.4626652302236105e+32\n",
      "iter11:[loss]:1.4480751445522582e+34\n",
      "iter11:[val_loss]:1.4480751445522702e+34\n",
      "iter12:[loss]:1.43363059498535e+36\n",
      "iter12:[val_loss]:1.4336305949853474e+36\n",
      "iter13:[loss]:1.4193301298003705e+38\n",
      "iter13:[val_loss]:1.4193301298003705e+38\n",
      "iter14:[loss]:1.4051723117556138e+40\n",
      "iter14:[val_loss]:1.4051723117556133e+40\n",
      "iter15:[loss]:1.3911557179458537e+42\n",
      "iter15:[val_loss]:1.3911557179458484e+42\n",
      "iter16:[loss]:1.3772789396593377e+44\n",
      "iter16:[val_loss]:1.3772789396593385e+44\n",
      "iter17:[loss]:1.3635405822362388e+46\n",
      "iter17:[val_loss]:1.363540582236236e+46\n",
      "iter18:[loss]:1.3499392649284311e+48\n",
      "iter18:[val_loss]:1.3499392649284311e+48\n",
      "iter19:[loss]:1.3364736207607676e+50\n",
      "iter19:[val_loss]:1.3364736207607723e+50\n",
      "iter20:[loss]:1.3231422963936827e+52\n",
      "iter20:[val_loss]:1.3231422963936865e+52\n",
      "iter21:[loss]:1.3099439519871571e+54\n",
      "iter21:[val_loss]:1.3099439519871597e+54\n",
      "iter22:[loss]:1.2968772610660903e+56\n",
      "iter22:[val_loss]:1.2968772610660903e+56\n",
      "iter23:[loss]:1.283940910386956e+58\n",
      "iter23:[val_loss]:1.2839409103869542e+58\n",
      "iter24:[loss]:1.2711335998058466e+60\n",
      "iter24:[val_loss]:1.2711335998058445e+60\n",
      "iter25:[loss]:1.2584540421477826e+62\n",
      "iter25:[val_loss]:1.2584540421477838e+62\n",
      "iter26:[loss]:1.2459009630773595e+64\n",
      "iter26:[val_loss]:1.2459009630773602e+64\n",
      "iter27:[loss]:1.2334731009706614e+66\n",
      "iter27:[val_loss]:1.2334731009706637e+66\n",
      "iter28:[loss]:1.2211692067884862e+68\n",
      "iter28:[val_loss]:1.221169206788483e+68\n",
      "iter29:[loss]:1.208988043950768e+70\n",
      "iter29:[val_loss]:1.20898804395077e+70\n",
      "iter30:[loss]:1.196928388212362e+72\n",
      "iter30:[val_loss]:1.1969283882123588e+72\n",
      "iter31:[loss]:1.1849890275399423e+74\n",
      "iter31:[val_loss]:1.1849890275399433e+74\n",
      "iter32:[loss]:1.173168761990237e+76\n",
      "iter32:[val_loss]:1.1731687619902323e+76\n",
      "iter33:[loss]:1.1614664035893834e+78\n",
      "iter33:[val_loss]:1.1614664035893804e+78\n",
      "iter34:[loss]:1.1498807762135768e+80\n",
      "iter34:[val_loss]:1.1498807762135767e+80\n",
      "iter35:[loss]:1.1384107154708479e+82\n",
      "iter35:[val_loss]:1.1384107154708436e+82\n",
      "iter36:[loss]:1.1270550685840268e+84\n",
      "iter36:[val_loss]:1.1270550685840227e+84\n",
      "iter37:[loss]:1.1158126942748959e+86\n",
      "iter37:[val_loss]:1.1158126942749e+86\n",
      "iter38:[loss]:1.1046824626495092e+88\n",
      "iter38:[val_loss]:1.1046824626495051e+88\n",
      "iter39:[loss]:1.0936632550845758e+90\n",
      "iter39:[val_loss]:1.0936632550845734e+90\n",
      "iter40:[loss]:1.0827539641151045e+92\n",
      "iter40:[val_loss]:1.082753964115105e+92\n",
      "iter41:[loss]:1.071953493323055e+94\n",
      "iter41:[val_loss]:1.0719534933230586e+94\n",
      "iter42:[loss]:1.0612607572271637e+96\n",
      "iter42:[val_loss]:1.0612607572271621e+96\n",
      "iter43:[loss]:1.0506746811738201e+98\n",
      "iter43:[val_loss]:1.0506746811738201e+98\n",
      "iter44:[loss]:1.0401942012291113e+100\n",
      "iter44:[val_loss]:1.040194201229114e+100\n",
      "iter45:[loss]:1.0298182640718518e+102\n",
      "iter45:[val_loss]:1.0298182640718517e+102\n",
      "iter46:[loss]:1.0195458268877396e+104\n",
      "iter46:[val_loss]:1.019545826887736e+104\n",
      "iter47:[loss]:1.0093758572645302e+106\n",
      "iter47:[val_loss]:1.0093758572645314e+106\n",
      "iter48:[loss]:9.993073330883176e+107\n",
      "iter48:[val_loss]:9.99307333088321e+107\n",
      "iter49:[loss]:9.893392424407609e+109\n",
      "iter49:[val_loss]:9.893392424407617e+109\n",
      "iter50:[loss]:9.794705834974162e+111\n",
      "iter50:[val_loss]:9.794705834974183e+111\n",
      "iter51:[loss]:9.697003644270316e+113\n",
      "iter51:[val_loss]:9.69700364427033e+113\n",
      "iter52:[loss]:9.600276032918747e+115\n",
      "iter52:[val_loss]:9.600276032918736e+115\n",
      "iter53:[loss]:9.504513279490392e+117\n",
      "iter53:[val_loss]:9.504513279490379e+117\n",
      "iter54:[loss]:9.409705759527425e+119\n",
      "iter54:[val_loss]:9.409705759527458e+119\n",
      "iter55:[loss]:9.315843944576155e+121\n",
      "iter55:[val_loss]:9.315843944576178e+121\n",
      "iter56:[loss]:9.222918401229043e+123\n",
      "iter56:[val_loss]:9.222918401229008e+123\n",
      "iter57:[loss]:9.130919790176781e+125\n",
      "iter57:[val_loss]:9.130919790176778e+125\n",
      "iter58:[loss]:9.039838865269707e+127\n",
      "iter58:[val_loss]:9.039838865269723e+127\n",
      "iter59:[loss]:8.949666472588687e+129\n",
      "iter59:[val_loss]:8.949666472588684e+129\n",
      "iter60:[loss]:8.860393549524577e+131\n",
      "iter60:[val_loss]:8.8603935495246e+131\n",
      "iter61:[loss]:8.772011123868107e+133\n",
      "iter61:[val_loss]:8.772011123868087e+133\n",
      "iter62:[loss]:8.684510312907528e+135\n",
      "iter62:[val_loss]:8.684510312907516e+135\n",
      "iter63:[loss]:8.597882322536277e+137\n",
      "iter63:[val_loss]:8.59788232253627e+137\n",
      "iter64:[loss]:8.51211844636895e+139\n",
      "iter64:[val_loss]:8.512118446368978e+139\n",
      "iter65:[loss]:8.427210064866423e+141\n",
      "iter65:[val_loss]:8.427210064866419e+141\n",
      "iter66:[loss]:8.343148644469355e+143\n",
      "iter66:[val_loss]:8.343148644469376e+143\n",
      "iter67:[loss]:8.259925736740795e+145\n",
      "iter67:[val_loss]:8.259925736740775e+145\n",
      "iter68:[loss]:8.177532977516817e+147\n",
      "iter68:[val_loss]:8.177532977516808e+147\n",
      "iter69:[loss]:8.095962086066064e+149\n",
      "iter69:[val_loss]:8.095962086066065e+149\n",
      "iter70:[loss]:8.015204864257537e+151\n",
      "iter70:[val_loss]:8.01520486425755e+151\n",
      "iter71:[loss]:7.935253195736594e+153\n",
      "iter71:[val_loss]:7.935253195736573e+153\n",
      "iter72:[loss]:7.85609904510913e+155\n",
      "iter72:[val_loss]:7.856099045109121e+155\n",
      "iter73:[loss]:7.777734457134123e+157\n",
      "iter73:[val_loss]:7.777734457134147e+157\n",
      "iter74:[loss]:7.700151555924229e+159\n",
      "iter74:[val_loss]:7.700151555924238e+159\n",
      "iter75:[loss]:7.623342544153867e+161\n",
      "iter75:[val_loss]:7.62334254415388e+161\n",
      "iter76:[loss]:7.547299702275948e+163\n",
      "iter76:[val_loss]:7.547299702275969e+163\n",
      "iter77:[loss]:7.472015387745775e+165\n",
      "iter77:[val_loss]:7.472015387745761e+165\n",
      "iter78:[loss]:7.397482034253012e+167\n",
      "iter78:[val_loss]:7.397482034253011e+167\n",
      "iter79:[loss]:7.32369215096136e+169\n",
      "iter79:[val_loss]:7.323692150961362e+169\n",
      "iter80:[loss]:7.250638321755518e+171\n",
      "iter80:[val_loss]:7.250638321755509e+171\n",
      "iter81:[loss]:7.178313204496018e+173\n",
      "iter81:[val_loss]:7.178313204495998e+173\n",
      "iter82:[loss]:7.106709530281165e+175\n",
      "iter82:[val_loss]:7.106709530281146e+175\n",
      "iter83:[loss]:7.035820102716621e+177\n",
      "iter83:[val_loss]:7.035820102716607e+177\n",
      "iter84:[loss]:6.965637797191985e+179\n",
      "iter84:[val_loss]:6.965637797192004e+179\n",
      "iter85:[loss]:6.896155560165017e+181\n",
      "iter85:[val_loss]:6.896155560165023e+181\n",
      "iter86:[loss]:6.827366408452381e+183\n",
      "iter86:[val_loss]:6.827366408452383e+183\n",
      "iter87:[loss]:6.759263428528077e+185\n",
      "iter87:[val_loss]:6.75926342852806e+185\n",
      "iter88:[loss]:6.691839775828504e+187\n",
      "iter88:[val_loss]:6.691839775828518e+187\n",
      "iter89:[loss]:6.625088674064647e+189\n",
      "iter89:[val_loss]:6.625088674064636e+189\n",
      "iter90:[loss]:6.559003414540856e+191\n",
      "iter90:[val_loss]:6.559003414540839e+191\n",
      "iter91:[loss]:6.493577355480821e+193\n",
      "iter91:[val_loss]:6.4935773554808085e+193\n",
      "iter92:[loss]:6.428803921359905e+195\n",
      "iter92:[val_loss]:6.428803921359898e+195\n",
      "iter93:[loss]:6.364676602244331e+197\n",
      "iter93:[val_loss]:6.364676602244308e+197\n",
      "iter94:[loss]:6.301188953136904e+199\n",
      "iter94:[val_loss]:6.301188953136915e+199\n",
      "iter95:[loss]:6.238334593329385e+201\n",
      "iter95:[val_loss]:6.238334593329369e+201\n",
      "iter96:[loss]:6.1761072057608865e+203\n",
      "iter96:[val_loss]:6.176107205760901e+203\n",
      "iter97:[loss]:6.114500536383394e+205\n",
      "iter97:[val_loss]:6.1145005363834125e+205\n",
      "iter98:[loss]:6.053508393532979e+207\n",
      "iter98:[val_loss]:6.053508393532994e+207\n",
      "iter99:[loss]:5.993124647307491e+209\n",
      "iter99:[val_loss]:5.993124647307489e+209\n"
     ]
    }
   ],
   "source": [
    "scr_lr = ScratchLinearRegression(num_iter=100, \n",
    "                                 lr=0.01, \n",
    "                                 no_bias = False,\n",
    "                                 verbose=True)\n",
    "\n",
    "scr_lr.fit(X_train_scaler, y_train, X_test_scaler, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 標準化してもダメだった。。。\n",
    "## クラス関数内に問題あり。クラス内コーディングは怖いので、いったんver2でgithubにアップしておく。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 描画\n",
    "x = np.arange(1, len(scr_lr.loss)+1)\n",
    "\n",
    "plt.plot(x, scr_lr.loss, label=\"train_loss\")\n",
    "plt.plot(x, scr_lr.val_loss, label=\"val_loss\")\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "467px",
    "left": "1550px",
    "right": "20px",
    "top": "120px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
