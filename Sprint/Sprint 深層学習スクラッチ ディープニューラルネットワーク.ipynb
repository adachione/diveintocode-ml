{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sprint 深層学習スクラッチ ディープニューラルネットワーク"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "import copy\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GetMiniBatch:\n",
    "    \"\"\"\n",
    "    ミニバッチを取得するイテレータ\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : 次の形のndarray, shape (n_samples, n_features)\n",
    "      訓練データ\n",
    "    y : 次の形のndarray, shape (n_samples, 1)\n",
    "      正解値\n",
    "    batch_size : int\n",
    "      バッチサイズ\n",
    "    seed : int\n",
    "      NumPyの乱数のシード\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, X, y, batch_size=20, seed=0):\n",
    "        self.batch_size = batch_size\n",
    "        np.random.seed(seed)\n",
    "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
    "        self._X = X[shuffle_index]\n",
    "        self._y = y[shuffle_index]\n",
    "        self._stop = np.ceil(X.shape[0] / self.batch_size).astype(np.int)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._stop\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        p0 = item * self.batch_size\n",
    "        p1 = item * self.batch_size + self.batch_size\n",
    "        return self._X[p0:p1], self._y[p0:p1]\n",
    "\n",
    "    def __iter__(self):\n",
    "        self._counter = 0\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        if self._counter >= self._stop:\n",
    "            raise StopIteration()\n",
    "        p0 = self._counter * self.batch_size\n",
    "        p1 = self._counter * self.batch_size + self.batch_size\n",
    "        self._counter += 1\n",
    "        return self._X[p0:p1], self._y[p0:p1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題1】全結合層のクラス化\n",
    "全結合層のクラス化を行なってください。\n",
    "\n",
    "\n",
    "以下に雛形を載せました。コンストラクタで重みやバイアスの初期化をして、あとはフォワードとバックワードのメソッドを用意します。重みW、バイアスB、およびフォワード時の入力Xをインスタンス変数として保持しておくことで、煩雑な入出力は不要になります。\n",
    "\n",
    "\n",
    "なお、インスタンスも引数として渡すことができます。そのため、初期化方法のインスタンスinitializerをコンストラクタで受け取れば、それにより初期化が行われます。渡すインスタンスを変えれば、初期化方法が変えられます。\n",
    "\n",
    "\n",
    "また、引数として自身のインスタンスselfを渡すこともできます。これを利用してself.optimizer.update(self)という風に層の重みの更新が可能です。更新に必要な値は複数ありますが、全て全結合層が持つインスタンス変数にすることができます。\n",
    "\n",
    "\n",
    "初期化方法と最適化手法のクラスについては後述します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FC:\n",
    "    \"\"\"\n",
    "    ノード数n_nodes1からn_nodes2への全結合層\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_nodes1 : int\n",
    "      前の層のノード数\n",
    "    n_nodes2 : int\n",
    "      後の層のノード数\n",
    "    initializer : 初期化方法のインスタンス\n",
    "    optimizer : 最適化手法のインスタンス\n",
    "    \"\"\"\n",
    "    def __init__(self, n_nodes1, n_nodes2, initializer, optimizer):\n",
    "        self.n_nodes1 = n_nodes1\n",
    "        self.n_nodes2 = n_nodes2\n",
    "        self.initializer = initializer\n",
    "        self.optimizer = optimizer\n",
    "        # 初期化\n",
    "        # initializerのメソッドを使い、self.Wとself.Bを初期化する\n",
    "        self.W = self.initializer.W(self.n_nodes1, self.n_nodes2)\n",
    "        self.B = self.initializer.B(self.n_nodes2)\n",
    "        self.dW = 0\n",
    "        self.dB = 0\n",
    "        \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        フォワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            入力\n",
    "        Returns\n",
    "        ----------\n",
    "        A : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            出力\n",
    "        \"\"\"\n",
    "        self.X = X\n",
    "        A = np.dot(X, self.W) + self.B   # (batch_size, n_nodes2)\n",
    "\n",
    "        return A\n",
    "    \n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "        バックワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        dA : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            後ろから流れてきた勾配\n",
    "        Returns\n",
    "        ----------\n",
    "        dZ : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            前に流す勾配\n",
    "        \"\"\"\n",
    "        n_batch = dA.shape[0]\n",
    "        \n",
    "        # TODO　dw とdbをバッチサイズで割る　FCで実行\n",
    "        self.dB = np.sum(dA, axis=0)/n_batch\n",
    "        self.dW = np.dot(self.X.T, dA)/n_batch\n",
    "        dZ = np.dot(dA, self.W.T)    # (batch_size, n_nodes1)\n",
    "        \n",
    "        # 更新\n",
    "        self = self.optimizer.update(self)\n",
    "        return dZ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題2】初期化方法のクラス化\n",
    "初期化を行うコードをクラス化してください。\n",
    "\n",
    "\n",
    "前述のように、全結合層のコンストラクタに初期化方法のインスタンスを渡せるようにします。以下の雛形に必要なコードを書き加えていってください。標準偏差の値（sigma）はコンストラクタで受け取るようにすることで、全結合層のクラス内にこの値（sigma）を渡さなくてすむようになります。\n",
    "\n",
    "\n",
    "これまで扱ってきた初期化方法はSimpleInitializerクラスと名付けることにします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleInitializer:\n",
    "    \"\"\"\n",
    "    ガウス分布によるシンプルな初期化\n",
    "    Parameters\n",
    "    ----------\n",
    "    sigma : float\n",
    "      ガウス分布の標準偏差\n",
    "    \"\"\"\n",
    "    def __init__(self, sigma):\n",
    "        self.sigma = sigma\n",
    "    def W(self, n_nodes1, n_nodes2):\n",
    "        \"\"\"\n",
    "        重みの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes1 : int\n",
    "          前の層のノード数\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        W :\n",
    "        \"\"\"\n",
    "        W = self.sigma * np.random.randn(n_nodes1, n_nodes2)\n",
    "        \n",
    "        return W\n",
    "    \n",
    "    def B(self, n_nodes2):\n",
    "        \"\"\"\n",
    "        バイアスの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        B :\n",
    "        \"\"\"\n",
    "        B = np.random.randn(n_nodes2)\n",
    "        \n",
    "        return B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題3】最適化手法のクラス化\n",
    "最適化手法のクラス化を行なってください。\n",
    "\n",
    "\n",
    "最適化手法に関しても初期化方法同様に全結合層にインスタンスとして渡します。バックワードのときにself.optimizer.update(self)のように更新できるようにします。以下の雛形に必要なコードを書き加えていってください。\n",
    "\n",
    "\n",
    "これまで扱ってきた最適化手法はSGDクラス（Stochastic Gradient Descent、確率的勾配降下法）として作成します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    \"\"\"\n",
    "    確率的勾配降下法\n",
    "    Parameters\n",
    "    ----------\n",
    "    lr : 学習率\n",
    "    \"\"\"\n",
    "    def __init__(self, lr=0.01):\n",
    "        self.lr = lr\n",
    "    def update(self, layer):\n",
    "        \"\"\"\n",
    "        ある層の重みやバイアスの更新\n",
    "        Parameters\n",
    "        ----------\n",
    "        layer : 更新前の層のインスタンス\n",
    "        \"\"\"\n",
    "        # TODO Eの計算\n",
    "        \n",
    "        layer.W -= self.lr * layer.dW\n",
    "        layer.B -= self.lr * layer.dB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題4】活性化関数のクラス化\n",
    "活性化関数のクラス化を行なってください。\n",
    "\n",
    "\n",
    "ソフトマックス関数のバックプロパゲーションには交差エントロピー誤差の計算も含む実装を行うことで計算が簡略化されます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tanh():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    def forward(self, A):\n",
    "        self.A = A\n",
    "        return np.tanh(A)\n",
    "    \n",
    "    def backward(self, dZ):\n",
    "        \"\"\"\n",
    "        dZ : n層目のバックプロバゲーション\n",
    "        A :　n層目のフォワードプロバゲーションのA\n",
    "        \n",
    "        \"\"\"\n",
    "        y = dZ*((1-self.forward(self.A)**2))\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    def forward(self, A):\n",
    "        self.A = A\n",
    "        return 1 / (1 + np.exp(-A))\n",
    "    \n",
    "    def backward(self, dZ):\n",
    "        \"\"\"\n",
    "        dZ : n層目のバックプロバゲーション\n",
    "        A :　n層目のフォワードプロバゲーションのA\n",
    "         \n",
    "        \"\"\"\n",
    "        y = dZ*(1-self.forward(self.A))*(self.forward(self.A))\n",
    "        return y    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax():\n",
    "    \"\"\"\n",
    "    ソフトマックス関数クラス\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    def forward(self, A):\n",
    "        # ソフトマックス計算 Zが出力値\n",
    "        self.A = A\n",
    "        c = np.max(A)\n",
    "        self.Z = np.exp(A-c) / np.sum(np.exp(A-c), axis=1, keepdims=True)\n",
    "        \n",
    "        return self.Z\n",
    "    \n",
    "    def backward(self, Z, y):\n",
    "        \"\"\"\n",
    "        dZ　: 最終層のZ\n",
    "        Y　:　（バッチ）サンプルラベル\n",
    "        \"\"\"\n",
    "        dZ = self.Z - y\n",
    "        \n",
    "        # クロスエントロピー誤差計算\n",
    "        batch_size = y.shape[0]\n",
    "        error = -np.sum(y*np.log(self.Z + 1e-7)) / batch_size\n",
    "        \n",
    "        return dZ , error\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題5】ReLUクラスの作成\n",
    "現在一般的に使われている活性化関数であるReLU（Rectified Linear Unit）をReLUクラスとして実装してください。\n",
    "\n",
    "\n",
    "ReLUは以下の数式です。\n",
    "\n",
    "$$\n",
    "f(x)=ReLU(x)=\n",
    "\\begin{cases}\n",
    "x \\quad x \\geqq 0 \\\\\n",
    "0 \\quad x < 0 \\\\\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "$x$ : ある特徴量。スカラー\n",
    "\n",
    "\n",
    "実装上はnp.maximumを使い配列に対してまとめて計算が可能です。\n",
    "\n",
    "\n",
    "numpy.maximum — NumPy v1.15 Manual\n",
    "\n",
    "\n",
    "一方、バックプロパゲーションのための $x$ に関する $f(x)$ の微分は以下のようになります。\n",
    "\n",
    "$$\n",
    "\\frac{∂f(x)}{∂x}=\n",
    "        \\begin{cases}\n",
    "        1 \\quad x \\geqq 0 \\\\\n",
    "        0 \\quad x ≦ 0 \\\\\n",
    "        \\end{cases}\n",
    "$$\n",
    "数学的には微分可能ではないですが、 $x=0$ のとき $0$ とすることで対応しています。\n",
    "\n",
    "\n",
    "フォワード時の $x$ の正負により、勾配を逆伝播するかどうかが決まるということになります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ゼロつくに乗っていたコード\n",
    "class Relu():\n",
    "    \"\"\"\n",
    "    Relu関数クラス\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.mask = None\n",
    "        \n",
    "    def forward(self, A):\n",
    "        self.mask = (A <= 0)\n",
    "        out = A.copy()\n",
    "        out[self.mask] = 0\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dz):\n",
    "        \"\"\"\n",
    "        Z:\n",
    "        y:\n",
    "        \n",
    "        \"\"\"\n",
    "        dz[self.mask] = 0\n",
    "        y = dz\n",
    "        \n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [1 2]\n",
      " [3 4]]\n",
      "[[0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [1 2]\n",
      " [3 4]]\n"
     ]
    }
   ],
   "source": [
    "# Relu関数テスト\n",
    "x = np.arange(-5, 5).reshape(5, 2)\n",
    "\n",
    "relu = Relu()\n",
    "\n",
    "print(relu.forward(x))\n",
    "print(relu.backward(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題6】重みの初期値\n",
    "ここまでは重みやバイアスの初期値は単純にガウス分布で、標準偏差をハイパーパラメータとして扱ってきました。しかし、どのような値にすると良いかが知られています。シグモイド関数やハイパボリックタンジェント関数のときは Xavierの初期値 （またはGlorotの初期値）、ReLUのときは Heの初期値 が使われます。\n",
    "\n",
    "\n",
    "XavierInitializerクラスと、HeInitializerクラスを作成してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XavierInitializer:\n",
    "    \"\"\"\n",
    "    Xavierの初期値作成クラス\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    def W(self, n_nodes1, n_nodes2):\n",
    "        \"\"\"\n",
    "        重みの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes1 : int\n",
    "          前の層のノード数\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        W :\n",
    "        \"\"\"\n",
    "        sigma = 1.0 / np.sqrt(n_nodes1)\n",
    "        W = sigma * np.random.randn(n_nodes1, n_nodes2)\n",
    "        \n",
    "        return W\n",
    "    \n",
    "    def B(self, n_nodes2):\n",
    "        \"\"\"\n",
    "        バイアスの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        B :\n",
    "        \"\"\"\n",
    "        B = np.zeros(n_nodes2)\n",
    "        \n",
    "        return B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.01757152 -0.01443763  0.04095086  0.06409849 -0.02930085  0.00599944\n",
      "   0.0505415  -0.03296982  0.08252301 -0.13629039 -0.01738088 -0.03795838\n",
      "  -0.03488657  0.0296471   0.03083641  0.00835433  0.08966769  0.02869655\n",
      "  -0.03323862 -0.01026759  0.02639661  0.01036517 -0.01728018 -0.02205717\n",
      "   0.02286911 -0.00426785  0.0625248   0.03208636 -0.02379951 -0.00353023\n",
      "   0.02679927 -0.01179315 -0.00544121 -0.09434819 -0.04643977  0.04556072\n",
      "   0.01627377 -0.07750355  0.11779255  0.01057348  0.01062915 -0.00537516\n",
      "  -0.03786391  0.00910667  0.04846207  0.04418837 -0.0436785   0.01185099\n",
      "   0.05718704  0.00445342  0.01083789  0.05690768  0.00269931 -0.00753285\n",
      "   0.07819799  0.09007609 -0.03049972  0.06344912 -0.01728064  0.05785465\n",
      "   0.02243426 -0.02621057 -0.01171879 -0.10739577 -0.0063357   0.06433557\n",
      "  -0.03454582  0.02636337 -0.08411003 -0.06431729  0.01686325 -0.07000186\n",
      "  -0.04453108  0.06257386  0.08023011  0.01653024  0.04468157 -0.03468137\n",
      "  -0.04276306 -0.01184822 -0.00625555  0.12061878 -0.02372626 -0.02238206\n",
      "   0.06396828  0.02754513 -0.01967235  0.04237969 -0.00099611  0.02497217\n",
      "  -0.06450527 -0.03858061  0.03821716  0.04558525  0.00143154 -0.03204972\n",
      "  -0.03624762 -0.04930527  0.0989671   0.04135982  0.01006627  0.042164\n",
      "   0.08459566 -0.09239281  0.00402398  0.01830671  0.03014698 -0.02372721\n",
      "  -0.04532611 -0.04438775  0.06936999  0.03508046  0.09376699  0.01292563\n",
      "   0.0127431   0.06573047  0.03852842 -0.0511142   0.03445453  0.11211876\n",
      "   0.04884073 -0.07666591  0.01741453  0.02399582 -0.08361875  0.03186278\n",
      "   0.0066949  -0.03628421  0.05509858 -0.02986002  0.04774383 -0.13787017\n",
      "   0.01274071 -0.02188558 -0.01368562  0.06646406  0.0667536   0.07074255\n",
      "   0.03147922  0.0073078  -0.04817715 -0.02880354 -0.02533482  0.05330122\n",
      "   0.02887     0.04435167  0.03267537 -0.02695404  0.01852023  0.05774334\n",
      "   0.03338383 -0.04315235  0.0411284  -0.00738504  0.0480886   0.05023988\n",
      "   0.04237732  0.04228674  0.0004165  -0.0107414   0.03331016  0.00356363\n",
      "   0.03594702  0.07493579  0.13420602  0.165995   -0.02518788 -0.0057652\n",
      "  -0.03795178 -0.02773296  0.10207982  0.04669151 -0.07717393 -0.02495952\n",
      "   0.06442205  0.09703085 -0.01284523  0.0722845  -0.07076269 -0.00973295\n",
      "  -0.00834817  0.05478782 -0.00520483  0.01483219  0.01065149 -0.10112853\n",
      "  -0.000388    0.00446552  0.11492283 -0.00206996  0.0424357  -0.00240633\n",
      "  -0.00438942 -0.03992556 -0.03698369 -0.0840108  -0.02140806  0.07335562\n",
      "  -0.01407503  0.03188037]\n",
      " [-0.09977265  0.07148793 -0.08481531  0.05053294 -0.09076244 -0.10381062\n",
      "   0.0360948  -0.04385217  0.01021611 -0.03416507  0.00062125  0.09042595\n",
      "   0.08879669 -0.02920735  0.09284256  0.03679218 -0.08556093 -0.02162575\n",
      "  -0.02277261  0.02554548  0.13520809 -0.05420388  0.0470164  -0.00635835\n",
      "  -0.01564641  0.00496833  0.00141486  0.03036175  0.00820145  0.04004302\n",
      "   0.02750924  0.03844696 -0.01856622 -0.01048833 -0.03626854 -0.04436454\n",
      "   0.06678112  0.00979036  0.06306457 -0.03914285 -0.0327092  -0.01144998\n",
      "   0.0227777  -0.08530164  0.03753809  0.0277378   0.03482662  0.11970841\n",
      "  -0.04283452  0.02995081 -0.12042993  0.04694656  0.046741   -0.05878815\n",
      "   0.02017367 -0.14200845  0.00775335  0.01491508 -0.1007539   0.02876541\n",
      "  -0.02143345 -0.01535356  0.031214   -0.00739537  0.01094662  0.06325327\n",
      "   0.0094489  -0.03535507  0.02298415 -0.00672755 -0.03923805 -0.05374275\n",
      "  -0.09423658 -0.02116757  0.03832337 -0.02428992  0.05545026 -0.03255043\n",
      "  -0.09374826 -0.03877828  0.02150834  0.00271827 -0.01106374 -0.01840788\n",
      "  -0.10792263 -0.03073131 -0.01827565  0.05184785 -0.06610418 -0.01541239\n",
      "  -0.00849731  0.04876801 -0.01411902 -0.00739723  0.03516358  0.00928641\n",
      "   0.02653056 -0.04283798  0.01619202 -0.07514231  0.00262915 -0.03507238\n",
      "  -0.05118736  0.02402168  0.12099073  0.04534497 -0.06630066  0.09388476\n",
      "  -0.06008324  0.02169071 -0.0036162   0.04691112  0.02211955  0.00034544\n",
      "  -0.06160566 -0.01087609  0.01876153  0.01802483 -0.04170485 -0.00827922\n",
      "  -0.04454075  0.06853201 -0.03276137 -0.0265219   0.03478244 -0.02382151\n",
      "  -0.0074016  -0.07491391 -0.02587138  0.04848091  0.01960567  0.00162783\n",
      "  -0.0477348  -0.02879379 -0.09965628  0.0057116   0.07277557  0.08657203\n",
      "  -0.02117388  0.05362786  0.05908505 -0.01375217  0.06101869 -0.04667859\n",
      "   0.06824981  0.01108399 -0.01546437  0.02416019  0.00551085  0.00377672\n",
      "  -0.02717295  0.03322717 -0.00955579  0.0582332   0.02003981 -0.05419004\n",
      "  -0.03908741 -0.01381806 -0.05724826  0.12722485  0.07873908  0.02587951\n",
      "   0.02032187  0.02717569 -0.02753796 -0.07185029 -0.03280982 -0.08974133\n",
      "  -0.01546568 -0.01547883  0.06755374 -0.03644463 -0.01143329  0.03724096\n",
      "   0.00779832  0.03492596  0.05721846 -0.08881408  0.02249134 -0.04976437\n",
      "   0.01871907 -0.0034391   0.0399174   0.05176034  0.00065997  0.07437511\n",
      "   0.02365355 -0.06738284  0.0003446  -0.02390025 -0.11149349 -0.06511131\n",
      "   0.0197678   0.02447236 -0.0156246  -0.03563052 -0.0480842   0.01379084\n",
      "  -0.02050173  0.05470756]\n",
      " [ 0.00270979 -0.01859518 -0.05507378 -0.01200768 -0.034237    0.03396134\n",
      "   0.05146782 -0.04521364  0.03324816  0.05793112  0.0384017   0.01084513\n",
      "  -0.00803477  0.03172076 -0.02416137 -0.0371883  -0.02745343  0.00706606\n",
      "   0.00894749 -0.07114404 -0.03777642 -0.03653833  0.00891701 -0.00575024\n",
      "  -0.02237652 -0.03742331  0.06641575  0.03430538  0.07721924 -0.04260709\n",
      "   0.02535477 -0.08088953 -0.01596765 -0.03591223 -0.05065525 -0.05372533\n",
      "   0.02155741 -0.05524526  0.01947591  0.01636539 -0.05198208  0.03854198\n",
      "  -0.07236108  0.0896878   0.0757665  -0.02023957  0.00950667 -0.05465148\n",
      "  -0.05642134 -0.02264262  0.09368311 -0.01155537  0.0586701   0.02366042\n",
      "   0.0539469  -0.06683838  0.0214543   0.0465881   0.00483892 -0.04567746\n",
      "   0.01838398 -0.05185792 -0.03475167  0.05446632 -0.04443734 -0.0211447\n",
      "  -0.01304309  0.04274192  0.00310857  0.02783839  0.01804914  0.05200606\n",
      "   0.0454971   0.0021175  -0.07718037  0.00798609  0.08211011 -0.02011512\n",
      "  -0.01163842 -0.05613674 -0.01108528  0.1123627  -0.10647286  0.0033933\n",
      "   0.08616049 -0.05047472 -0.02142818 -0.00370552  0.04101931  0.00195183\n",
      "  -0.08825807  0.06267724  0.04224049  0.00109474 -0.01456288  0.02286159\n",
      "  -0.00663387  0.02347825  0.05652429 -0.03423143 -0.01132819 -0.00843076\n",
      "  -0.02931225  0.06866141  0.10086358 -0.03368724  0.0410536   0.03876052\n",
      "   0.00275699 -0.12863064 -0.03174062  0.07888807 -0.00968399 -0.11923536\n",
      "   0.00417704  0.07093125 -0.02058019  0.02892847 -0.02628795  0.01672855\n",
      "  -0.0266337  -0.02293887 -0.03490314 -0.00305887 -0.08792258 -0.02522015\n",
      "   0.01636955  0.0448843   0.05909399  0.00944845 -0.01168885  0.02136854\n",
      "   0.02157318 -0.06904112 -0.01214803 -0.01829478 -0.02965899 -0.04958505\n",
      "  -0.01605147 -0.00702     0.06637936  0.03565946 -0.05486316 -0.00334859\n",
      "  -0.02516682 -0.03462655 -0.01613886 -0.00919625 -0.01519153 -0.05115707\n",
      "   0.0050681   0.1044597  -0.03786805 -0.01590775 -0.18164769  0.00269132\n",
      "  -0.07692901  0.0013027   0.1354651  -0.1372163   0.03050024 -0.01284799\n",
      "   0.03519827  0.00989533 -0.00074002 -0.04265287 -0.04355314 -0.00610736\n",
      "   0.01664938 -0.00039535 -0.04669275 -0.06505164  0.05942573 -0.03077551\n",
      "   0.05800691 -0.05626967 -0.03700066  0.02813863  0.01369784  0.0134861\n",
      "   0.01077794  0.03100997 -0.03196563 -0.05262608  0.01999294 -0.05899889\n",
      "  -0.10709712  0.09131168 -0.02217369 -0.06763115 -0.02997449 -0.02993893\n",
      "   0.0687304   0.00191138  0.07350591  0.07050659 -0.00920244 -0.01620373\n",
      "  -0.04199105 -0.00577921]\n",
      " [ 0.01982629  0.02021617 -0.08975001  0.04219118 -0.03892151 -0.03473191\n",
      "  -0.02781021 -0.06295121 -0.08027941 -0.05164841 -0.049105   -0.02909438\n",
      "  -0.03250746 -0.06521804 -0.02933012 -0.04392593 -0.03100051  0.02128089\n",
      "  -0.00901865 -0.02532044 -0.09538352  0.09450935 -0.03041651  0.00261803\n",
      "   0.01493817  0.01353044 -0.08343704 -0.0210877  -0.01785095  0.01688544\n",
      "  -0.04243368  0.00343461 -0.09113683  0.01936461 -0.01408021 -0.07775745\n",
      "  -0.01173562 -0.09849193 -0.01993914 -0.01287894 -0.01326443  0.02506326\n",
      "   0.03148216 -0.09164296  0.04498351 -0.06400316  0.00374024 -0.03833857\n",
      "   0.06150603  0.00431708 -0.06707541  0.03259793  0.02954971  0.08942288\n",
      "   0.04923228  0.02808206 -0.04818211 -0.00173238 -0.05397218 -0.01432221\n",
      "  -0.02115102 -0.03779908 -0.02523189 -0.0381793   0.0809561  -0.01889076\n",
      "   0.01998581 -0.00446338 -0.12478232 -0.07180838  0.01086771 -0.05830548\n",
      "  -0.05729396  0.07964657  0.03089337 -0.04899195 -0.01112068  0.02697394\n",
      "   0.07231102  0.01895001 -0.02513704 -0.1214868  -0.03600328  0.02321796\n",
      "   0.03087564  0.01230805 -0.09260544 -0.03405375 -0.12401983 -0.0782237\n",
      "  -0.00765473  0.05502108  0.08480726  0.03889498  0.00387208 -0.04247208\n",
      "   0.01532993 -0.03309121  0.02263952 -0.06164579 -0.00928951  0.01541737\n",
      "   0.03294328 -0.05759109  0.04377991 -0.04538735  0.02743862  0.04368478\n",
      "  -0.0211464   0.10039012 -0.09247805  0.00251883 -0.02383071 -0.06750315\n",
      "   0.0300417   0.08560187 -0.02667413  0.07827397  0.02329791 -0.02410829\n",
      "   0.06179819 -0.02203117  0.06236067 -0.12304384  0.00794501  0.00975702\n",
      "  -0.01576127 -0.01294015 -0.01450804 -0.13317533  0.02954013  0.03857255\n",
      "   0.0767525  -0.02282594 -0.05786311 -0.00622654  0.00797799  0.03689417\n",
      "  -0.0736721  -0.05620894 -0.04879545  0.01692151 -0.02124072 -0.04606912\n",
      "   0.09274472 -0.09126046 -0.04326735 -0.1362622  -0.00506626  0.06642217\n",
      "   0.04732273  0.01444269  0.01079624 -0.03796786  0.07404001  0.00232422\n",
      "   0.04754125  0.02252293 -0.120131   -0.08043713  0.00320578  0.04694076\n",
      "  -0.03136255 -0.01999122 -0.0574526   0.01728101  0.07815903 -0.01554829\n",
      "  -0.10621868  0.03463812  0.04245918 -0.03031669 -0.05006909  0.02090379\n",
      "   0.00835571  0.01277246  0.0307387   0.02126178 -0.0686894   0.00300894\n",
      "  -0.05741397 -0.01757637  0.06571221  0.05962435  0.01023396 -0.08525038\n",
      "   0.10099791 -0.03137943  0.068191    0.01716697 -0.00282448 -0.0468216\n",
      "  -0.07398255  0.00977037 -0.01997992 -0.01356867  0.00685958  0.01794202\n",
      "  -0.03904033 -0.00417963]\n",
      " [ 0.02516044 -0.07317319  0.01899827 -0.04539267  0.0377096   0.05725971\n",
      "  -0.09968471  0.00091632  0.03699222  0.03652596  0.05228306 -0.03579872\n",
      "   0.05577214  0.00294219 -0.04121024  0.01886938 -0.01190293 -0.06336582\n",
      "   0.03768588 -0.05774366 -0.03993333 -0.02386339 -0.06219843  0.01632956\n",
      "   0.03224387  0.0769488  -0.06387157 -0.04082903 -0.02033224  0.05098296\n",
      "  -0.03478058  0.03613094  0.02393788  0.02572042  0.02181166  0.0522889\n",
      "   0.05028318  0.00944145  0.01890946  0.01480585  0.02674371 -0.00755557\n",
      "   0.04193356 -0.03313754 -0.03850092  0.03765045  0.00619848 -0.02224062\n",
      "  -0.03849347  0.03248833 -0.05611223 -0.03099678  0.01734945 -0.03882193\n",
      "  -0.04471658 -0.00923024 -0.0251414   0.06560029 -0.00419243 -0.06183826\n",
      "   0.06866779  0.07220662  0.04457365  0.00302079 -0.01467501 -0.03276545\n",
      "  -0.03200503 -0.03481051  0.03495619  0.07547103  0.03670446 -0.02466547\n",
      "  -0.05125685  0.02665926 -0.00536874 -0.02813059 -0.09830879 -0.00437986\n",
      "  -0.01133691 -0.00553574 -0.0198959  -0.05931076  0.09099268 -0.06794783\n",
      "  -0.10211688  0.05538081 -0.00053393 -0.0237748  -0.03779238  0.08610965\n",
      "   0.0909455  -0.105164   -0.0701528  -0.02882181 -0.11641618 -0.10412574\n",
      "  -0.02281417 -0.03555368  0.01496038  0.02704449 -0.05174755  0.0325538\n",
      "   0.05710754  0.0212807   0.08207898 -0.05631308 -0.02793759 -0.06817807\n",
      "   0.03001497  0.01939865  0.04749569 -0.08565167  0.02147386 -0.01078314\n",
      "   0.01941456  0.03110562 -0.00361834  0.0257594   0.09009178 -0.09030072\n",
      "  -0.08422001  0.02516631  0.05107101 -0.04311212 -0.04134121  0.01968007\n",
      "  -0.076036   -0.10084459  0.02770171  0.07015891 -0.01683401 -0.04582585\n",
      "   0.03358477 -0.01339468 -0.014746   -0.01900465 -0.06556583 -0.08506723\n",
      "   0.13678618  0.01410594  0.06743075 -0.01625347 -0.03842056  0.00051684\n",
      "  -0.03643616 -0.01217479 -0.09441965  0.01966869  0.03073554  0.00510489\n",
      "  -0.04077634 -0.05925269 -0.03174091 -0.03652662  0.04670732  0.00613725\n",
      "   0.01583876 -0.15519341  0.03144783  0.03581417 -0.06268936 -0.04234748\n",
      "  -0.03122289  0.12880937  0.04386153 -0.02370628  0.03664429 -0.0260625\n",
      "  -0.02341598 -0.01146739  0.01107203 -0.15242677  0.05237499 -0.00173614\n",
      "  -0.06365956  0.00579376  0.08765108  0.03057089  0.00493466  0.03799921\n",
      "  -0.02507728  0.08704502 -0.09288289 -0.0493871  -0.06480026  0.02501485\n",
      "   0.01881485  0.03942208  0.01371601  0.0085039   0.03122818 -0.00984641\n",
      "   0.09948033 -0.03122261 -0.01203492  0.05192787  0.07008872  0.02221133\n",
      "   0.0942154  -0.03115461]]\n",
      "[0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# XavierInitializerテスト\n",
    "xavier = XavierInitializer()\n",
    "\n",
    "print(xavier.W(400, 200)[:5])\n",
    "print(xavier.B(200)[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeInitializer:\n",
    "    \"\"\"\n",
    "    Heの初期値作成クラス\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    def W(self, n_nodes1, n_nodes2):\n",
    "        \"\"\"\n",
    "        重みの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes1 : int\n",
    "          前の層のノード数\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        W :\n",
    "        \"\"\"\n",
    "        sigma = np.sqrt(2.0 / n_nodes1)\n",
    "        W = sigma * np.random.randn(n_nodes1, n_nodes2)\n",
    "        \n",
    "        return W\n",
    "    \n",
    "    def B(self, n_nodes2):\n",
    "        \"\"\"\n",
    "        バイアスの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        B :\n",
    "        \"\"\"\n",
    "        B = np.random.randn(n_nodes2)\n",
    "        \n",
    "        return B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.35038371e-02 -2.88338023e-02 -4.08142286e-02  2.33282409e-02\n",
      "  -3.69598713e-02  9.43694148e-03  4.28817159e-02  3.87236580e-02\n",
      "   4.94023398e-02  1.45814966e-01 -7.88563320e-02  2.05889708e-02\n",
      "   1.83592854e-02 -7.61476687e-02 -1.73310088e-02  4.48309863e-02\n",
      "   4.65069671e-02 -4.06001638e-02 -1.36767103e-02  8.67989230e-02\n",
      "  -1.13752155e-01  6.07331335e-03 -5.45192033e-02 -9.66989468e-02\n",
      "   1.58531890e-01  1.35440077e-01 -2.61856666e-02 -1.29110175e-02\n",
      "   3.14984420e-02  9.98407698e-02  5.34138600e-02 -1.54356756e-02\n",
      "  -7.84936214e-02  4.47715908e-02 -5.25398910e-02  8.94431111e-03\n",
      "   4.84114530e-02 -2.30577123e-02 -2.27642189e-02 -1.36287435e-02\n",
      "   3.33770855e-02 -7.24007133e-02  9.06458162e-02  6.19248302e-02\n",
      "  -8.93787395e-03  7.79258896e-02 -4.70742754e-02  3.83931823e-03\n",
      "   1.64930410e-02 -9.83983255e-02 -1.33557184e-01 -6.55359074e-02\n",
      "   6.80448468e-02  1.34055292e-01  2.15189965e-02 -7.85874848e-02\n",
      "   2.03639188e-01 -3.81463839e-04 -7.96571598e-02 -2.59803657e-03\n",
      "   7.73431323e-02  8.31570352e-03 -6.25404220e-03  7.08025146e-02\n",
      "  -8.66438171e-02 -1.37480822e-01  1.98032833e-03 -5.69635396e-02\n",
      "   5.53279246e-02 -1.58637296e-01 -1.48814375e-02 -2.02023876e-02\n",
      "   3.19718216e-02  9.36745400e-02  3.34434086e-02 -1.35628256e-02\n",
      "  -2.31087515e-02  2.73586493e-02 -2.80744060e-02 -1.51439136e-02\n",
      "  -1.28666284e-01 -5.83935013e-02  4.14914317e-02  3.27832927e-02\n",
      "   5.23321118e-02 -4.73522157e-02 -4.03534759e-02 -2.42741890e-02\n",
      "  -6.40433345e-02  6.89708579e-02 -3.65390912e-02 -2.70077990e-02\n",
      "  -7.73103658e-02  1.75027772e-04  2.98316383e-02  1.75158683e-02\n",
      "  -5.10182898e-02  4.92835196e-04  2.86596835e-02  1.11089774e-02\n",
      "  -9.56489238e-02  1.46340357e-01 -7.76297828e-02  1.91794009e-02\n",
      "   4.30492385e-02 -1.68661595e-02  6.51167588e-02  8.58637776e-03\n",
      "  -3.87341705e-02 -9.71885047e-02 -1.26373322e-02 -1.22804811e-01\n",
      "  -1.02321977e-01 -2.71632629e-02 -1.34560062e-02  6.72650425e-02\n",
      "   9.60568321e-02  7.49431580e-02  5.19337650e-02  3.61159564e-04\n",
      "   5.81780038e-02  1.78316133e-02  7.65869118e-03  4.67296937e-02\n",
      "   1.26686645e-01 -6.44988846e-03 -4.32266635e-02 -7.12647325e-02\n",
      "  -1.35725749e-02  1.24576495e-01 -3.44807982e-02 -6.81434257e-02\n",
      "  -8.82690579e-02 -2.31523124e-02  9.36741977e-02 -4.75502605e-02\n",
      "   7.14065297e-02  6.90585825e-02 -5.90129811e-02 -1.44619448e-02\n",
      "   2.00054052e-02  1.07682139e-01 -2.63009927e-02  8.51637437e-03\n",
      "   1.55307797e-01  2.21762211e-01  2.90146243e-02 -2.80281461e-02\n",
      "   9.53699881e-03  4.94478090e-02  8.15320335e-04  1.91336290e-01\n",
      "   1.24547081e-01  1.56738365e-01  6.29102041e-02  3.83098705e-02\n",
      "   2.46581750e-02 -1.00280521e-01  2.71220239e-02 -6.36764150e-02\n",
      "   6.30564715e-02  4.81981888e-02 -2.95843579e-02 -1.09706514e-02\n",
      "   2.77547094e-02  9.40051011e-02 -3.19420263e-02  1.23149262e-01\n",
      "  -8.51619919e-02  4.01635858e-02  3.22851302e-02 -3.72281640e-03\n",
      "   1.05428521e-01  1.23007873e-01  3.04067900e-02  3.26157452e-02\n",
      "  -1.82179794e-01  6.52961640e-02 -2.41374714e-02 -6.30886646e-02\n",
      "   5.24750924e-02 -1.26177484e-02 -5.95909340e-02  1.97004770e-02\n",
      "  -4.23110889e-02 -3.09969917e-03  2.00959657e-02 -3.87582832e-03\n",
      "   6.85079302e-02  5.43797532e-02  6.81093433e-03 -3.50059981e-02\n",
      "   1.19784938e-01 -7.61066764e-02 -9.09701312e-02 -4.73292384e-02\n",
      "   4.24486767e-02  7.60801259e-02 -1.13483870e-01  2.84785412e-02]\n",
      " [ 2.56534204e-02  5.17131247e-02  3.05807914e-02 -7.45876263e-02\n",
      "  -3.82544269e-02  1.42628697e-01 -3.36119520e-04  7.74628495e-02\n",
      "   1.89771549e-02 -3.05979895e-02  9.15136259e-02  3.92636918e-02\n",
      "  -4.02167850e-02  5.75369567e-02 -3.52405988e-03  4.67238013e-02\n",
      "  -1.15329221e-01  1.30996629e-01 -1.23367369e-03 -3.79881559e-02\n",
      "   7.35508370e-02 -1.50257286e-01  4.25607840e-02 -4.79967065e-03\n",
      "   1.04983019e-02 -1.23308461e-02 -7.97190420e-03  1.22413496e-02\n",
      "   9.24369344e-02  6.22128738e-02 -6.20363583e-02  6.96922195e-02\n",
      "  -6.36864409e-02  1.35520014e-01 -6.71231739e-02  4.43833584e-02\n",
      "   1.53071694e-01 -2.84694443e-02 -4.85225092e-02  9.88418975e-03\n",
      "  -1.39746718e-02 -2.73056769e-02 -1.23011586e-02  5.83166158e-02\n",
      "   7.82802953e-02 -7.04356918e-02 -1.38192805e-01 -3.01406014e-02\n",
      "  -2.42758207e-02  8.27821720e-02  7.67925762e-02 -1.24559793e-01\n",
      "   5.82202505e-02 -1.80819317e-02 -7.31640708e-03  6.62032597e-02\n",
      "   9.73001654e-02 -1.01834814e-02  1.48391798e-02  3.59224749e-02\n",
      "   4.58251695e-02 -1.62686942e-02  4.07180490e-02  8.33240869e-02\n",
      "  -5.55225502e-02 -2.33834823e-02  7.56366427e-02 -2.26232574e-02\n",
      "   1.07739478e-01  7.76469698e-02 -1.05105298e-01 -2.73767700e-02\n",
      "   4.98169740e-02  4.53586656e-02  4.65098814e-02 -1.88246572e-02\n",
      "  -1.29893362e-01 -8.40756048e-02  3.29239807e-02 -1.73650505e-01\n",
      "   1.02187296e-01  2.60607954e-02  2.25794748e-02 -2.87165050e-02\n",
      "   3.06176228e-02  2.42408338e-02  3.30889803e-02 -7.81357783e-02\n",
      "   7.00805101e-02  4.84654759e-02  6.43561170e-02  1.34353810e-01\n",
      "   2.24484867e-02  2.77628283e-02  1.40517899e-02  8.12461038e-03\n",
      "  -1.24309230e-01  3.88402210e-02 -8.44447059e-02  2.34330051e-02\n",
      "  -2.14499467e-03  5.26637576e-02  9.72144384e-02  7.62815602e-02\n",
      "   7.46832871e-02  8.86126632e-02  6.69043210e-02  1.40700193e-04\n",
      "   7.12240956e-02 -3.74794866e-02 -7.79444657e-02 -1.91371619e-02\n",
      "  -8.27132808e-02 -1.44414610e-02 -5.68654644e-02 -1.03198328e-01\n",
      "   2.14897413e-02 -2.54015355e-02 -1.22394716e-01  1.98814690e-02\n",
      "   8.90335690e-02 -5.48140719e-02 -1.55645835e-02  1.08932057e-01\n",
      "  -1.65215265e-01 -4.29017253e-02 -1.34179287e-01 -2.80689621e-03\n",
      "   6.99055704e-02  1.85515779e-02 -5.44752132e-02  1.74463891e-01\n",
      "   6.61253630e-02  6.64653849e-02 -8.26822945e-02 -2.67229838e-02\n",
      "  -8.18405194e-03 -2.12077198e-01  1.57383080e-02 -1.20169308e-01\n",
      "  -4.55183268e-02  1.22330591e-01  2.46862619e-02 -4.86796081e-02\n",
      "   1.38476808e-01  1.63756468e-02  2.28167768e-02 -2.23559573e-02\n",
      "  -6.39192485e-02  9.01101476e-02 -7.06824054e-02 -4.61552828e-02\n",
      "   1.03998918e-02 -1.69303285e-01 -5.62135080e-02  1.19562354e-02\n",
      "  -9.70948628e-02  9.09691150e-02  9.41755737e-04 -3.47632233e-02\n",
      "   9.48041782e-02 -1.23333497e-01 -4.21803243e-02 -4.44769868e-02\n",
      "  -8.89176003e-02 -1.98266371e-02 -3.68677380e-02  5.54099932e-02\n",
      "  -1.21724172e-01 -1.04851244e-02  1.09016737e-02  1.20444910e-02\n",
      "  -6.54356829e-02 -5.41431354e-02 -7.96748797e-02 -1.31797579e-01\n",
      "   6.28846796e-02  2.19045304e-02  6.77688811e-02 -3.37180676e-02\n",
      "  -7.39695551e-02 -3.71753100e-02  1.26776778e-01 -1.00116988e-01\n",
      "  -2.23329956e-02 -5.36553461e-03  6.38183814e-02  3.06657180e-02\n",
      "   1.96725644e-02  3.87531927e-03  6.12212677e-02  4.68217173e-03\n",
      "  -2.64058929e-02 -5.26540413e-02  4.86455109e-02  2.81319396e-02\n",
      "   2.37075381e-02 -1.30455231e-01  1.21859049e-02  4.53124491e-02]\n",
      " [-7.29817325e-03  3.30183057e-02 -5.26041775e-02  1.38367729e-02\n",
      "   7.98390857e-02  9.72952746e-02  7.14879383e-02  1.20643360e-01\n",
      "   1.31731228e-01 -1.15219276e-02  1.05203176e-02  8.63493258e-02\n",
      "   5.68265326e-02  6.51327504e-02 -3.41970136e-02  1.75471510e-01\n",
      "   7.47086903e-02 -1.71883488e-02  9.56165928e-03  1.91756437e-02\n",
      "  -2.16533953e-01 -4.79590184e-02 -4.58670940e-03  1.60771313e-01\n",
      "  -6.16718086e-02 -6.42878105e-02  2.25893860e-02 -4.71617221e-02\n",
      "  -3.75778330e-02  6.50862910e-02 -3.82538344e-02 -4.69449680e-03\n",
      "  -9.63276263e-02 -7.67591464e-02 -1.82759523e-01 -2.04260375e-03\n",
      "  -2.68972572e-03  7.13498532e-02 -2.14347308e-02  1.22274754e-01\n",
      "   1.03038035e-01  1.59576747e-01  1.29237197e-04  4.40359337e-02\n",
      "   2.41364123e-02 -6.98502085e-02  1.15996737e-01  9.45352183e-03\n",
      "  -4.90766294e-02  7.55134294e-02  1.15484882e-01 -1.89582840e-02\n",
      "  -9.70768260e-03  7.75178014e-02  1.57119850e-02 -3.90085641e-03\n",
      "  -5.06842551e-02 -5.68356257e-02  4.65067352e-03 -5.91289322e-02\n",
      "   1.12029186e-01  5.18109651e-02 -2.15465612e-02  8.38271214e-03\n",
      "   1.80722582e-03 -1.21261696e-01 -5.66639099e-04  5.78209675e-02\n",
      "  -7.18789295e-02  5.40018565e-02 -3.22469458e-02  6.34794076e-02\n",
      "  -7.47613619e-02  2.47936943e-03 -8.34510266e-02  1.28491173e-01\n",
      "   2.23585570e-02  1.68970826e-02  4.02562649e-02  1.66862584e-01\n",
      "  -4.78817910e-02 -1.64676238e-02 -8.79938271e-02 -1.47115646e-01\n",
      "   7.48827245e-02  8.50655799e-02  2.29558467e-02 -3.81036149e-02\n",
      "  -5.11123362e-02 -1.31708985e-01  8.57479587e-02  1.02436392e-01\n",
      "   5.93352500e-02 -1.77345766e-02  1.26334289e-02 -1.43435393e-03\n",
      "   7.68536137e-03 -5.64265419e-02 -1.66013808e-02  5.83093379e-02\n",
      "   6.19510471e-02 -5.56279363e-02  4.59505030e-02 -1.82047712e-01\n",
      "   1.45901452e-01 -8.68830993e-02  8.08462746e-03 -8.22599808e-02\n",
      "   9.49246427e-02  3.97150926e-02  5.08177267e-02  2.17737610e-02\n",
      "  -2.00341876e-02 -8.91315268e-02 -4.93018129e-02 -5.59137800e-02\n",
      "   5.99784433e-02  3.31635580e-02  2.10372037e-04 -1.21064713e-02\n",
      "   5.64219960e-02  6.51160737e-02  4.69431608e-02 -4.73479588e-02\n",
      "  -4.36904539e-02  9.36475962e-04  6.07670070e-03  3.29602141e-03\n",
      "   9.33434503e-02  8.14427120e-02  1.05041099e-01  1.11940052e-02\n",
      "  -4.55744759e-03 -7.09239885e-02  4.52946092e-02  2.36846260e-02\n",
      "  -4.92049919e-02 -8.36305782e-03 -2.67184950e-02 -9.63209404e-02\n",
      "   7.88374584e-02 -6.11831881e-02  2.77733961e-02 -1.41500384e-01\n",
      "  -6.12043876e-02 -4.38566720e-02  8.77951416e-03 -1.32710263e-01\n",
      "   1.25363137e-01  6.73341579e-02 -5.09349688e-02  5.24582477e-02\n",
      "  -7.59553353e-02  3.67673023e-03  4.47732897e-02  5.87190245e-02\n",
      "   1.43328374e-01  2.08300429e-02  4.07922666e-02  2.31078710e-02\n",
      "  -1.61287902e-02  1.37207099e-02 -8.73962364e-02  2.70480839e-02\n",
      "   1.84785301e-02  3.99745693e-02 -3.84471061e-02 -5.60116286e-02\n",
      "   2.37227511e-02  4.34089811e-02  4.75525380e-04 -8.46930354e-02\n",
      "   5.41214684e-02  5.90564720e-02  3.33184870e-02 -6.55142005e-02\n",
      "   7.11953033e-02 -1.93795614e-02 -2.90505190e-02  1.25653721e-02\n",
      "   3.69587004e-02  2.13624161e-02 -6.09313942e-02  9.12957410e-02\n",
      "  -7.59699262e-02 -1.52880540e-01 -1.10061104e-01  5.94882891e-02\n",
      "   5.22857324e-02 -1.50166183e-02 -6.99567893e-02 -2.96367167e-02\n",
      "   5.84973047e-02  4.69635920e-02 -8.96498694e-02  8.10032827e-02\n",
      "  -4.09117126e-02 -1.22752067e-01 -4.13416232e-02 -3.17443644e-02]\n",
      " [-2.11734793e-02  8.53077290e-02 -8.76255056e-02  2.91312245e-02\n",
      "   1.21806511e-02 -6.84025489e-02  6.03485914e-02 -1.24176594e-01\n",
      "   3.63125701e-02  1.26066270e-01  6.36425203e-03 -4.59782298e-02\n",
      "   1.20129304e-01 -1.31186283e-01  2.52249531e-02 -1.34190002e-02\n",
      "   1.89300482e-02 -6.59697515e-02  5.34251635e-02 -1.16297338e-02\n",
      "  -4.72812111e-02  4.40842476e-02  8.73673289e-02 -1.03051922e-01\n",
      "   8.81326779e-02  1.38100627e-01  9.73949416e-03  8.48354821e-02\n",
      "   8.63175210e-02  2.76885820e-02  3.62829323e-02 -1.86901614e-01\n",
      "  -7.84085548e-02  1.22410264e-02 -8.04121013e-02 -1.30195384e-02\n",
      "   3.62954905e-02 -1.62764887e-02 -8.56026969e-02  6.51264291e-02\n",
      "   2.89029190e-02  1.27163427e-01 -1.68986763e-02 -1.35152804e-01\n",
      "  -1.09365930e-01 -7.39125206e-02 -1.15423738e-01 -1.21898157e-01\n",
      "   4.68253801e-02 -5.63119114e-02  4.33263282e-02 -3.90348418e-02\n",
      "   8.27397595e-03 -5.15580322e-02  3.36345468e-03 -8.22520248e-02\n",
      "  -2.61718025e-02  2.91449597e-02 -7.75192582e-03  2.41442146e-02\n",
      "   2.55897355e-02  4.13638690e-02  7.58439002e-02 -1.29266223e-02\n",
      "  -8.30812876e-02  2.27021174e-02 -5.25606675e-02 -5.43472324e-02\n",
      "   3.02929128e-03 -5.06753893e-02  8.97017601e-02  2.33001966e-02\n",
      "  -2.05604825e-02  8.66219308e-02  2.41164846e-02 -4.94331945e-02\n",
      "  -1.28306011e-01  1.26878830e-01 -8.48676839e-03  7.64306563e-02\n",
      "  -6.34207357e-02 -2.96246619e-02  1.30053025e-02  3.08225453e-02\n",
      "  -9.19184515e-02  7.45427669e-02  8.51683695e-02 -1.24015428e-01\n",
      "  -1.06734574e-02  6.64013667e-03 -1.93673805e-02 -7.22983894e-02\n",
      "   6.11747898e-02 -6.35613579e-02 -8.74566407e-02  7.43032113e-02\n",
      "   1.06406133e-01 -3.02372946e-02  7.77281282e-03 -1.05536754e-01\n",
      "   6.36973792e-03 -2.54052157e-02 -3.13080335e-02  1.83290989e-02\n",
      "  -5.97424411e-02  5.76974860e-02 -7.62490806e-02  6.62625773e-02\n",
      "  -3.39672514e-02  6.71178917e-02  1.58999831e-02  4.38832118e-02\n",
      "   3.68669126e-02  2.88236330e-02 -8.38958263e-02  1.79922025e-02\n",
      "   6.60699264e-02 -1.93870593e-02 -7.51858660e-02  7.86261027e-02\n",
      "   3.19990394e-02 -1.22033616e-01  1.96790754e-02  3.47667796e-02\n",
      "   5.70930425e-02 -4.03529322e-02  1.29546804e-01 -1.00759105e-01\n",
      "  -7.77070505e-02 -2.90263297e-02 -7.43725476e-02  9.25437771e-02\n",
      "   3.46381389e-02 -1.15193818e-01 -1.57365117e-02  4.73492766e-02\n",
      "   1.01506795e-01  5.52155221e-02 -5.08328570e-02 -1.42796406e-01\n",
      "   2.27049212e-02  4.56522044e-02 -8.08210742e-02 -8.69031022e-02\n",
      "   4.80070535e-02  2.30055198e-02 -1.18862609e-01 -2.63512420e-02\n",
      "  -6.78033992e-02 -5.96979338e-02  3.66654226e-02 -2.65685996e-02\n",
      "  -8.43304961e-02  5.47158654e-03  5.10703796e-03  1.57385640e-01\n",
      "  -1.16885724e-01  7.18589461e-02  4.35975996e-02 -1.24554457e-02\n",
      "   1.10822778e-01  1.08585266e-01 -8.16591045e-02  4.96038071e-02\n",
      "  -1.28810704e-01 -1.06624053e-01  1.52762611e-02  6.09421050e-02\n",
      "  -1.26004610e-01  1.31298640e-02 -1.54724464e-01  6.32547752e-02\n",
      "   1.51410436e-01  8.54840831e-02 -8.68415091e-02 -3.85467807e-02\n",
      "  -4.40379641e-02  1.46572399e-02 -1.09305725e-01 -8.22367634e-02\n",
      "   6.20783081e-02  4.63015409e-02 -4.93067502e-02 -3.71069700e-02\n",
      "  -8.08433895e-02  2.87470632e-02  1.35642199e-03 -3.03343973e-02\n",
      "  -4.15095086e-03 -2.10884826e-02 -1.21052964e-01 -1.06331151e-01\n",
      "   6.06818706e-02  3.23057168e-02 -7.56426476e-02 -1.02391762e-01\n",
      "  -6.73396324e-02 -8.60612661e-03  9.66533575e-03  4.86258858e-02]\n",
      " [-3.67557314e-02 -4.91096549e-03 -4.15161319e-03  5.05464722e-02\n",
      "  -4.15413388e-03  8.59228503e-02  1.25974778e-02 -5.92879574e-02\n",
      "   9.31115744e-02 -1.30207942e-01  4.99585060e-02  9.84361032e-02\n",
      "   2.38807935e-02 -9.24709807e-02  4.31737688e-02 -9.15661223e-02\n",
      "   1.02341679e-01 -1.13098451e-01 -1.89148824e-02 -5.48704786e-02\n",
      "  -7.17271671e-02  3.52203879e-02  4.52981732e-02 -3.15166694e-03\n",
      "   1.46496444e-01  1.24582870e-02 -1.16400865e-01 -6.75547393e-03\n",
      "   1.64386536e-01 -5.08079908e-02  6.84625026e-02  3.28752800e-02\n",
      "  -4.87824988e-02  2.91438910e-02 -4.74147036e-02  6.97519862e-02\n",
      "   4.57083322e-02 -2.60018037e-03 -6.44078168e-02  9.55951627e-02\n",
      "  -6.26147394e-03 -5.34208589e-02 -3.30968545e-02  2.06867661e-02\n",
      "  -1.06887553e-01 -6.28621348e-03 -3.26240892e-02 -8.01733813e-02\n",
      "  -5.96174467e-03  5.16936498e-02  1.20524103e-01 -9.37662255e-02\n",
      "  -1.30490877e-02 -6.70773275e-03 -9.44952025e-02 -3.49010523e-02\n",
      "   3.03854753e-03 -1.17394202e-02  1.21010887e-01  8.09975701e-02\n",
      "  -6.16600247e-02  3.69137245e-02 -6.79088761e-02 -2.19001949e-02\n",
      "   5.58320467e-02  1.96466017e-02  1.01557935e-02  1.78241500e-02\n",
      "   5.63858407e-02 -5.92963767e-03 -7.77672862e-02 -9.28891732e-02\n",
      "   2.76460749e-02  4.91876326e-02  3.44231246e-02 -5.29656967e-02\n",
      "   9.90905004e-02 -9.10597615e-02 -1.02531070e-01 -3.86261467e-02\n",
      "   2.46290050e-02 -2.16303558e-02 -5.31436580e-02 -1.70293338e-03\n",
      "   3.91055312e-02 -1.22566528e-02 -2.98568545e-02  5.04455559e-02\n",
      "   2.24253655e-02 -5.67409563e-02  7.12431198e-02  1.66682107e-01\n",
      "  -6.40747723e-02  5.39034835e-02 -2.97789607e-02  8.13927067e-03\n",
      "   8.60428671e-02 -1.56481302e-01  4.90014619e-03  9.41890254e-02\n",
      "  -8.04768889e-02 -6.16079207e-02 -3.02270500e-02  3.10919038e-02\n",
      "  -5.06922515e-02  6.71387120e-02 -4.41813414e-02 -1.07745487e-01\n",
      "   6.90745296e-02  3.07601546e-03 -4.96170281e-03  2.11260718e-02\n",
      "  -3.32846941e-02  5.39658961e-02  1.71582015e-02 -4.88483478e-02\n",
      "   5.07474958e-02  1.28168494e-01 -1.10248578e-01 -1.65731862e-03\n",
      "  -5.30987142e-02  7.32647009e-02 -2.27311257e-02 -9.58407731e-03\n",
      "   6.16151880e-02 -4.66073557e-02  1.29956648e-02  1.56861546e-01\n",
      "  -7.32931177e-02  2.21898298e-02  3.39897798e-02 -5.58666616e-02\n",
      "  -7.86407087e-02  2.24742230e-02  2.05653618e-02 -2.24397193e-02\n",
      "   9.17556866e-02 -5.53315433e-02 -5.23553615e-02  9.75878520e-02\n",
      "   1.31519258e-01  3.24185416e-02 -8.77619884e-02 -2.03603087e-04\n",
      "   7.51492222e-02  5.81441962e-02  6.90116952e-02 -6.94844458e-02\n",
      "   7.47756531e-02 -2.96004360e-02  3.43647474e-03 -6.59841940e-02\n",
      "   3.33399794e-02  2.68803617e-02 -7.88901320e-02 -1.98050323e-02\n",
      "  -3.66727467e-02  1.40968299e-01  8.33054343e-02  4.52741294e-02\n",
      "   5.89399651e-02 -1.34363666e-01  2.26328970e-02  7.51342088e-03\n",
      "  -2.68889234e-02  6.03981916e-02 -4.82976191e-02 -4.65923229e-02\n",
      "   2.20256238e-02 -7.02429817e-02  1.23286005e-01 -6.16093343e-03\n",
      "   8.18553000e-02  1.49338777e-02 -6.83322903e-02  3.04747897e-03\n",
      "  -8.78763474e-02 -2.70159478e-02  2.50322223e-02  4.30124808e-02\n",
      "  -8.14781086e-03  2.15032843e-02  3.45374262e-02  8.54719487e-02\n",
      "  -8.69185834e-03 -3.12396669e-02 -1.24599862e-01 -4.79699450e-02\n",
      "   2.12219685e-02 -1.18035745e-02 -2.79220914e-02  4.94096780e-02\n",
      "  -2.96416967e-02 -1.90827073e-02 -4.26018713e-02 -3.17518060e-02\n",
      "   3.20835240e-02  5.33768742e-02  7.20268472e-03 -1.99987514e-02]]\n",
      "[ 0.92379373 -0.51051149 -0.07205209 -0.4220182  -0.11824375]\n"
     ]
    }
   ],
   "source": [
    "# HeInitializerテスト\n",
    "he = HeInitializer()\n",
    "\n",
    "print(he.W(400, 200)[:5])\n",
    "print(he.B(200)[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題7】最適化手法\n",
    "学習率は学習過程で変化させていく方法が一般的です。基本的な手法である AdaGrad のクラスを作成してください。\n",
    "\n",
    "\n",
    "まず、これまで使ってきたSGDを確認します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adagrad:\n",
    "    \"\"\"\n",
    "    確率的勾配降下法\n",
    "    Parameters\n",
    "    ----------\n",
    "    lr : 学習率\n",
    "    \"\"\"\n",
    "    def __init__(self, lr=0.01):\n",
    "        self.lr = lr\n",
    "        self.HW = 0\n",
    "        self.HB = 0\n",
    "    def update(self, layer):\n",
    "        \"\"\"\n",
    "        ある層の重みやバイアスの更新\n",
    "        Parameters\n",
    "        ----------\n",
    "        layer : 更新前の層のインスタンス\n",
    "        \"\"\"\n",
    "        self.HW += np.square(layer.dW)\n",
    "        layer.W -= self.lr * layer.dW / (np.sqrt(self.HW) + 1e-7)\n",
    "        \n",
    "        self.HB += np.square(layer.dB)\n",
    "        layer.B -= self.lr * layer.dB / (np.sqrt(self.HB) + 1e-7)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題8】クラスの完成\n",
    "任意の構成で学習と推定が行えるScratchDeepNeuralNetrowkClassifierクラスを完成させてください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "(10000, 28, 28)\n",
      "uint8\n",
      "[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   3  18  18  18 126 136\n",
      "  175  26 166 255 247 127   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0  30  36  94 154 170 253 253 253 253 253\n",
      "  225 172 253 242 195  64   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0  49 238 253 253 253 253 253 253 253 253 251\n",
      "   93  82  82  56  39   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0  18 219 253 253 253 253 253 198 182 247 241\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0  80 156 107 253 253 205  11   0  43 154\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0  14   1 154 253  90   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0 139 253 190   2   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0  11 190 253  70   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0  35 241 225 160 108   1\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0  81 240 253 253 119\n",
      "   25   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0  45 186 253 253\n",
      "  150  27   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  16  93 252\n",
      "  253 187   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 249\n",
      "  253 249  64   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0  46 130 183 253\n",
      "  253 207   2   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0  39 148 229 253 253 253\n",
      "  250 182   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0  24 114 221 253 253 253 253 201\n",
      "   78   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0  23  66 213 253 253 253 253 198  81   2\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0  18 171 219 253 253 253 253 195  80   9   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0  55 172 226 253 253 253 253 244 133  11   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0 136 253 253 253 212 135 132  16   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]]\n",
      "1.0\n",
      "0.0\n",
      "(60000,)\n",
      "(60000, 10)\n",
      "float64\n",
      "(48000, 784)\n",
      "(12000, 784)\n"
     ]
    }
   ],
   "source": [
    "# データセットの読み込みから前処理まで\n",
    "from keras.datasets import mnist\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "print(X_train.shape) # (60000, 28, 28)\n",
    "print(X_test.shape) # (10000, 28, 28)\n",
    "print(X_train[0].dtype) # uint8\n",
    "print(X_train[0])\n",
    "\n",
    "X_train = X_train.reshape(-1, 784)\n",
    "X_test = X_test.reshape(-1, 784)\n",
    "\n",
    "X_train = X_train.astype(np.float)\n",
    "X_test = X_test.astype(np.float)\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "print(X_train.max()) # 1.0\n",
    "print(X_train.min()) # 0.0\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "y_train_one_hot = enc.fit_transform(y_train[:, np.newaxis])\n",
    "y_test_one_hot = enc.transform(y_test[:, np.newaxis])\n",
    "print(y_train.shape) # (60000,)\n",
    "print(y_train_one_hot.shape) # (60000, 10)\n",
    "print(y_train_one_hot.dtype) # float64\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2)\n",
    "print(X_train.shape) # (48000, 784)\n",
    "print(X_val.shape) # (12000, 784)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SGD, SimpleInitializer, Tanh \n",
    "class ScratchDeepNeuralNetrowkClassifier:\n",
    "    def __init__(self, lr, n_nodes1, n_nodes2, n_output, sigma=0.01):\n",
    "        self.lr = lr\n",
    "        self.n_nodes1 = n_nodes1\n",
    "        self.n_nodes2 = n_nodes2\n",
    "        self.n_output = n_output\n",
    "        self.sigma = sigma\n",
    "    \n",
    "    def fit(self, X, y, X_val=None, y_val=None, epoch=20, batch_size=20):\n",
    "        \"\"\"\n",
    "        ニューラルネットワーク分類器を学習する。\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (n_samples, self.n_features)\n",
    "            訓練データの特徴量\n",
    "        y : 次の形のndarray, shape (n_samples, )\n",
    "            訓練データの正解値\n",
    "        X_val : 次の形のndarray, shape (n_samples, self.n_features)\n",
    "            検証データの特徴量\n",
    "        y_val : 次の形のndarray, shape (n_samples, )\n",
    "            検証データの正解値\n",
    "        \"\"\"\n",
    "        self.n_samples = X.shape[0]\n",
    "        self.n_features = X.shape[1]\n",
    "        self.loss = np.zeros(epoch)\n",
    "        self.val_loss = np.zeros(epoch)\n",
    "        \n",
    "        # 各インスタンス初期化\n",
    "        optimizer = SGD(self.lr)\n",
    "        self.FC1 = FC(self.n_features, self.n_nodes1, SimpleInitializer(self.sigma), optimizer)\n",
    "        self.activation1 = Tanh()\n",
    "        self.FC2 = FC(self.n_nodes1, self.n_nodes2, SimpleInitializer(self.sigma), optimizer)\n",
    "        self.activation2 = Tanh()\n",
    "        self.FC3 = FC(self.n_nodes2, self.n_output, SimpleInitializer(self.sigma), optimizer)\n",
    "        self.activation3 = Softmax()\n",
    "              \n",
    "        # ラベル化\n",
    "        enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "        y_train_one_hot = enc.fit_transform(y[:, np.newaxis])\n",
    "        \n",
    "        if X_val is not None and y_val is not None:\n",
    "            y_val_one_hot = enc.fit_transform(y_val[:, np.newaxis])\n",
    " \n",
    "        \n",
    "        for i in range(epoch):\n",
    "            get_mini_batch = GetMiniBatch(X, y_train_one_hot, batch_size=batch_size)\n",
    "            loss_list = []\n",
    "            \n",
    "            for mini_X_train, mini_y_train in get_mini_batch:\n",
    "                # フォワードプロバゲーション\n",
    "                A1 = self.FC1.forward(mini_X_train)     #shape(n_batch, n_nodes1)\n",
    "                Z1 = self.activation1.forward(A1)     #shape(n_batch, n_nodes1)\n",
    "                A2 = self.FC2.forward(Z1)        #shape(n_batch, n_nodes2\n",
    "                Z2 = self.activation2.forward(A2)       #shape(n_batch, n_nodes2)\n",
    "                A3 = self.FC3.forward(Z2)       #shape(n_batch, n_output)\n",
    "                Z3 = self.activation3.forward(A3)     #shape(n_batch , n_output)\n",
    "                \n",
    "                # バックプロバゲーション\n",
    "                dA3, error = self.activation3.backward(Z3, mini_y_train) # 交差エントロピー誤差とソフトマックスを合わせている\n",
    "                loss_list.append(error)\n",
    "                dZ2 = self.FC3.backward(dA3)     \n",
    "                dA2 = self.activation2.backward(dZ2)\n",
    "                dZ1 = self.FC2.backward(dA2)\n",
    "                dA1 = self.activation1.backward(dZ1)\n",
    "                dZ0 = self.FC1.backward(dA1) # dZ0は使用しない        \n",
    "            \n",
    "            self.loss[i] =np.sum(loss_list)/len(loss_list)\n",
    "            \n",
    "        for i in range(epoch):\n",
    "            get_mini_batch = GetMiniBatch(X_val, y_val_one_hot, batch_size=20)\n",
    "            val_loss_list = []\n",
    "\n",
    "            for mini_X_val, mini_y_val in get_mini_batch:\n",
    "                # フォワードプロバゲーション**************************\n",
    "                A1 = self.FC1.forward(mini_X_val)     #shape(n_batch, n_nodes1)\n",
    "                Z1 = self.activation1.forward(A1)     #shape(n_batch, n_nodes1)\n",
    "                A2 = self.FC2.forward(Z1)        #shape(n_batch, n_nodes2\n",
    "                Z2 = self.activation2.forward(A2)       #shape(n_batch, n_nodes2)\n",
    "                A3 = self.FC3.forward(Z2)       #shape(n_batch, n_output)\n",
    "                Z3 = self.activation3.forward(A3)     #shape(n_batch , n_output)\n",
    "\n",
    "                # クロスエントロピー誤差**************************            \n",
    "                _, val_error = self.activation3.backward(Z3, mini_y_val) # 交差エントロピー誤差とソフトマックスを合わせている\n",
    "                val_loss_list.append(val_error)\n",
    "\n",
    "            self.val_loss[i]= np.sum(val_loss_list) / len(val_loss_list)\n",
    "            \n",
    "    def predict(self,X):\n",
    "        A1 = self.FC1.forward(X)\n",
    "        Z1 = self.activation1.forward(A1)\n",
    "        A2 = self.FC2.forward(Z1)\n",
    "        Z2 = self.activation2.forward(A2)\n",
    "        A3 = self.FC3.forward(Z2)\n",
    "        y = self.activation3.forward(A3)\n",
    "        \n",
    "        pred = np.argmax(y, axis=1)\n",
    "        \n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:0.9649\n"
     ]
    }
   ],
   "source": [
    "# SGD、Tanhで実行\n",
    "scr_DNN = ScratchDeepNeuralNetrowkClassifier(lr=0.01, n_nodes1=400, n_nodes2=200, n_output=10)\n",
    "\n",
    "scr_DNN.fit(X_train, y_train, X_val, y_val, epoch=20)\n",
    "pred = scr_DNN.predict(X_test)\n",
    "print(\"Accuracy:{}\".format(accuracy_score(y_test, pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# loss可視化\n",
    "plt.plot(scr_DNN.loss, marker=\".\", label=\"train_loss\")\n",
    "plt.plot(scr_DNN.val_loss, marker=\".\", label=\"val_loss\")\n",
    "\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adagrad、Heinitializer, Relu\n",
    "# Node数を３００→２００→１００→５０→１０\n",
    "class ScratchDeepNeuralNetrowkClassifier2:\n",
    "    def __init__(self, lr, n_nodes1, n_nodes2, n_nodes3, n_nodes4, n_output, sigma=0.01):\n",
    "        self.lr = lr\n",
    "        self.n_nodes1 = n_nodes1\n",
    "        self.n_nodes2 = n_nodes2\n",
    "        self.n_nodes3 = n_nodes3\n",
    "        self.n_nodes4 = n_nodes4\n",
    "        self.n_output = n_output\n",
    "        self.sigma = sigma\n",
    "    \n",
    "    def fit(self, X, y, X_val=None, y_val=None, epoch=20, batch_size=20):\n",
    "        \"\"\"\n",
    "        ニューラルネットワーク分類器を学習する。\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (n_samples, self.n_features)\n",
    "            訓練データの特徴量\n",
    "        y : 次の形のndarray, shape (n_samples, )\n",
    "            訓練データの正解値\n",
    "        X_val : 次の形のndarray, shape (n_samples, self.n_features)\n",
    "            検証データの特徴量\n",
    "        y_val : 次の形のndarray, shape (n_samples, )\n",
    "            検証データの正解値\n",
    "        \"\"\"\n",
    "        self.n_samples = X.shape[0]\n",
    "        self.n_features = X.shape[1]\n",
    "        self.loss = np.zeros(epoch)\n",
    "        self.val_loss = np.zeros(epoch)\n",
    "        \n",
    "        # 各インスタンス初期化\n",
    "        optimizer = Adagrad(self.lr)\n",
    "        self.FC1 = FC(self.n_features, self.n_nodes1, HeInitializer(), copy.deepcopy(optimizer))\n",
    "        self.activation1 = Tanh()\n",
    "        self.FC2 = FC(self.n_nodes1, self.n_nodes2, HeInitializer(), copy.deepcopy(optimizer))\n",
    "        self.activation2 = Tanh()\n",
    "        self.FC3 = FC(self.n_nodes2, self.n_nodes3, HeInitializer(), copy.deepcopy(optimizer))\n",
    "        self.activation3 = Relu()\n",
    "        self.FC4 = FC(self.n_nodes3, self.n_nodes4, HeInitializer(), copy.deepcopy(optimizer))        \n",
    "        self.activation4 = Relu()\n",
    "        self.FC5 = FC(self.n_nodes4, self.n_output, HeInitializer(), copy.deepcopy(optimizer))\n",
    "        self.activation5 = Softmax()\n",
    "              \n",
    "        # ラベル化\n",
    "        enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "        y_train_one_hot = enc.fit_transform(y[:, np.newaxis])\n",
    "        \n",
    "        if X_val is not None and y_val is not None:\n",
    "            y_val_one_hot = enc.fit_transform(y_val[:, np.newaxis])\n",
    " \n",
    "        \n",
    "        for i in range(epoch):\n",
    "            get_mini_batch = GetMiniBatch(X, y_train_one_hot, batch_size=batch_size)\n",
    "            loss_list = []\n",
    "            \n",
    "            for mini_X_train, mini_y_train in get_mini_batch:\n",
    "                # フォワードプロバゲーション\n",
    "                A1 = self.FC1.forward(mini_X_train)     #shape(n_batch, n_nodes1)\n",
    "                Z1 = self.activation1.forward(A1)     #shape(n_batch, n_nodes1)\n",
    "                A2 = self.FC2.forward(Z1)        #shape(n_batch, n_nodes2\n",
    "                Z2 = self.activation2.forward(A2)       #shape(n_batch, n_nodes2)\n",
    "                A3 = self.FC3.forward(Z2)       #shape(n_batch, n_output)\n",
    "                Z3 = self.activation3.forward(A3)     #shape(n_batch , n_output)\n",
    "                A4 = self.FC4.forward(Z3)       #shape(n_batch, n_output)\n",
    "                Z4 = self.activation4.forward(A4)     #shape(n_batch , n_output)\n",
    "                A5 = self.FC5.forward(Z4)       #shape(n_batch, n_output)\n",
    "                Z5 = self.activation5.forward(A5)     #shape(n_batch , n_output)\n",
    "                \n",
    "                # バックプロバゲーション\n",
    "                dA5, error = self.activation5.backward(Z5, mini_y_train) # 交差エントロピー誤差とソフトマックスを合わせている\n",
    "                loss_list.append(error)\n",
    "                dZ4 = self.FC5.backward(dA5)     \n",
    "                dA4 = self.activation4.backward(dZ4)\n",
    "                dZ3 = self.FC4.backward(dA4)     \n",
    "                dA3 = self.activation3.backward(dZ3)\n",
    "                dZ2 = self.FC3.backward(dA3)     \n",
    "                dA2 = self.activation2.backward(dZ2)\n",
    "                dZ1 = self.FC2.backward(dA2)\n",
    "                dA1 = self.activation1.backward(dZ1)\n",
    "                dZ0 = self.FC1.backward(dA1) # dZ0は使用しない        \n",
    "            \n",
    "            self.loss[i] =np.sum(loss_list)/len(loss_list)\n",
    "            \n",
    "        for i in range(epoch):\n",
    "            get_mini_batch = GetMiniBatch(X_val, y_val_one_hot, batch_size=20)\n",
    "            val_loss_list = []\n",
    "\n",
    "            for mini_X_val, mini_y_val in get_mini_batch:\n",
    "                # フォワードプロバゲーション**************************\n",
    "                A1 = self.FC1.forward(mini_X_val)     #shape(n_batch, n_nodes1)\n",
    "                Z1 = self.activation1.forward(A1)     #shape(n_batch, n_nodes1)\n",
    "                A2 = self.FC2.forward(Z1)        #shape(n_batch, n_nodes2\n",
    "                Z2 = self.activation2.forward(A2)       #shape(n_batch, n_nodes2)\n",
    "                A3 = self.FC3.forward(Z2)       #shape(n_batch, n_output)\n",
    "                Z3 = self.activation3.forward(A3)     #shape(n_batch , n_output)\n",
    "                A4 = self.FC4.forward(Z3)       #shape(n_batch, n_output)\n",
    "                Z4 = self.activation4.forward(A4)     #shape(n_batch , n_output)\n",
    "                A5 = self.FC5.forward(Z4)       #shape(n_batch, n_output)\n",
    "                Z5 = self.activation5.forward(A5)     #shape(n_batch , n_output)\n",
    "                # クロスエントロピー誤差**************************            \n",
    "                _, val_error = self.activation5.backward(Z5, mini_y_val) # 交差エントロピー誤差とソフトマックスを合わせている\n",
    "                val_loss_list.append(val_error)\n",
    "\n",
    "            self.val_loss[i]= np.sum(val_loss_list) / len(val_loss_list)\n",
    "            \n",
    "    def predict(self,X):\n",
    "        A1 = self.FC1.forward(X)\n",
    "        Z1 = self.activation1.forward(A1)\n",
    "        A2 = self.FC2.forward(Z1)\n",
    "        Z2 = self.activation2.forward(A2)\n",
    "        A3 = self.FC3.forward(Z2)\n",
    "        Z3 = self.activation3.forward(A3)\n",
    "        A4 = self.FC4.forward(Z3)       #shape(n_batch, n_output)\n",
    "        Z4 = self.activation4.forward(A4)     #shape(n_batch , n_output)\n",
    "        A5 = self.FC5.forward(Z4)       #shape(n_batch, n_output)\n",
    "        y = self.activation5.forward(A5)     #shape(n_batch , n_output)\n",
    "        \n",
    "        pred = np.argmax(y, axis=1)\n",
    "        \n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:0.9783\n"
     ]
    }
   ],
   "source": [
    "scr_DNN2 = ScratchDeepNeuralNetrowkClassifier2(lr=0.01, n_nodes1=300, n_nodes2=200, n_nodes3=100, n_nodes4=50, n_output=10)\n",
    "\n",
    "scr_DNN2.fit(X_train, y_train, X_val, y_val, epoch=20)\n",
    "pred2 = scr_DNN2.predict(X_test)\n",
    "print(\"Accuracy:{}\".format(accuracy_score(y_test, pred2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# loss可視化\n",
    "plt.plot(scr_DNN2.loss, marker=\".\", label=\"train_loss\")\n",
    "plt.plot(scr_DNN2.val_loss, marker=\".\", label=\"val_loss\")\n",
    "\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 以下は各クラスの実装に問題がないか過程で調べるのに作成"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adagrad、SimpleInitializer, Tanhで実行\n",
    "\n",
    "class ScratchDeepNeuralNetrowkClassifier2:\n",
    "    def __init__(self, lr, n_nodes1, n_nodes2, n_output, sigma=0.01):\n",
    "        self.lr = lr\n",
    "        self.n_nodes1 = n_nodes1\n",
    "        self.n_nodes2 = n_nodes2\n",
    "        self.n_output = n_output\n",
    "        self.sigma = sigma\n",
    "    \n",
    "    \n",
    "    def fit(self, X, y, X_val=None, y_val=None, epoch=20):\n",
    "        \"\"\"\n",
    "        ニューラルネットワーク分類器を学習する。\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (n_samples, self.n_features)\n",
    "            訓練データの特徴量\n",
    "        y : 次の形のndarray, shape (n_samples, )\n",
    "            訓練データの正解値\n",
    "        X_val : 次の形のndarray, shape (n_samples, self.n_features)\n",
    "            検証データの特徴量\n",
    "        y_val : 次の形のndarray, shape (n_samples, )\n",
    "            検証データの正解値\n",
    "        \"\"\"\n",
    "        self.n_features = X.shape[1]\n",
    "        self.loss = np.zeros(epoch)\n",
    "        \n",
    "        # 各インスタンス初期化\n",
    "        optimizer = Adagrad(self.lr)\n",
    "        self.FC1 = FC(self.n_features, self.n_nodes1, SimpleInitializer(sigma=0.01), copy.deepcopy(optimizer))\n",
    "        self.activation1 = Tanh()\n",
    "        self.FC2 = FC(self.n_nodes1, self.n_nodes2, SimpleInitializer(sigma=0.01), copy.deepcopy(optimizer))\n",
    "        self.activation2 = Tanh()\n",
    "        self.FC3 = FC(self.n_nodes2, self.n_output, SimpleInitializer(sigma=0.01), copy.deepcopy(optimizer))\n",
    "        self.activation3 = Softmax()\n",
    "              \n",
    "        # ラベル化\n",
    "        enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "        y_train_one_hot = enc.fit_transform(y[:, np.newaxis])\n",
    "        \n",
    "        for i in range(epoch):\n",
    "            get_mini_batch = GetMiniBatch(X, y_train_one_hot, batch_size=20)\n",
    "            \n",
    "            for mini_X_train, mini_y_train in get_mini_batch:\n",
    "                # フォワードプロバゲーション\n",
    "                A1 = self.FC1.forward(mini_X_train)     #shape(n_batch, n_nodes1)\n",
    "                Z1 = self.activation1.forward(A1)     #shape(n_batch, n_nodes1)\n",
    "                A2 = self.FC2.forward(Z1)        #shape(n_batch, n_nodes2\n",
    "                Z2 = self.activation2.forward(A2)       #shape(n_batch, n_nodes2)\n",
    "                A3 = self.FC3.forward(Z2)       #shape(n_batch, n_output)\n",
    "                Z3 = self.activation3.forward(A3)     #shape(n_batch , n_output)\n",
    "                \n",
    "                # バックプロバゲーション\n",
    "                dA3, self.loss[i] = self.activation3.backward(Z3, mini_y_train) # 交差エントロピー誤差とソフトマックスを合わせている\n",
    "                dZ2 = self.FC3.backward(dA3)     \n",
    "                dA2 = self.activation2.backward(dZ2)\n",
    "                dZ1 = self.FC2.backward(dA2)\n",
    "                dA1 = self.activation1.backward(dZ1)\n",
    "                dZ0 = self.FC1.backward(dA1) # dZ0は使用しない        \n",
    "        \n",
    "        \n",
    "    def predict(self,X):\n",
    "        A1 = self.FC1.forward(X)\n",
    "        Z1 = self.activation1.forward(A1)\n",
    "        A2 = self.FC2.forward(Z1)\n",
    "        Z2 = self.activation2.forward(A2)\n",
    "        A3 = self.FC3.forward(Z2)\n",
    "        y = self.activation3.forward(A3)\n",
    "        \n",
    "        pred = np.argmax(y, axis=1)\n",
    "        \n",
    "        return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scr_DNN2 = ScratchDeepNeuralNetrowkClassifier2(lr=0.01, n_nodes1=400, n_nodes2=200, n_output=10)\n",
    "\n",
    "scr_DNN2.fit(X_train, y_train, epoch=20)\n",
    "\n",
    "pred2 = scr_DNN2.predict(X_test)\n",
    "print(\"Accuracy:{}\".format(accuracy_score(y_test, pred2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SGD、XavierInitializer, sigmoidで実行\n",
    "\n",
    "class ScratchDeepNeuralNetrowkClassifier3:\n",
    "    def __init__(self, lr, n_nodes1, n_nodes2, n_output, sigma=0.01):\n",
    "        self.lr = lr\n",
    "        self.n_nodes1 = n_nodes1\n",
    "        self.n_nodes2 = n_nodes2\n",
    "        self.n_output = n_output\n",
    "        self.sigma = sigma\n",
    "    \n",
    "    \n",
    "    def fit(self, X, y, X_val=None, y_val=None, epoch=20):\n",
    "        \"\"\"\n",
    "        ニューラルネットワーク分類器を学習する。\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (n_samples, self.n_features)\n",
    "            訓練データの特徴量\n",
    "        y : 次の形のndarray, shape (n_samples, )\n",
    "            訓練データの正解値\n",
    "        X_val : 次の形のndarray, shape (n_samples, self.n_features)\n",
    "            検証データの特徴量\n",
    "        y_val : 次の形のndarray, shape (n_samples, )\n",
    "            検証データの正解値\n",
    "        \"\"\"\n",
    "        self.n_features = X.shape[1]\n",
    "        self.loss = np.zeros(epoch)\n",
    "        \n",
    "        # 各インスタンス初期化\n",
    "        optimizer = SGD(self.lr)\n",
    "        self.FC1 = FC(self.n_features, self.n_nodes1, XavierInitializer(), copy.deepcopy(optimizer))\n",
    "        self.activation1 = Sigmoid()\n",
    "        self.FC2 = FC(self.n_nodes1, self.n_nodes2, XavierInitializer(), copy.deepcopy(optimizer))\n",
    "        self.activation2 = Sigmoid()\n",
    "        self.FC3 = FC(self.n_nodes2, self.n_output, XavierInitializer(), copy.deepcopy(optimizer))\n",
    "        self.activation3 = Softmax()\n",
    "              \n",
    "        # ラベル化\n",
    "        enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "        y_train_one_hot = enc.fit_transform(y[:, np.newaxis])\n",
    "        \n",
    "        for i in range(epoch):\n",
    "            get_mini_batch = GetMiniBatch(X, y_train_one_hot, batch_size=20)\n",
    "            \n",
    "            for mini_X_train, mini_y_train in get_mini_batch:\n",
    "                # フォワードプロバゲーション\n",
    "                A1 = self.FC1.forward(mini_X_train)     #shape(n_batch, n_nodes1)\n",
    "                Z1 = self.activation1.forward(A1)     #shape(n_batch, n_nodes1)\n",
    "                A2 = self.FC2.forward(Z1)        #shape(n_batch, n_nodes2\n",
    "                Z2 = self.activation2.forward(A2)       #shape(n_batch, n_nodes2)\n",
    "                A3 = self.FC3.forward(Z2)       #shape(n_batch, n_output)\n",
    "                Z3 = self.activation3.forward(A3)     #shape(n_batch , n_output)\n",
    "                \n",
    "                # バックプロバゲーション\n",
    "                dA3, self.loss[i] = self.activation3.backward(Z3, mini_y_train) # 交差エントロピー誤差とソフトマックスを合わせている\n",
    "                dZ2 = self.FC3.backward(dA3)     \n",
    "                dA2 = self.activation2.backward(dZ2)\n",
    "                dZ1 = self.FC2.backward(dA2)\n",
    "                dA1 = self.activation1.backward(dZ1)\n",
    "                dZ0 = self.FC1.backward(dA1) # dZ0は使用しない        \n",
    "        \n",
    "        \n",
    "    def predict(self,X):\n",
    "        A1 = self.FC1.forward(X)\n",
    "        Z1 = self.activation1.forward(A1)\n",
    "        A2 = self.FC2.forward(Z1)\n",
    "        Z2 = self.activation2.forward(A2)\n",
    "        A3 = self.FC3.forward(Z2)\n",
    "        y = self.activation3.forward(A3)\n",
    "        \n",
    "        pred = np.argmax(y, axis=1)\n",
    "        \n",
    "        return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scr_DNN3 = ScratchDeepNeuralNetrowkClassifier3(lr=0.01, n_nodes1=400, n_nodes2=200, n_output=10)\n",
    "\n",
    "scr_DNN3.fit(X_train, y_train, epoch=20)\n",
    "\n",
    "pred3 = scr_DNN3.predict(X_test)\n",
    "\n",
    "print(\"Accuracy:{}\".format(accuracy_score(y_test, pred3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SGD、Heinitializer, Reluで実行\n",
    "\n",
    "class ScratchDeepNeuralNetrowkClassifier4:\n",
    "    def __init__(self, lr, n_nodes1, n_nodes2, n_output, sigma=0.01):\n",
    "        self.lr = lr\n",
    "        self.n_nodes1 = n_nodes1\n",
    "        self.n_nodes2 = n_nodes2\n",
    "        self.n_output = n_output\n",
    "        self.sigma = sigma\n",
    "    \n",
    "    \n",
    "    def fit(self, X, y, X_val=None, y_val=None, epoch=20):\n",
    "        \"\"\"\n",
    "        ニューラルネットワーク分類器を学習する。\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (n_samples, self.n_features)\n",
    "            訓練データの特徴量\n",
    "        y : 次の形のndarray, shape (n_samples, )\n",
    "            訓練データの正解値\n",
    "        X_val : 次の形のndarray, shape (n_samples, self.n_features)\n",
    "            検証データの特徴量\n",
    "        y_val : 次の形のndarray, shape (n_samples, )\n",
    "            検証データの正解値\n",
    "        \"\"\"\n",
    "        self.n_features = X.shape[1]\n",
    "        self.loss = np.zeros(epoch)\n",
    "        \n",
    "        # 各インスタンス初期化\n",
    "        optimizer = SGD(self.lr)\n",
    "        self.FC1 = FC(self.n_features, self.n_nodes1, HeInitializer(), copy.deepcopy(optimizer))\n",
    "        self.activation1 = Relu()\n",
    "        self.FC2 = FC(self.n_nodes1, self.n_nodes2, HeInitializer(), copy.deepcopy(optimizer))\n",
    "        self.activation2 = Relu()\n",
    "        self.FC3 = FC(self.n_nodes2, self.n_output, HeInitializer(), copy.deepcopy(optimizer))\n",
    "        self.activation3 = Softmax()\n",
    "              \n",
    "        # ラベル化\n",
    "        enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "        y_train_one_hot = enc.fit_transform(y[:, np.newaxis])\n",
    "        \n",
    "        for i in range(epoch):\n",
    "            get_mini_batch = GetMiniBatch(X, y_train_one_hot, batch_size=20)\n",
    "            \n",
    "            for mini_X_train, mini_y_train in get_mini_batch:\n",
    "                # フォワードプロバゲーション\n",
    "                A1 = self.FC1.forward(mini_X_train)     #shape(n_batch, n_nodes1)\n",
    "                Z1 = self.activation1.forward(A1)     #shape(n_batch, n_nodes1)\n",
    "                A2 = self.FC2.forward(Z1)        #shape(n_batch, n_nodes2\n",
    "                Z2 = self.activation2.forward(A2)       #shape(n_batch, n_nodes2)\n",
    "                A3 = self.FC3.forward(Z2)       #shape(n_batch, n_output)\n",
    "                Z3 = self.activation3.forward(A3)     #shape(n_batch , n_output)\n",
    "                \n",
    "                # バックプロバゲーション\n",
    "                dA3, self.loss[i] = self.activation3.backward(Z3, mini_y_train) # 交差エントロピー誤差とソフトマックスを合わせている\n",
    "                dZ2 = self.FC3.backward(dA3)     \n",
    "                dA2 = self.activation2.backward(dZ2)\n",
    "                dZ1 = self.FC2.backward(dA2)\n",
    "                dA1 = self.activation1.backward(dZ1)\n",
    "                dZ0 = self.FC1.backward(dA1) # dZ0は使用しない        \n",
    "        \n",
    "        \n",
    "    def predict(self,X):\n",
    "        A1 = self.FC1.forward(X)\n",
    "        Z1 = self.activation1.forward(A1)\n",
    "        A2 = self.FC2.forward(Z1)\n",
    "        Z2 = self.activation2.forward(A2)\n",
    "        A3 = self.FC3.forward(Z2)\n",
    "        y = self.activation3.forward(A3)\n",
    "        \n",
    "        pred = np.argmax(y, axis=1)\n",
    "        \n",
    "        return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scr_DNN4 = ScratchDeepNeuralNetrowkClassifier4(lr=0.01, n_nodes1=400, n_nodes2=200, n_output=10)\n",
    "\n",
    "scr_DNN4.fit(X_train, y_train, epoch=20)\n",
    "\n",
    "pred4 = scr_DNN4.predict(X_test)\n",
    "print(\"Accuracy:{}\".format(accuracy_score(y_test, pred4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 自習\n",
    "- sklearnモデル[sklearn.neural_network.MLPClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html)を利用してみる\n",
    "- パラメータが非常に多い。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:0.9754\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "clf = MLPClassifier(random_state=1, max_iter=300, early_stopping=True)\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "pred3 = clf.predict(X_test)\n",
    "print(\"Accuracy:{}\".format(accuracy_score(y_test, pred3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### スクラッチモデルをsklearnと同じようにインスタンス時のパラメータでinitilizerやoptimizerを選べるようにしてみる"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SGD, SimpleInitializer, Tanh \n",
    "# val_loss追加\n",
    "# loss計算をエポックごとの平均値を取得し格納\n",
    "class ScratchDeepNeuralNetrowkClassifier５:\n",
    "    def __init__(self, lr, n_nodes1, n_nodes2, n_output, sigma=0.01):\n",
    "        self.lr = lr\n",
    "        self.n_nodes1 = n_nodes1\n",
    "        self.n_nodes2 = n_nodes2\n",
    "        self.n_output = n_output\n",
    "        self.sigma = sigma\n",
    "        \n",
    "    def fit(self, X, y, X_val=None, y_val=None, epoch=20, batch_size=20):\n",
    "        \"\"\"\n",
    "        ニューラルネットワーク分類器を学習する。\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (n_samples, self.n_features)\n",
    "            訓練データの特徴量\n",
    "        y : 次の形のndarray, shape (n_samples, )\n",
    "            訓練データの正解値\n",
    "        X_val : 次の形のndarray, shape (n_samples, self.n_features)\n",
    "            検証データの特徴量\n",
    "        y_val : 次の形のndarray, shape (n_samples, )\n",
    "            検証データの正解値\n",
    "        \"\"\"\n",
    "        self.n_samples = X.shape[0]\n",
    "        self.n_features = X.shape[1]\n",
    "        self.loss = np.zeros(epoch)\n",
    "        self.val_loss = np.zeros(epoch)\n",
    "    \n",
    "        # 各インスタンス初期化\n",
    "        optimizer = SGD(self.lr)\n",
    "        self.FC1 = FC(self.n_features, self.n_nodes1, SimpleInitializer(self.sigma), optimizer)\n",
    "        self.activation1 = Tanh()\n",
    "        self.FC2 = FC(self.n_nodes1, self.n_nodes2, SimpleInitializer(self.sigma), optimizer)\n",
    "        self.activation2 = Tanh()\n",
    "        self.FC3 = FC(self.n_nodes2, self.n_output, SimpleInitializer(self.sigma), optimizer)\n",
    "        self.activation3 = Softmax()\n",
    "              \n",
    "        # ラベル化\n",
    "        enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "        y_train_one_hot = enc.fit_transform(y[:, np.newaxis])\n",
    "        \n",
    "        loss_list = []\n",
    "        \n",
    "        for i in range(epoch):\n",
    "            get_mini_batch = GetMiniBatch(X, y_train_one_hot, batch_size=batch_size)\n",
    "            \n",
    "            for mini_X_train, mini_y_train in get_mini_batch:\n",
    "                # フォワードプロバゲーション\n",
    "                A1 = self.FC1.forward(mini_X_train)     #shape(n_batch, n_nodes1)\n",
    "                Z1 = self.activation1.forward(A1)     #shape(n_batch, n_nodes1)\n",
    "                A2 = self.FC2.forward(Z1)        #shape(n_batch, n_nodes2\n",
    "                Z2 = self.activation2.forward(A2)       #shape(n_batch, n_nodes2)\n",
    "                A3 = self.FC3.forward(Z2)       #shape(n_batch, n_output)\n",
    "                Z3 = self.activation3.forward(A3)     #shape(n_batch , n_output)\n",
    "                \n",
    "                # バックプロバゲーション\n",
    "                dA3, error = self.activation3.backward(Z3, mini_y_train) # 交差エントロピー誤差とソフトマックスを合わせている\n",
    "                loss_list.append(error)\n",
    "                dZ2 = self.FC3.backward(dA3)     \n",
    "                dA2 = self.activation2.backward(dZ2)\n",
    "                dZ1 = self.FC2.backward(dA2)\n",
    "                dA1 = self.activation1.backward(dZ1)\n",
    "                dZ0 = self.FC1.backward(dA1) # dZ0は使用しない        \n",
    "            \n",
    "            self.loss[i] = np.sum(loss_list) / len(loss_list)\n",
    "        \n",
    "        if X_val is not None and y_val is not None:\n",
    "            y_val_one_hot = enc.fit_transform(y_val[:, np.newaxis])\n",
    "            \n",
    "            for i in range(epoch):\n",
    "                get_mini_batch = GetMiniBatch(X_val, y_val_one_hot, batch_size=20)\n",
    "                val_loss_list = []\n",
    "\n",
    "                for mini_X_val, mini_y_val in get_mini_batch:\n",
    "                    # フォワードプロバゲーション**************************\n",
    "                    A1 = self.FC1.forward(mini_X_val)     #shape(n_batch, n_nodes1)\n",
    "                    Z1 = self.activation1.forward(A1)     #shape(n_batch, n_nodes1)\n",
    "                    A2 = self.FC2.forward(Z1)        #shape(n_batch, n_nodes2\n",
    "                    Z2 = self.activation2.forward(A2)       #shape(n_batch, n_nodes2)\n",
    "                    A3 = self.FC3.forward(Z2)       #shape(n_batch, n_output)\n",
    "                    Z3 = self.activation3.forward(A3)     #shape(n_batch , n_output)\n",
    "                    \n",
    "                    # バックプロバゲーション**************************            \n",
    "                    dA3, val_error = self.activation3.backward(Z3, mini_y_val) # 交差エントロピー誤差とソフトマックスを合わせている\n",
    "                    val_loss_list.append(error)\n",
    "                    dZ2 = self.FC3.backward(dA3)     \n",
    "                    dA2 = self.activation2.backward(dZ2)\n",
    "                    dZ1 = self.FC2.backward(dA2)\n",
    "                    dA1 = self.activation1.backward(dZ1)\n",
    "                    dZ0 = self.FC1.backward(dA1) # dZ0は使用しない        \n",
    "                \n",
    "                self.val_loss[i] = np.sum(val_loss_list) / len(val_loss_list)\n",
    "        \n",
    "    def predict(self,X):\n",
    "        A1 = self.FC1.forward(X)\n",
    "        Z1 = self.activation1.forward(A1)\n",
    "        A2 = self.FC2.forward(Z1)\n",
    "        Z2 = self.activation2.forward(A2)\n",
    "        A3 = self.FC3.forward(Z2)\n",
    "        y = self.activation3.forward(A3)\n",
    "        \n",
    "        pred = np.argmax(y, axis=1)\n",
    "        \n",
    "        return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 上記は個人的宿題"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "426.667px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "747.205px",
    "left": "1763.33px",
    "right": "20px",
    "top": "120px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
