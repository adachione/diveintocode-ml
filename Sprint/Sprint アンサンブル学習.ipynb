{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sprint アンサンブル学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/adachi-yuya/.pyenv/versions/anaconda3-5.3.1/lib/python3.7/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import statistics\n",
    "import math\n",
    "import itertools\n",
    "import copy\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import linear_model\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"../Sprint/train.csv\")\n",
    "df_train = df_train.set_index(\"Id\")\n",
    "df_test = pd.read_csv(\"../Sprint/test.csv\")\n",
    "df_test = df_test.set_index(\"Id\")\n",
    "# display(df_train.head())\n",
    "# display(df_test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1168, 2)\n",
      "(292, 2)\n",
      "(1168,)\n",
      "(292,)\n"
     ]
    }
   ],
   "source": [
    "# (問題文)目的変数としてSalePrice、説明変数として、GrLivAreaとYearBuiltを使います。\n",
    "# 訓練データと検証データに分割\n",
    "X1 = df_train.loc[:, [\"GrLivArea\", \"YearBuilt\"]].values\n",
    "y1 = df_train.iloc[:, -1].values\n",
    "\n",
    "X1_train, X1_test, y1_train, y1_test =  train_test_split(X1, y1, test_size=0.2, random_state=0)\n",
    "print(X1_train.shape)\n",
    "print(X1_test.shape)\n",
    "print(y1_train.shape)\n",
    "print(y1_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習関数\n",
    "# 標準化しないモデル\n",
    "def _notScaler(X, y, model):\n",
    "    # (問題文)train.csvを学習用（train）8割、検証用（val）2割に分割\n",
    "    X_train, X_test, y_train, y_test =  train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "    return mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習関数\n",
    "# 標準化しないモデル\n",
    "def _Scaler(X, y, model):\n",
    "    # (問題文)train.csvを学習用（train）8割、検証用（val）2割に分割\n",
    "    X_train, X_test, y_train, y_test =  train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X_train)   #後でスケーリングするために使用する平均と標準を計算します。\n",
    "    X_train_scaler = scaler.transform(X_train)   # 標準化　センタリングとスケーリングによって標準化を実行する\n",
    "    X_test_scaler = scaler.transform(X_test)     # 標準化　センタリングとスケーリングによって標準化を実行する\n",
    "    \n",
    "    model.fit(X_train_scaler, y_train)\n",
    "    y_pred = model.predict(X_test_scaler)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "    return mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# グラフ化関数\n",
    "# classの実装で利用\n",
    "def _yyplot(y_test, y_pred, model):\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    yvalues = np.concatenate([y_test.flatten(), y_pred.flatten()])\n",
    "    ymin, ymax, yrange = np.amin(yvalues), np.amax(yvalues), np.ptp(yvalues)\n",
    "    fig = plt.figure(figsize=(8, 8))\n",
    "    plt.scatter(y_test, y_pred, label='MSE : %.3f, MAE : %.3f \\n R2 = %.3f'%(\n",
    "                mse, mae, r2s))\n",
    "    \n",
    "    plt.plot([ymin - yrange * 0.01, ymax + yrange * 0.01], [ymin - yrange * 0.01, ymax + yrange * 0.01])\n",
    "    plt.xlim(ymin - yrange * 0.01, ymax + yrange * 0.01)\n",
    "    plt.ylim(ymin - yrange * 0.01, ymax + yrange * 0.01)\n",
    "    plt.xlabel('y_test', fontsize=24)\n",
    "    plt.ylabel('y_predicted', fontsize=24)\n",
    "    plt.title('Plot by {}'.format(model), fontsize=16)\n",
    "    plt.tick_params(labelsize=16)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 線形回帰モデル用意\n",
    "lr = LinearRegression()\n",
    "svm = SVR()\n",
    "dt = DecisionTreeRegressor(random_state=0)\n",
    "dt2 = DecisionTreeRegressor(max_depth=10, max_leaf_nodes=5, random_state=0)\n",
    "reg = linear_model.Lasso(alpha=0.1)\n",
    "NN = MLPRegressor(random_state=0, max_iter=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/adachi-yuya/.pyenv/versions/anaconda3-5.3.1/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LinearRegression</th>\n",
       "      <th>SVM</th>\n",
       "      <th>DecisionTreeRegressor</th>\n",
       "      <th>DecisionTreeRegressor(Param_change)</th>\n",
       "      <th>Lasso</th>\n",
       "      <th>NeuralN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>MSE</th>\n",
       "      <td>2.942067e+09</td>\n",
       "      <td>7.243320e+09</td>\n",
       "      <td>3.009170e+09</td>\n",
       "      <td>2.916621e+09</td>\n",
       "      <td>2.942067e+09</td>\n",
       "      <td>2.840032e+10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     LinearRegression           SVM  DecisionTreeRegressor  \\\n",
       "MSE      2.942067e+09  7.243320e+09           3.009170e+09   \n",
       "\n",
       "     DecisionTreeRegressor(Param_change)         Lasso       NeuralN  \n",
       "MSE                         2.916621e+09  2.942067e+09  2.840032e+10  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 単独学習\n",
    "index_list = [\"MSE\"]\n",
    "model_dict = {\"LinearRegression\": lr, \"SVM\": svm, \n",
    "              \"DecisionTreeRegressor\": dt, \"DecisionTreeRegressor(Param_change)\": dt2,\n",
    "              \"Lasso\": reg, \"NeuralN\": NN}\n",
    "df_table = pd.DataFrame(np.zeros, index=index_list, columns=model_dict.keys())\n",
    "\n",
    "for model_name, model in model_dict.items():\n",
    "    df_table[model_name] = _notScaler(X1, y1, model=model)\n",
    "\n",
    "df_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/adachi-yuya/.pyenv/versions/anaconda3-5.3.1/lib/python3.7/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/Users/adachi-yuya/.pyenv/versions/anaconda3-5.3.1/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LinearRegression</th>\n",
       "      <th>SVM</th>\n",
       "      <th>DecisionTreeRegressor</th>\n",
       "      <th>DecisionTreeRegressor(Param_change)</th>\n",
       "      <th>Lasso</th>\n",
       "      <th>NeuralN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>MSE</th>\n",
       "      <td>2.942067e+09</td>\n",
       "      <td>7.221625e+09</td>\n",
       "      <td>2.983519e+09</td>\n",
       "      <td>2.916621e+09</td>\n",
       "      <td>2.942065e+09</td>\n",
       "      <td>3.979373e+10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     LinearRegression           SVM  DecisionTreeRegressor  \\\n",
       "MSE      2.942067e+09  7.221625e+09           2.983519e+09   \n",
       "\n",
       "     DecisionTreeRegressor(Param_change)         Lasso       NeuralN  \n",
       "MSE                         2.916621e+09  2.942065e+09  3.979373e+10  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 単独学習\n",
    "# 標準化\n",
    "index_list = [\"MSE\"]\n",
    "model_dict = {\"LinearRegression\": lr, \"SVM\": svm, \n",
    "              \"DecisionTreeRegressor\": dt, \"DecisionTreeRegressor(Param_change)\": dt2,\n",
    "              \"Lasso\": reg, \"NeuralN\": NN}\n",
    "\n",
    "df_table_scaler = pd.DataFrame(np.zeros, index=index_list, columns=model_dict.keys())\n",
    "\n",
    "for model_name, model in model_dict.items():\n",
    "    df_table_scaler[model_name] = _Scaler(X1, y1, model=model)\n",
    "    \n",
    "df_table_scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 単一モデルのMSEの値が確認できた\n",
    "- 線形回帰モデルが一番低い値となった。\n",
    "- 標準化処置の有無で若干の値変動はあるが、そこまで重要でない模様。\n",
    "- 線形回帰の2.942e+9を基準とする。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【問題1】ブレンディングのスクラッチ実装\n",
    "ブレンディング をスクラッチ実装し、単一モデルより精度があがる例を 最低3つ 示してください。精度があがるとは、検証用データに対する平均二乗誤差（MSE）が小さくなることを指します。\n",
    "\n",
    "\n",
    "ブレンディングとは\n",
    "ブレンディングとは、N個の多様なモデルを独立して学習させ、推定結果を重み付けした上で足し合わせる方法です。最も単純には平均をとります。多様なモデルとは、以下のような条件を変化させることで作り出すものです。\n",
    "\n",
    "\n",
    "手法（例：線形回帰、SVM、決定木、ニューラルネットワークなど）\n",
    "ハイパーパラメータ（例：SVMのカーネルの種類、重みの初期値など）\n",
    "入力データの前処理の仕方（例：標準化、対数変換、PCAなど）\n",
    "\n",
    "重要なのはそれぞれのモデルが大きく異なることです。\n",
    "\n",
    "\n",
    "回帰問題でのブレンディングは非常に単純であるため、scikit-learnには用意されていません。\n",
    "\n",
    "\n",
    "《補足》\n",
    "\n",
    "\n",
    "分類問題の場合は、多数決を行います。回帰問題に比べると複雑なため、scikit-learnにはVotingClassifierが用意されています。\n",
    "\n",
    "\n",
    "sklearn.ensemble.VotingClassifier — scikit-learn 0.21.3 documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScratchBlending():\n",
    "    \"\"\"\n",
    "    K-meansのスクラッチ実装\n",
    "\n",
    "    Parameters\n",
    "    ----------  ##########TODOあとで書き換える###########\n",
    "    estimators : 学習モデル群_辞書型\n",
    "    n_jobs : 使うか不明だが仮置き\n",
    "    weights :使うか不明だが仮置き\n",
    "    verbose : 学習過程のプリント\n",
    "    \"\"\"\n",
    "    def __init__(self, estimators, n_jobs=None, weights=None, verbose=False):\n",
    "        # ハイパーパラメータを属性として記録\n",
    "        self.estimators = estimators\n",
    "        self.n_jobs = n_jobs\n",
    "        self.weights = weights\n",
    "        self.verbose = verbose\n",
    "\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        複数の学習モデルをfor文で全通り学習させる\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (n_samples, n_features)\n",
    "            訓練データの特徴量\n",
    "        y : 次の形のndarray, shape (n_samples, )\n",
    "            訓練データのラベル\n",
    "        \"\"\"\n",
    "        self.n_samples = X.shape[0]\n",
    "        self.n_features = X.shape[1]\n",
    "        self.fit_data_list = []\n",
    "        \n",
    "        for model in self.estimators:\n",
    "            fit_data = model.fit(X, y)\n",
    "            self.fit_data_list.append(fit_data)           \n",
    "            \n",
    "        if self.verbose:\n",
    "            pass\n",
    "            #verboseをTrueにした際は学習過程を出力\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        K-meansによるクラスタリングを計算\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (n_samples, n_features)\n",
    "            訓練データの特徴量\n",
    "        \n",
    "        return\n",
    "        pred_values_mean : 次の形のndarray, shape (n_samples, )\n",
    "        訓練データの学習モデル分のpredict値の平均\n",
    "\n",
    "        \"\"\"\n",
    "        # 平均値を取る箱を作成\n",
    "        pred_values = np.zeros((len(self.estimators), X.shape[0]))\n",
    "        \n",
    "        # 全モデルのpredictメソッドをループさせる\n",
    "        for i, model in enumerate(self.fit_data_list):\n",
    "            pred_values[i] = model.predict(X)\n",
    "        \n",
    "        # 平均値を計算\n",
    "        pred_values_mean = np.mean(pred_values, axis=0)\n",
    "        \n",
    "        return pred_values_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list = [lr, svm, dt, dt2, reg, NN]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 例1（学習モデルは3つ、特徴量は未標準化）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[model1]\n",
      "MSE:2.722e+09\n"
     ]
    }
   ],
   "source": [
    "# 学習モデル3つ(線形回帰, SVM, 決定木)\n",
    "# 未標準化\n",
    "model1_list = model_list[:3]\n",
    "\n",
    "model1 = ScratchBlending(estimators=model1_list)\n",
    "\n",
    "# 未標準化の訓練データで学習\n",
    "model1.fit(X1_train, y1_train)\n",
    "\n",
    "# 予測値は全モデルの平均値\n",
    "pred1 = model1.predict(X1_test)\n",
    "\n",
    "# MSEを算出\n",
    "mse = mean_squared_error(y1_test, pred1)\n",
    "\n",
    "print(\"[model1]\")\n",
    "print(\"MSE:{:.3e}\".format(mse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 線形回帰と決定木の単独モデルより少し高い結果になった。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 例2（学習モデルは3つ、特徴量を標準化）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[model2]\n",
      "MSE:2.719e+09\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/adachi-yuya/.pyenv/versions/anaconda3-5.3.1/lib/python3.7/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "# 標準化\n",
    "scaler = StandardScaler()\n",
    "\n",
    "scaler.fit(X1_train)   #後でスケーリングするために使用する平均と標準を計算します。\n",
    "X1_train_scaler = scaler.transform(X1_train)   # 標準化　センタリングとスケーリングによって標準化を実行する\n",
    "X1_test_scaler = scaler.transform(X1_test)     # 標準化　センタリングとスケーリングによって標準化を実行する\n",
    "\n",
    "# 学習モデルはmodel1と同様\n",
    "model2 = ScratchBlending(estimators=model1_list)\n",
    "\n",
    "# 標準化の訓練データで学習\n",
    "model2.fit(X1_train_scaler, y1_train)\n",
    "\n",
    "# 予測値は全モデルの平均値\n",
    "pred2 = model2.predict(X1_test_scaler)\n",
    "\n",
    "# MSEを算出\n",
    "mse2 = mean_squared_error(y1_test, pred2)\n",
    "\n",
    "print(\"[model2]\")\n",
    "print(\"MSE:{:.3e}\".format(mse2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 例2も下がるには下がったが、単独で最も数値が悪かったSVMが入っているのは悪影響？\n",
    "- 標準化の有無はさほど影響なしと思われる"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 例3　SVMを含めない3つのモデルでブレンディング"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[model3]\n",
      "MSE:5.642e+09\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/adachi-yuya/.pyenv/versions/anaconda3-5.3.1/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "# 学習モデル3つ(決定木パラム変更, Lasso, NN)\n",
    "model3_list = model_list[-3:]\n",
    "\n",
    "model3 = ScratchBlending(estimators=model3_list)\n",
    "\n",
    "# 未標準化の訓練データで学習\n",
    "model3.fit(X1_train, y1_train)\n",
    "\n",
    "# 予測値は全モデルの平均値\n",
    "pred3 = model3.predict(X1_test)\n",
    "\n",
    "# MSEを算出\n",
    "mse3 = mean_squared_error(y1_test, pred3)\n",
    "\n",
    "print(\"[model3]\")\n",
    "print(\"MSE:{:.3e}\".format(mse3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ものすごく悪くなった。。。原因不明。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 例4~6（学習モデルは2つ_各組み合わせで実行）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[学習モデル:線形回帰とSVM]MSE:3.776e+09\n",
      "[決定木（パラムチェンジ）とlasso]MSE:2.917e+09\n",
      "[lassoとNN]MSE:9.107e+09\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/adachi-yuya/.pyenv/versions/anaconda3-5.3.1/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "# 学習モデル組み合わせ\n",
    "model_list4 = model_list[:2]       # 線形回帰とSVMの2つ\n",
    "model_list5 = model_list[3:4]    # 決定木（パラムチェンジ）とlasso\n",
    "model_list6 = model_list[-2:]     # lassoとNN\n",
    "\n",
    "model4 = ScratchBlending(estimators=model_list4)\n",
    "model5 = ScratchBlending(estimators=model_list5)\n",
    "model6 = ScratchBlending(estimators=model_list6)\n",
    "\n",
    "# 標準化の訓練データで学習\n",
    "model4.fit(X1_train, y1_train)\n",
    "model5.fit(X1_train, y1_train)\n",
    "model6.fit(X1_train, y1_train)\n",
    "\n",
    "# 予測値は全モデルの平均値\n",
    "pred4 = model4.predict(X1_test)\n",
    "pred5 = model5.predict(X1_test)\n",
    "pred6 = model6.predict(X1_test)\n",
    "\n",
    "# MSEを算出\n",
    "mse4 = mean_squared_error(y1_test, pred4)\n",
    "mse5 = mean_squared_error(y1_test, pred5)\n",
    "mse6 = mean_squared_error(y1_test, pred6)\n",
    "\n",
    "print(\"[学習モデル:線形回帰とSVM]MSE:{:.3e}\".format(mse4))\n",
    "print(\"[決定木（パラムチェンジ）とlasso]MSE:{:.3e}\".format(mse5))\n",
    "print(\"[lassoとNN]MSE:{:.3e}\".format(mse6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2つの組み合わせだとさほど変わらない、悪くなることもある"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 例7~9　学習モデルを4つ以上で複数組み合わせ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[学習モデル4つ]MSE:2.576e+09\n",
      "[学習モデル5つ]MSE:2.523e+09\n",
      "[学習モデル6つ]MSE:3.608e+09\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/adachi-yuya/.pyenv/versions/anaconda3-5.3.1/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "# 例7−9\n",
    "model_list7 = model_list[:4]    # 線形回帰とSVMの2つ\n",
    "model_list8 = model_list[:5]    # 決定木（パラムチェンジ）とlasso\n",
    "model_list9 = model_list[:]      # lassoとNN\n",
    "\n",
    "model7 = ScratchBlending(estimators=model_list7)\n",
    "model8 = ScratchBlending(estimators=model_list8)\n",
    "model9 = ScratchBlending(estimators=model_list9)\n",
    "\n",
    "# 標準化の訓練データで学習\n",
    "model7.fit(X1_train, y1_train)\n",
    "model8.fit(X1_train, y1_train)\n",
    "model9.fit(X1_train, y1_train)\n",
    "\n",
    "# 予測値は全モデルの平均値\n",
    "pred7 = model7.predict(X1_test)\n",
    "pred8 = model8.predict(X1_test)\n",
    "pred9 = model9.predict(X1_test)\n",
    "\n",
    "# MSEを算出\n",
    "mse7 = mean_squared_error(y1_test, pred7)\n",
    "mse8 = mean_squared_error(y1_test, pred8)\n",
    "mse9 = mean_squared_error(y1_test, pred9)\n",
    "\n",
    "print(\"[学習モデル4つ]MSE:{:.3e}\".format(mse7))\n",
    "print(\"[学習モデル5つ]MSE:{:.3e}\".format(mse8))\n",
    "print(\"[学習モデル6つ]MSE:{:.3e}\".format(mse9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MSEが小さくなったモデルは例1と例7と例8のモデル（例2は参考外）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "基準値MSEは線形回帰単独モデルの2.942e+9\n",
      "[model1:学習モデル3つ]MSE:2.722e+09\n",
      "[model7:学習モデル4つ]MSE:2.576e+09\n",
      "[model8:学習モデル5つ]MSE:2.523e+09\n"
     ]
    }
   ],
   "source": [
    "print(\"基準値MSEは線形回帰単独モデルの2.942e+9\")\n",
    "print(\"[model1:学習モデル3つ]MSE:{:.3e}\".format(mse))\n",
    "print(\"[model7:学習モデル4つ]MSE:{:.3e}\".format(mse7))\n",
    "print(\"[model8:学習モデル5つ]MSE:{:.3e}\".format(mse8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 最終考察\n",
    "- ブレンディングの効果について理解できた。\n",
    "- NeuralNetworkモデルが入ったケースは悪くなった印象\n",
    "- 学習モデルを増やせばMSEも下がる傾向だが、model9のように悪影響を与えてしまうケースもあることがわかった。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題2】バギングのスクラッチ実装\n",
    "バギング をスクラッチ実装し、単一モデルより精度があがる例を 最低1つ 示してください。\n",
    "\n",
    "\n",
    "バギングとは\n",
    "バギングは入力データの選び方を多様化する方法です。学習データから重複を許した上でランダムに抜き出すことで、N種類のサブセット（ ブートストラップサンプル ）を作り出します。それらによってモデルをN個学習し、推定結果の平均をとります。ブレンディングと異なり、それぞれの重み付けを変えることはありません。\n",
    "\n",
    "\n",
    "sklearn.model_selection.train_test_split — scikit-learn 0.21.3 documentation\n",
    "\n",
    "\n",
    "scikit-learnのtrain_test_splitを、shuffleパラメータをTrueにして使うことで、ランダムにデータを分割することができます。これによりブートストラップサンプルが手に入ります。\n",
    "\n",
    "\n",
    "推定結果の平均をとる部分はブースティングと同様の実装になります。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScratchBagging():\n",
    "    \"\"\"\n",
    "    K-meansのスクラッチ実装\n",
    "\n",
    "    Parameters\n",
    "    ----------  ##########TODOあとで書き換える###########\n",
    "    estimators : 学習モデル群_辞書型\n",
    "    n_jobs : \n",
    "    weights :\n",
    "    verbose : \n",
    "    \"\"\"\n",
    "    def __init__(self, model, bagging, n_jobs=None, weights=None, verbose=False):\n",
    "        # ハイパーパラメータを属性として記録\n",
    "        self.model = model\n",
    "        self.bagging = bagging\n",
    "        self.n_jobs = n_jobs\n",
    "        self.weights = weights\n",
    "        self.verbose = verbose\n",
    "\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        K-meansによるクラスタリングを計算\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (n_samples, n_features)\n",
    "            訓練データの特徴量\n",
    "        y : 次の形のndarray, shape (n_samples, )\n",
    "            訓練データのラベル\n",
    "        \"\"\"\n",
    "        self.n_samples = X.shape[0]\n",
    "        self.n_features = X.shape[1]\n",
    "        self.fit_data_list = []\n",
    "        \n",
    "        for i in range(self.bagging):\n",
    "            _, bootstrap_X, _ , bootstrap_y  = train_test_split(X, y, shuffle=True, test_size=0.2)\n",
    "            model = copy.deepcopy(self.model)\n",
    "            fit_data = model.fit(bootstrap_X, bootstrap_y)\n",
    "            self.fit_data_list.append(fit_data)\n",
    "            \n",
    "                \n",
    "        if self.verbose:\n",
    "            pass\n",
    "            #verboseをTrueにした際は学習過程を出力\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        K-meansによるクラスタリングを計算\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (n_samples, n_features)\n",
    "            訓練データの特徴量\n",
    "        \n",
    "        return\n",
    "        pred_values_mean : 次の形のndarray, shape (n_samples, )\n",
    "        訓練データの学習モデル分のpredict値の平均\n",
    "\n",
    "        \"\"\"\n",
    "        # 平均値を取る箱を作成\n",
    "        pred_values = np.zeros((self.bagging, X.shape[0]))\n",
    "        \n",
    "        # 全モデルのpredictメソッドをループさせる\n",
    "        for i, model in enumerate(self.fit_data_list):\n",
    "            pred_values[i] = model.predict(X)\n",
    "                \n",
    "        # 平均値を計算\n",
    "        pred_values_mean = np.mean(pred_values, axis=0)\n",
    "        \n",
    "        return pred_values_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "scr_bagging = ScratchBagging(model=lr,bagging=10, verbose=False)\n",
    "scr_bagging.fit(X1_train, y1_train)\n",
    "bag_pred = scr_bagging.predict(X1_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[学習モデル:線形回帰_バギング:10model]MSE:2.965e+09\n"
     ]
    }
   ],
   "source": [
    "mse10 = mean_squared_error(y1_test, bag_pred)\n",
    "\n",
    "print(\"[学習モデル:線形回帰_バギング:10model]MSE:{:.3e}\".format(mse10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 単独モデルより若干下がった・・・"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【自習】決定木モデルで仮想ランダムフォレスト"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "scr_bagging2 = ScratchBagging(model=dt,bagging=10, verbose=False)\n",
    "\n",
    "scr_bagging2.fit(X1_train, y1_train)\n",
    "\n",
    "bag_pred2 = scr_bagging2.predict(X1_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[学習モデル:決定木_バギング:10model]MSE:2.002e+09\n"
     ]
    }
   ],
   "source": [
    "mse10 = mean_squared_error(y1_test, bag_pred2)\n",
    "print(\"[学習モデル:決定木_バギング:10model]MSE:{:.3e}\".format(mse10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 学習モデルを決定木に変えたところ、大きく減少した。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題3】スタッキングのスクラッチ実装\n",
    "スタッキング をスクラッチ実装し、単一モデルより精度があがる例を 最低1つ 示してください。\n",
    "\n",
    "\n",
    "スタッキングとは\n",
    "スタッキングの手順は以下の通りです。最低限ステージ0とステージ1があればスタッキングは成立するため、それを実装してください。まずは $K_0=3, M_0=2$ 程度にします。\n",
    "\n",
    "\n",
    "《学習時》\n",
    "\n",
    "\n",
    "（ステージ $0$ ）\n",
    "\n",
    "\n",
    "学習データを $K_0$ 個に分割する。\n",
    "分割した内の $(K_0 - 1)$ 個をまとめて学習用データ、残り $1$ 個を推定用データとする組み合わせが $K_0$ 個作れる。\n",
    "あるモデルのインスタンスを $K_0$ 個用意し、異なる学習用データを使い学習する。\n",
    "それぞれの学習済みモデルに対して、使っていない残り $1$ 個の推定用データを入力し、推定値を得る。（これをブレンドデータと呼ぶ）\n",
    "さらに、異なるモデルのインスタンスも $K_0$ 個用意し、同様のことを行う。モデルが $M_0$ 個あれば、 $M_0$ 個のブレンドデータが得られる。\n",
    "\n",
    "（ステージ $n$ ）\n",
    "\n",
    "\n",
    "ステージ $n-1$ のブレンドデータを$M_{n-1}$ 次元の特徴量を持つ学習用データと考え、 $K_n$ 個に分割する。以下同様である。\n",
    "\n",
    "（ステージ $N$ ）＊最後のステージ\n",
    "\n",
    "\n",
    "ステージ $N-1$ の $M_{N-1}$ 個のブレンドデータを$M_{N-1}$ 次元の特徴量の入力として、1種類のモデルの学習を行う。これが最終的な推定を行うモデルとなる。\n",
    "\n",
    "《推定時》\n",
    "\n",
    "\n",
    "（ステージ $0$ ）\n",
    "\n",
    "\n",
    "テストデータを $K_0×M_0$ 個の学習済みモデルに入力し、$K_0×M_0$ 個の推定値を得る。これを $K_0$ の軸で平均値を求め $M_0$ 次元の特徴量を持つデータを得る。（ブレンドテストと呼ぶ）\n",
    "\n",
    "（ステージ $n$ ）\n",
    "\n",
    "\n",
    "ステージ $n-1$ で得たブレンドテストを $K_n×M_n$ 個の学習済みモデルに入力し、$K_n×M_n$ 個の推定値を得る。これを $K_n$ の軸で平均値を求め $M_0$ 次元の特徴量を持つデータを得る。（ブレンドテストと呼ぶ）\n",
    "\n",
    "（ステージ $N$ ）＊最後のステージ\n",
    "\n",
    "\n",
    "ステージ $N-1$ で得たブレンドテストを学習済みモデルに入力し、推定値を得る。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScratchStacking():\n",
    "    \"\"\"\n",
    "    スタッキング\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, k=3, verbose=False, verbose_interval=1, random_state=None):\n",
    "        # ハイパーパラメータを属性として記録\n",
    "        self.k = k\n",
    "        self.verbose = verbose\n",
    "        self.verbose_interval = verbose_interval\n",
    "        self.random_state = random_state\n",
    "        self.next_stage = None\n",
    "       \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (n_samples, n_features)\n",
    "            訓練データの特徴量\n",
    "        \"\"\"\n",
    "        \n",
    "        self.n_samples = len(X)\n",
    "        self.n_models = 3\n",
    "        self.blend_data = np.zeros([self.n_samples, self.n_models])\n",
    "        # 学習\n",
    "        self.trainingForStacking = TrainingForStacking(self.n_samples, self.n_models, self.k, self.verbose, self.verbose_interval, self.random_state)\n",
    "        self.blend_data = self.trainingForStacking._stage_n(X, y, self.blend_data)\n",
    "        # 学習（ラストステージ）\n",
    "        self.trainingForStacking._last_stage(self.blend_data, y)\n",
    "        \n",
    "        #verboseをTrueにした際は学習過程を出力            \n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        推定\n",
    "        \"\"\"\n",
    "        preds = np.zeros([len(X), self.n_models])\n",
    "        pre_pred = self.trainingForStacking._predict_stage_n(X, preds) / self.n_samples\n",
    "        pred = self.trainingForStacking._predict_last_stage(pre_pred)\n",
    "        \n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingForStacking():\n",
    "    def __init__(self, n_samples, n_models, k, verbose, verbose_interval, random_state):\n",
    "        # ハイパーパラメータを属性として記録\n",
    "        self.k = k\n",
    "        self.verbose = verbose\n",
    "        self.verbose_interval = verbose_interval\n",
    "        self.random_state = random_state\n",
    "        self.next_stage = None\n",
    "        self.model_list = ['LinearRegression',\n",
    "                           'DecisionTreeRegressor',\n",
    "                           \"Lasso\"]\n",
    "        self.n_samples = n_samples\n",
    "        self.n_models = n_models\n",
    "        self.all_idx = np.arange(self.n_samples)\n",
    "        self.div_idx = np.array_split(self.all_idx, self.k)\n",
    "        \n",
    "        self.stop = False\n",
    "        \n",
    "    def _stage_n(self, X, y, blend_data, div_no=0):\n",
    "        self.div_no = div_no\n",
    "        self.blend_data = blend_data.copy()\n",
    "        \n",
    "        self.other_idx = np.delete(self.all_idx, self.div_idx[self.div_no])\n",
    "        \n",
    "        X_train, X_test = X[self.other_idx, :], X[self.div_idx[self.div_no]]\n",
    "        y_train, y_test = y[self.other_idx], y[self.div_idx[self.div_no]]\n",
    "        \n",
    "        self.model_0 = LinearRegression()\n",
    "        self.model_1 = DecisionTreeRegressor()\n",
    "        self.model_2 = linear_model.Lasso(alpha=0.1)\n",
    "\n",
    "        self.model_0.fit(X_train, y_train)\n",
    "        self.model_1.fit(X_train, y_train)\n",
    "        self.model_2.fit(X_train, y_train)\n",
    "        \n",
    "        self.blend_data[self.div_idx[self.div_no], 0] = self.model_0.predict(X_test)\n",
    "        self.blend_data[self.div_idx[self.div_no], 1] = self.model_1.predict(X_test)\n",
    "        self.blend_data[self.div_idx[self.div_no], 2] = self.model_2.predict(X_test)\n",
    "        \n",
    "        self.mse_0 = mean_squared_error(y_test, self.blend_data[self.div_idx[self.div_no], 0])\n",
    "        self.mse_1 = mean_squared_error(y_test, self.blend_data[self.div_idx[self.div_no], 1])\n",
    "        self.mse_2 = mean_squared_error(y_test, self.blend_data[self.div_idx[self.div_no], 2])\n",
    "\n",
    "        if self.verbose:\n",
    "            print('**********', self.model_list[0], '-', self.div_no, '**********')\n",
    "            print('MSE(平均二乗誤差)(test) : {:,.3e}'.format(self.mse_0))\n",
    "            print('**********', self.model_list[1], '-', self.div_no, '**********')\n",
    "            print('MSE(平均二乗誤差)(test) : {:,.3e}'.format(self.mse_1))\n",
    "            print('**********', self.model_list[2], '-', self.div_no, '**********')\n",
    "            print('MSE(平均二乗誤差)(test) : {:,.3e}'.format(self.mse_2))\n",
    "        \n",
    "        self.div_no += 1\n",
    "        if self.div_no >= self.k:\n",
    "            self.stop = True\n",
    "            return self.blend_data\n",
    "        self.next_stage = TrainingForStacking(self.n_samples, self.n_models, self.k, self.verbose, self.verbose_interval, self.random_state)\n",
    "        self.blend_data = self.next_stage._stage_n(X, y, self.blend_data, self.div_no)\n",
    "        \n",
    "        return self.blend_data\n",
    "\n",
    "        \n",
    "        \n",
    "    def _last_stage(self, blend_data, y):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(blend_data, y, test_size=0.2, random_state=0)\n",
    "        self.model_last = DecisionTreeRegressor()\n",
    "        self.model_last.fit(X_train, y_train)\n",
    "        last_pred_train = self.model_last.predict(X_train)\n",
    "        last_pred = self.model_last.predict(X_test)\n",
    "        \n",
    "        return last_pred\n",
    "        l\n",
    "    def _predict_stage_n(self, X, preds):\n",
    "        if self.stop:\n",
    "            return preds\n",
    "        else:\n",
    "            preds[:, 0] += self.model_0.predict(X)\n",
    "            preds[:, 1] += self.model_1.predict(X)\n",
    "            preds[:, 2] += self.model_2.predict(X)\n",
    "            return self.next_stage._predict_stage_n(X, preds)\n",
    "        \n",
    "    def _predict_last_stage(self, X):\n",
    "        pred = self.model_last.predict(X)\n",
    "        return pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********** LinearRegression - 0 **********\n",
      "MSE(平均二乗誤差)(test) : 2.156e+09\n",
      "********** DecisionTreeRegressor - 0 **********\n",
      "MSE(平均二乗誤差)(test) : 3.405e+09\n",
      "********** Lasso - 0 **********\n",
      "MSE(平均二乗誤差)(test) : 2.156e+09\n",
      "********** LinearRegression - 1 **********\n",
      "MSE(平均二乗誤差)(test) : 2.378e+09\n",
      "********** DecisionTreeRegressor - 1 **********\n",
      "MSE(平均二乗誤差)(test) : 3.469e+09\n",
      "********** Lasso - 1 **********\n",
      "MSE(平均二乗誤差)(test) : 2.378e+09\n",
      "********** LinearRegression - 2 **********\n",
      "MSE(平均二乗誤差)(test) : 1.600e+09\n",
      "********** DecisionTreeRegressor - 2 **********\n",
      "MSE(平均二乗誤差)(test) : 2.115e+09\n",
      "********** Lasso - 2 **********\n",
      "MSE(平均二乗誤差)(test) : 1.600e+09\n",
      "[学習モデル:決定木_バギング:10model]MSE:2.164e+09\n"
     ]
    }
   ],
   "source": [
    "scr_st = ScratchStacking(verbose=True, random_state=0)\n",
    "scr_st.fit(X1_train, y1_train)\n",
    "\n",
    "st_pred = scr_st.predict(X1_test)\n",
    "\n",
    "mse11 = mean_squared_error(y1_test, st_pred)\n",
    "print(\"[学習モデル:決定木_バギング:10model]MSE:{:.3e}\".format(mse11))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### スタッキングでもMSEが下がった。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "735px",
    "left": "1965.98px",
    "right": "20px",
    "top": "123px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
