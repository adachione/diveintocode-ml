{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sprint 論文読解入門"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 論文\n",
    "[[8]Ren, S., He, K., Girshick, R., Sun, J.: Faster r-cnn: Towards real-time object detection with region proposal networks. In: Advances in neural information processing systems. (2015) 91–99](https://arxiv.org/pdf/1506.01497.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 参考\n",
    "- [Faster R-CNNにおけるRPNの世界一分かりやすい解説](https://medium.com/lsc-psd/faster-r-cnn%E3%81%AB%E3%81%8A%E3%81%91%E3%82%8Brpn%E3%81%AE%E4%B8%96%E7%95%8C%E4%B8%80%E5%88%86%E3%81%8B%E3%82%8A%E3%82%84%E3%81%99%E3%81%84%E8%A7%A3%E8%AA%AC-dfc0c293cb69)\n",
    "- [Faster R-CNN (object detection) implemented by Keras for custom data from Google’s Open Images Dataset V4](https://towardsdatascience.com/faster-r-cnn-object-detection-implemented-by-keras-for-custom-data-from-googles-open-images-125f62b9141a)\n",
    "- [Fast-CNN](https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Girshick_Fast_R-CNN_ICCV_2015_paper.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 問題\n",
    "それぞれについてJupyter Notebookにマークダウン形式で記述してください。\n",
    "\n",
    "\n",
    "(1) 物体検出の分野にはどういった手法が存在したか。\n",
    "\n",
    "\n",
    "(2) Fasterとあるが、どういった仕組みで高速化したのか。\n",
    "\n",
    "\n",
    "(3) One-Stageの手法とTwo-Stageの手法はどう違うのか。\n",
    "\n",
    "\n",
    "(4) RPNとは何か。\n",
    "\n",
    "\n",
    "(5) RoIプーリングとは何か。\n",
    "\n",
    "\n",
    "(6) Anchorのサイズはどうするのが適切か。\n",
    "\n",
    "\n",
    "(7) 何というデータセットを使い、先行研究に比べどういった指標値が得られているか。\n",
    "\n",
    "\n",
    "(8) （アドバンス課題）Faster R-CNNよりも新しい物体検出の論文では、Faster R-CNNがどう引用されているか。\n",
    "\n",
    "\n",
    "-----条件-----\n",
    "1. 答える際は論文のどの部分からそれが分かるかを書く。\n",
    "2. 必要に応じて先行研究（引用されている論文）も探しにいく。最低2つは他の論文を利用して回答すること。\n",
    "3. 論文の紹介記事を見ても良い。ただし、答えは論文内に根拠を探すこと。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1)物体検出の分野にはどういった手法が存在したか。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[モデル]\n",
    "- SPPnet\n",
    "- Fast R-CNN\n",
    "\n",
    "[手法]\n",
    "- region proposal methods\n",
    "- region-based convolutional neural networks (RCNNs) \n",
    "- Selective Search\n",
    "- EdgeBoxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) Fasterとあるが、どういった仕組みで高速化したのか。\n",
    "- 従来のR-CNNは切り取った矩形領域を一つ一つNNにかけていたが、CNNと組み合わせ重みを共有することで矩形切取→物体検出までを一気通貫でできるようにした。\n",
    ">- (P2.The first module is a deep\n",
    "fully convolutional network that proposes regions,\n",
    "and the second module is the Fast R-CNN detector [2]\n",
    "that uses the proposed regions. The entire system is a single, unified network for object detection (Figure 2).)\n",
    "> - (P3. Because our ultimate goal is to share computation with a Fast R-CNN\n",
    "object detection network )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3) One-Stageの手法とTwo-Stageの手法はどう違うのか。\n",
    "- Two-stage概要\n",
    "  - 回帰と分類それぞれの計算ができる構造  \n",
    "  (P10.The OverFeat paper [9] proposes a detection\n",
    "method that uses regressors and classifiers on sliding\n",
    "windows over convolutional feature maps.)\n",
    "  - 特徴\n",
    "  領域ごとの特徴が適応的になり、領域の特徴を具体的にカバーできる。  \n",
    "    (P10.In the second stage of\n",
    "our cascade, the region-wise features are adaptively\n",
    "pooled [1], [2] from proposal boxes that more faithfully cover the features of the regions. We believe\n",
    "these features lead to more accurate detections.)\n",
    "\n",
    "  \n",
    "- 関連論文\n",
    "  1. [R-CNN minus R](https://arxiv.org/pdf/1506.06981.pdf)\n",
    "    - Two-stageについて記述箇所。要約すると「領域をCNNに取り込み総合的に判断する」とあり。総合的というのは回帰・分類ということではないかと推測。  \n",
    "    (P2.  The two-stage computation using region\n",
    "proposal generation is a form of cascade detector [20]\n",
    "or jumping window [17, 19]. However, they differ in\n",
    "part-based detector such as [7] in that they do not explicitly model object parts in learning; instead parts\n",
    "are implicitly capture in the CNN. Integrated training of SPP-CNN as a single CNN learning problem,\n",
    "not dissimilar to some of the ideas of Section 3.2,\n",
    "have very recently been explored in the unpublished\n",
    "manuscript [8].)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4) RPNとは何か。\n",
    "- Region Proposal Network\n",
    "  - 役割：矩形領域を複数取得し、それぞれにスコアをつける。(P3.A Region Proposal Network (RPN) takes an image\n",
    "(of any size) as input and outputs a set of rectangular\n",
    "object proposals, each with an objectness score.3)　　\n",
    "\n",
    "  - スコア（重み）をつけ、もう1層（stage）のCNNとスコア共有をする。Fast R-CNNモジュールがどこを見るべきかを指示(P3.the RPN　module tells the Fast R-CNN module where to look.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5) RoIプーリングとは何か。\n",
    "- 畳み込み処理が終わった「feature map」からresion proporsalを取得したのちに、全てのregion proposalを元の画像サイズにプーリングさせる手法。これにより共通の次元サイズでNN計算ができる。バックプロバゲーション時にはプーリング層は無視される。ただし解像度が落ちてしまうという欠点あり。\n",
    ">- (P5.To account\n",
    "for varying sizes, a set of k bounding-box regressors\n",
    "are learned. Each regressor is responsible for one scale\n",
    "and one aspect ratio, and the k regressors do not share\n",
    "weights. As such, it is still possible to predict boxes of\n",
    "various sizes even though the features are of a fixed\n",
    "size/scale, thanks to the design of anchors.\n",
    ">- (P5.Next we describe algorithms that\n",
    "learn a unified network composed of RPN and Fast\n",
    "R-CNN with shared convolutional layers)\n",
    ">- (P6.These\n",
    "gradients are ignored in the above approximate joint\n",
    "training)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6) Anchorのサイズはどうするのが適切か。\n",
    "- Anchorとは\n",
    "  - Feature mapからピクセル等間隔で割り振っていく中心点のようなもの\n",
    "  \n",
    "- AnchorBoxとは\n",
    "  - 各Anchorから基準の長さとアスペクト比を設定して作る矩形。\n",
    "\n",
    "- R-CNNでは基準の長さを(128^2, 256^2, 512^2)、アスペクト比を（1:1, 1:2, 2:1（正方形、横長、縦長））としていて、1つのAnchorに対し基準の長さとアスペクト比のそれぞれを組み合わせた９通りのAnchorBoxが作られるということになる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (7) 何というデータセットを使い、先行研究に比べどういった指標値が得られているか。\n",
    "- データセット:PASCAL VOC 2007 , PASCAL VOC 2012, MS COCO\n",
    "\n",
    "\n",
    "- 先行研究との比較指標値\n",
    "  >- mAP(mean Average Precision):全ての時点iにおけるAP(平均適合率)を平均することで、より一般的な平均値を求める為のもの。\n",
    "  において、従来の手法や従来のregion proposal数（２０００）の15%の数のregion proposal数(300)で59.9%という指標値が得られている。\n",
    "  (P7.Table2, RPN\n",
    "with Fast R-CNN achieves competitive results, with\n",
    "an mAP of 59.9% while using up to 300 proposals8.)\n",
    "  >- 実行時間においても、従来のSSから比べ約1/30に削減。\n",
    "  (P8. Table5)\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (8) （アドバンス課題）Faster R-CNNよりも新しい物体検出の論文では、Faster R-CNNがどう引用されているか。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [YOLO](https://arxiv.org/pdf/1506.02640.pdf)\n",
    "  - one-stage手法\n",
    "  - 損失関数を工夫（かなり複雑で一切わかりませんでしたが）\n",
    "  - Fast R-CNNとYOLOを組み合わせて、mAPが70％程度に上がっている。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## その他\n",
    "- [Keras Faster-RCNNのgithub](https://github.com/you359/Keras-FasterRCNN)\n",
    "- 画像処理系のKerasチュートリアルを実行し、別ファイルにて保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
