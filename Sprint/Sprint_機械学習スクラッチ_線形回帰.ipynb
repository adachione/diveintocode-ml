{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sprint 機械学習スクラッチ 線形回帰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "X_array = np.array([[1, 1], [1, 2], [1, 3]])\n",
    "y_array = np.ones(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScratchLinearRegression():\n",
    "    \"\"\"\n",
    "    線形回帰のスクラッチ実装\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    num_iter : int\n",
    "      イテレーション数\n",
    "    lr : float\n",
    "      学習率\n",
    "    no_bias : bool\n",
    "      バイアス項を入れない場合はTrue\n",
    "    verbose : bool\n",
    "      学習過程を出力する場合はTrue\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    self.coef_ : 次の形のndarray, shape (n_features,)\n",
    "      パラメータ\n",
    "    self.loss : 次の形のndarray, shape (self.iter,)\n",
    "      訓練データに対する損失の記録\n",
    "    self.val_loss : 次の形のndarray, shape (self.iter,)\n",
    "      検証データに対する損失の記録\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_iter, lr, no_bias, verbose):\n",
    "        # ハイパーパラメータを属性として記録\n",
    "        self.iter = num_iter\n",
    "        self.lr = lr\n",
    "        self.no_bias = no_bias\n",
    "        self.verbose = verbose\n",
    "        # 損失を記録する配列を用意\n",
    "        self.loss = np.zeros(self.iter)\n",
    "        self.val_loss = np.zeros(self.iter)\n",
    "        #np.random.seed(0)\n",
    "\n",
    "    def fit(self, X, y, X_val=None, y_val=None):\n",
    "        \"\"\"\n",
    "        線形回帰を学習する。検証データが入力された場合はそれに対する損失と精度もイテレーションごとに計算する。\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (n_samples, n_features)\n",
    "            訓練データの特徴量\n",
    "        y : 次の形のndarray, shape (n_samples, )\n",
    "            訓練データの正解値\n",
    "        X_val : 次の形のndarray, shape (n_samples, n_features)\n",
    "            検証データの特徴量\n",
    "        y_val : 次の形のndarray, shape (n_samples, )\n",
    "            検証データの正解値\n",
    "        \"\"\"\n",
    "        np.random.seed(0)\n",
    "        self.theta = np.random.randn(X.shape[1])\n",
    "\n",
    "        self.X = np.copy(X)\n",
    "        self.y = np.copy(y)\n",
    "        self.X_val = np.copy(X_val)\n",
    "        self.y_val = np.copy(y_val)\n",
    "        \n",
    "        if self.verbose:\n",
    "            # verboseをTrueにした際は学習過程を出力\n",
    "            print()\n",
    "\n",
    "        for i in range(self.iter):\n",
    "            self.linear_hypo = self._linear_hypothesis(self.X)\n",
    "            #print(self.linear_hypo)\n",
    "            \n",
    "            self.grad = self._gradient_descent(self.X, self.y)\n",
    "            #print(\"[theta]\", self.grad)\n",
    "            \n",
    "            self.pred = self.predict(self.X)\n",
    "            #print(\"[pred]\", self.pred)\n",
    "            \n",
    "            self.loss[i] = self.loss_func(self.y)\n",
    "            print(\"loss\", self.loss)\n",
    "            \n",
    "            #if X_val != None and y_val != None:\n",
    "                #self.val_loss[i] = self.loss(self.y_val)\n",
    "\n",
    "            #grad = self._gradient_descent(X, y, alpha=0.1, error=0)  \n",
    "            \n",
    "            #print(grad)\n",
    "\n",
    "    def _linear_hypothesis(self, X):\n",
    "        \"\"\"\n",
    "        線形の仮定関数を計算する\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (n_samples, n_features)\n",
    "          訓練データ\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "          次の形のndarray, shape (n_samples, 1)\n",
    "          線形の仮定関数による推定結果\n",
    "\n",
    "        \"\"\"\n",
    "        self.hypo = np.dot(self.X, self.theta.T)\n",
    "\n",
    "        return self.hypo\n",
    "\n",
    "    def _gradient_descent(self, X, y, alpha=0.1, error=0):\n",
    "        \"\"\"\n",
    "        最急降下法\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (n_samples, n_features)\n",
    "          訓練データ\n",
    "\n",
    "        alpha：学習率\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "\n",
    "        \"\"\"\n",
    "        # m = len(self.y)\n",
    "        self.theta = self.theta - alpha*(1/2)*np.dot(self.X.T,(self._linear_hypothesis(self.X)-y))\n",
    "        \n",
    "        # theta = np.random.random(2)\n",
    "        \n",
    "        #for x in range(X.shape[0]):\n",
    "        # self.temp0 = theta[0] - alpha*(1/(m))*np.sum(theta[0]+theta[1]*X-y)\n",
    "        # self.temp1 = theta[1] - alpha*(1/(m))*np.sum((theta[0]+theta[1]*X-y)*X)\n",
    "        \n",
    "        # self.gradient += alpha * (self._linear_hypothesis(X)[x] - y[x]) * X[x, :]\n",
    "\n",
    "        # self.gradient = self.gradient / X.shape[0]\n",
    "\n",
    "        # thetaの更新\n",
    "        # theta = np.array([self.temp0, self.temp1])\n",
    "        \n",
    "        return self.theta\n",
    "\n",
    "    def predict(self, X):#self.linear_hypo\n",
    "        \"\"\"\n",
    "        線形回帰を使い推定する。\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (n_samples, n_features)\n",
    "            サンプル\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            次の形のndarray, shape (n_samples, 1)\n",
    "            線形回帰による推定結果\n",
    "        \"\"\"\n",
    "        # print(self.X)\n",
    "\n",
    "        pred = np.dot(self.X, self._gradient_descent(self.X, self.y).T)\n",
    "\n",
    "        return pred\n",
    "    \n",
    "    def loss_func(self, y):\n",
    "        \"\"\"\n",
    "        平均二乗誤差の計算\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        y_pred : 次の形のndarray, shape (n_samples,)\n",
    "          推定した値\n",
    "        y : 次の形のndarray, shape (n_samples,)\n",
    "          正解値\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        mse : numpy.float\n",
    "          平均二乗誤差\n",
    "        \"\"\"\n",
    "        loss = np.average((self.pred - y) ** 2) / 2\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "scr_lr = ScratchLinearRegression(num_iter=15, \n",
    "                                 lr=1, \n",
    "                                 no_bias=1, \n",
    "                                 verbose=False, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss [0.01719295 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.        ]\n",
      "loss [0.01719295 0.01505335 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.        ]\n",
      "loss [0.01719295 0.01505335 0.013996   0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.        ]\n",
      "loss [0.01719295 0.01505335 0.013996   0.01301361 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.        ]\n",
      "loss [0.01719295 0.01505335 0.013996   0.01301361 0.01210017 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.        ]\n",
      "loss [0.01719295 0.01505335 0.013996   0.01301361 0.01210017 0.01125085\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.        ]\n",
      "loss [0.01719295 0.01505335 0.013996   0.01301361 0.01210017 0.01125085\n",
      " 0.01046114 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.        ]\n",
      "loss [0.01719295 0.01505335 0.013996   0.01301361 0.01210017 0.01125085\n",
      " 0.01046114 0.00972687 0.         0.         0.         0.\n",
      " 0.         0.         0.        ]\n",
      "loss [0.01719295 0.01505335 0.013996   0.01301361 0.01210017 0.01125085\n",
      " 0.01046114 0.00972687 0.00904413 0.         0.         0.\n",
      " 0.         0.         0.        ]\n",
      "loss [0.01719295 0.01505335 0.013996   0.01301361 0.01210017 0.01125085\n",
      " 0.01046114 0.00972687 0.00904413 0.00840931 0.         0.\n",
      " 0.         0.         0.        ]\n",
      "loss [0.01719295 0.01505335 0.013996   0.01301361 0.01210017 0.01125085\n",
      " 0.01046114 0.00972687 0.00904413 0.00840931 0.00781906 0.\n",
      " 0.         0.         0.        ]\n",
      "loss [0.01719295 0.01505335 0.013996   0.01301361 0.01210017 0.01125085\n",
      " 0.01046114 0.00972687 0.00904413 0.00840931 0.00781906 0.00727023\n",
      " 0.         0.         0.        ]\n",
      "loss [0.01719295 0.01505335 0.013996   0.01301361 0.01210017 0.01125085\n",
      " 0.01046114 0.00972687 0.00904413 0.00840931 0.00781906 0.00727023\n",
      " 0.00675993 0.         0.        ]\n",
      "loss [0.01719295 0.01505335 0.013996   0.01301361 0.01210017 0.01125085\n",
      " 0.01046114 0.00972687 0.00904413 0.00840931 0.00781906 0.00727023\n",
      " 0.00675993 0.00628544 0.        ]\n",
      "loss [0.01719295 0.01505335 0.013996   0.01301361 0.01210017 0.01125085\n",
      " 0.01046114 0.00972687 0.00904413 0.00840931 0.00781906 0.00727023\n",
      " 0.00675993 0.00628544 0.00584426]\n"
     ]
    }
   ],
   "source": [
    "scr_lr.fit(X_array, y_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題1】仮定関数\n",
    "以下の数式で表される線形回帰の仮定関数を実装してください。メソッドの雛形を用意してあります。\n",
    "\n",
    "\n",
    "h\n",
    "θ\n",
    "(\n",
    "x\n",
    ")\n",
    "=\n",
    "θ\n",
    "0\n",
    "x\n",
    "0\n",
    "+\n",
    "θ\n",
    "1\n",
    "x\n",
    "1\n",
    "+\n",
    ".\n",
    ".\n",
    ".\n",
    "+\n",
    "θ\n",
    "j\n",
    "x\n",
    "j\n",
    "+\n",
    ".\n",
    ".\n",
    ".\n",
    "+\n",
    "θ\n",
    "n\n",
    "x\n",
    "n\n",
    ".\n",
    "(\n",
    "x\n",
    "0\n",
    "=\n",
    "1\n",
    ")\n",
    "\n",
    "$x$ : 特徴量ベクトル\n",
    "\n",
    "\n",
    "$\\theta$ : パラメータベクトル\n",
    "\n",
    "\n",
    "$n$ : 特徴量の数\n",
    "\n",
    "\n",
    "$x_j$ : j番目の特徴量\n",
    "\n",
    "\n",
    "$\\theta_j$ : j番目のパラメータ（重み）\n",
    "\n",
    "\n",
    "特徴量の数$n$は任意の値に対応できる実装にしてください。\n",
    "\n",
    "\n",
    "なお、ベクトル形式で表すと以下のようになります。\n",
    "\n",
    "\n",
    "h\n",
    "θ\n",
    "(\n",
    "x\n",
    ")\n",
    "=\n",
    "θ\n",
    "T\n",
    "⋅\n",
    "x\n",
    ".\n",
    "\n",
    "雛形\n",
    "\n",
    "\n",
    "クラスの外から呼び出すことがないメソッドのため、Pythonの慣例としてアンダースコアを先頭にひとつつけています。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _linear_hypothesis(X):\n",
    "    \"\"\"\n",
    "    線形の仮定関数を計算する\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : 次の形のndarray, shape (n_samples, n_features)\n",
    "      訓練データ\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "      次の形のndarray, shape (n_samples, 1)\n",
    "      線形の仮定関数による推定結果\n",
    "\n",
    "    \"\"\"\n",
    "    line_hypo = np.dot(X, theta.T)\n",
    "    \n",
    "    return line_hypo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "li_hy = _linear_hypothesis(X_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題2】最急降下法\n",
    "最急降下法により学習させる実装を行なってください。以下の式で表されるパラメータの更新式のメソッド_gradient_descentを追加し、fit\n",
    "メソッドから呼び出すようにしてください。\n",
    "\n",
    "\n",
    "θ\n",
    "j\n",
    ":=\n",
    "θ\n",
    "j\n",
    "−\n",
    "α\n",
    "1\n",
    "m\n",
    "m\n",
    "∑\n",
    "i\n",
    "=\n",
    "1\n",
    " \n",
    "[\n",
    "(\n",
    "h\n",
    "θ\n",
    "(\n",
    "x\n",
    "(\n",
    "i\n",
    ")\n",
    ")\n",
    "−\n",
    "y\n",
    "(\n",
    "i\n",
    ")\n",
    ")\n",
    "x\n",
    "(\n",
    "i\n",
    ")\n",
    "j\n",
    "]\n",
    "\n",
    "$\\alpha$ : 学習率\n",
    "\n",
    "\n",
    "$i$ : サンプルのインデックス\n",
    "\n",
    "\n",
    "$j$ : 特徴量のインデックス\n",
    "\n",
    "\n",
    "雛形\n",
    "\n",
    "\n",
    "ScratchLinearRegressionクラスへ以下のメソッドを追加してください。コメントアウト部分の説明も記述してください。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _gradient_descent_1(X, y, alpha=0.1, error=0):\n",
    "    \"\"\"\n",
    "    最急降下法\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : 次の形のndarray, shape (n_samples, n_features)\n",
    "      訓練データ\n",
    "    \n",
    "    alpha：学習率\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    \n",
    "    \"\"\"\n",
    "    # データフレーム型だった場合にnd_array型に変換\n",
    "    if type(X) is pd.core.frame.DataFrame:\n",
    "        X = X.values\n",
    "        \n",
    "    np.random.seed(0)\n",
    "    theta = np.random.random(size=X.shape[1])\n",
    "\n",
    "    y = np.ones(5)\n",
    "    \n",
    "    # gradientを求める\n",
    "    gradient = 0\n",
    "    for x in range(X.shape[1]):\n",
    "        gradient += alpha * (_linear_hypothesis(X)[x] - y[x]) * X[x, :]\n",
    "    \n",
    "    gradient = gradient / X.shape[1]\n",
    "    \n",
    "    # thetaの更新\n",
    "    error = theta - gradient\n",
    "    \n",
    "    return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.48665375, 0.60407   ])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = _gradient_descent(X_array, y_array, alpha=0.1, error=0)\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題3】推定\n",
    "推定する仕組みを実装してください。ScratchLinearRegressionクラスの雛形に含まれるpredictメソッドに書き加えてください。\n",
    "\n",
    "\n",
    "仮定関数 $h_\\theta(x)$ の出力が推定結果です。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X):\n",
    "    \"\"\"\n",
    "    線形回帰を使い推定する。\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : 次の形のndarray, shape (n_samples, n_features)\n",
    "        サンプル\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        次の形のndarray, shape (n_samples, 1)\n",
    "        線形回帰による推定結果\n",
    "    \"\"\"\n",
    "\n",
    "    theta = np.random.random(size=X.shape[1])\n",
    "    theta = np.dot(X, _gradient_descent(X).T)    \n",
    "    \n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.53251795, -1.35858852, -3.01072967, -4.66287081, -6.31501196])"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = predict(X_array)\n",
    "pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題4】平均二乗誤差\n",
    "線形回帰の指標値として用いられる平均二乗誤差（mean square error, MSE）の関数を作成してください。\n",
    "\n",
    "\n",
    "平均二乗誤差関数は回帰問題全般で使える関数のため、ScratchLinearRegressionクラスのメソッドではなく、別の関数として作成してください。雛形を用意してあります。\n",
    "\n",
    "\n",
    "平均二乗誤差は以下の数式で表されます。\n",
    "\n",
    "\n",
    "L\n",
    "(\n",
    "θ\n",
    ")\n",
    "=\n",
    "1\n",
    "m\n",
    "m\n",
    "∑\n",
    "i\n",
    "=\n",
    "1\n",
    " \n",
    "(\n",
    "h\n",
    "θ\n",
    "(\n",
    "x\n",
    "(\n",
    "i\n",
    ")\n",
    ")\n",
    "−\n",
    "y\n",
    "(\n",
    "i\n",
    ")\n",
    ")\n",
    "2\n",
    ".\n",
    "\n",
    "$m$ : 入力されるデータの数\n",
    "\n",
    "\n",
    "$h_\\theta()$ : 仮定関数\n",
    "\n",
    "\n",
    "$x^{(i)}$ : i番目のサンプルの特徴量ベクトル\n",
    "\n",
    "\n",
    "$y^{(i)}$ : i番目のサンプルの正解値\n",
    "\n",
    "\n",
    "なお、最急降下法のための目的関数（損失関数）としては、これを2で割ったものを使用します。（問題5, 9）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gradient = 0\n",
    "y = np.ones(5)\n",
    "alpha = 0.1\n",
    "\n",
    "for i in range(1, 3):\n",
    "    cnt = 1\n",
    "    for x in range(X_array.shape[0]):\n",
    "        gradient += X_array[x] - alpha *(_linear_hypothesis(X_array, theta_array)[i] - y[i]) * X_array[i]\n",
    "    cnt +=1\n",
    "\n",
    "gradient = gradient / cnt\n",
    "\n",
    "print(gradient)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y_array = np.ones(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "alpha * (_linear_hypothesis(X=X_array, theta=theta_array)[1] - y[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_linear_hypothesis(X=X_array, theta=theta_array)[1] - y[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gradient_1 = X_array[1] - alpha *(_linear_hypothesis(X_array, theta_array)[1] - y[1]) * X_array[1] \n",
    "gradient_2 = X_array[2] - alpha *(_linear_hypothesis(X_array, theta_array)[2] - y[2]) * X_array[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_linear_hypothesis(X=X_array, theta=theta_array)[2] - y[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X_array[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "theta_array.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "theta_array.shape[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題4】平均二乗誤差\n",
    "線形回帰の指標値として用いられる平均二乗誤差（mean square error, MSE）の関数を作成してください。\n",
    "\n",
    "\n",
    "平均二乗誤差関数は回帰問題全般で使える関数のため、ScratchLinearRegressionクラスのメソッドではなく、別の関数として作成してください。雛形を用意してあります。\n",
    "\n",
    "\n",
    "平均二乗誤差は以下の数式で表されます。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSE(y_pred, y):\n",
    "    \"\"\"\n",
    "    平均二乗誤差の計算\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_pred : 次の形のndarray, shape (n_samples,)\n",
    "      推定した値\n",
    "    y : 次の形のndarray, shape (n_samples,)\n",
    "      正解値\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    mse : numpy.float\n",
    "      平均二乗誤差\n",
    "    \"\"\"\n",
    "    mse = np.average((y_pred - y) ** 2)\n",
    "    \n",
    "    return mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題5】目的関数\n",
    "以下の数式で表される線形回帰の 目的関数（損失関数） を実装してください。そして、これをself.loss, self.val_lossに記録するようにしてください。\n",
    "\n",
    "\n",
    "目的関数（損失関数） \n",
    "J\n",
    "(\n",
    "θ\n",
    ")\n",
    " は次の式です。\n",
    "\n",
    "\n",
    "J\n",
    "(\n",
    "θ\n",
    ")\n",
    "=\n",
    "1\n",
    "2\n",
    "m\n",
    "m\n",
    "∑\n",
    "i\n",
    "=\n",
    "1\n",
    " \n",
    "(\n",
    "h\n",
    "θ\n",
    "(\n",
    "x\n",
    "(\n",
    "i\n",
    ")\n",
    ")\n",
    "−\n",
    "y\n",
    "(\n",
    "i\n",
    ")\n",
    ")\n",
    "2\n",
    ".\n",
    "\n",
    "m\n",
    " : 入力されるデータの数\n",
    "\n",
    "\n",
    "h\n",
    "θ\n",
    "(\n",
    ")\n",
    " : 仮定関数\n",
    "\n",
    "\n",
    "x\n",
    "(\n",
    "i\n",
    ")\n",
    " : i番目のサンプルの特徴量ベクトル\n",
    "\n",
    "\n",
    "y\n",
    "(\n",
    "i\n",
    ")\n",
    " : i番目のサンプルの正解値"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(y_pred, y):\n",
    "    \"\"\"\n",
    "    平均二乗誤差の計算\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_pred : 次の形のndarray, shape (n_samples,)\n",
    "      推定した値\n",
    "    y : 次の形のndarray, shape (n_samples,)\n",
    "      正解値\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    mse : numpy.float\n",
    "      平均二乗誤差\n",
    "    \"\"\"\n",
    "    \n",
    "    loss = np.average((y_pred - y) ** 2) / 2\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_loss(y_true, y):\n",
    "    \"\"\"\n",
    "    平均二乗誤差の計算\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_pred : 次の形のndarray, shape (n_samples,)\n",
    "      推定した値\n",
    "    y : 次の形のndarray, shape (n_samples,)\n",
    "      正解値\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    mse : numpy.float\n",
    "      平均二乗誤差\n",
    "    \"\"\"\n",
    "    \n",
    "    val_loss = np.average((y_true - y) ** 2) / 2\n",
    "    \n",
    "    return val_loss    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.54340494, 0.27836939, 0.42451759, 0.84477613, 0.00471886])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(100)\n",
    "y_1 = np.random.random(5)\n",
    "y_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_2 = np.ones(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_3 = np.array([0,1,0,1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.41501775984547484"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "err = np.average((y_2 - y_1) ** 2)\n",
    "err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.41501775984547484"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MSE(y_1, y_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.20750887992273742"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss(y_1, y_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_loss(y_3, y_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題6】学習と推定\n",
    "機械学習スクラッチ入門のSprintで用意したHouse Pricesコンペティションのデータに対してスクラッチ実装の学習と推定を行なってください。\n",
    "\n",
    "\n",
    "scikit-learnによる実装と比べ、正しく動いているかを確認してください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題7】学習曲線のプロット\n",
    "学習曲線を表示する関数を作成し、実行してください。グラフを見て損失が適切に下がっているかどうか確認してください。\n",
    "\n",
    "\n",
    "線形回帰クラスの雛形ではself.loss, self.val_lossに損失を記録しておくようになっているため、入力にはこれを利用してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "467px",
    "left": "1550px",
    "right": "20px",
    "top": "120px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
